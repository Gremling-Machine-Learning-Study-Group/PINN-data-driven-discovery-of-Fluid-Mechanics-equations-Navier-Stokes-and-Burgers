{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Burgers_nu3_2000_2 layers -.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvsbii5zbr9p",
        "outputId": "96ed839a-3888-43bc-ac0e-79acfea7d65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-lmodern fonts-noto-mono libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpotrace0 libptexenc1 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13\n",
            "  lmodern poppler-data t1utils tex-common texlive-base texlive-binaries\n",
            "  texlive-latex-base\n",
            "Suggested packages:\n",
            "  fonts-noto poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum debhelper gv\n",
            "  | postscript-viewer perl-tk xpdf-reader | pdf-viewer texlive-latex-base-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-lmodern fonts-noto-mono libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpotrace0 libptexenc1 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13\n",
            "  lmodern poppler-data t1utils tex-common texlive-base texlive-binaries\n",
            "  texlive-latex-base texlive-latex-recommended\n",
            "0 upgraded, 24 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 68.4 MB of archives.\n",
            "After this operation, 223 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.15 [5,092 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [2,265 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Fetched 68.4 MB in 3s (26.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 24.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 155335 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../01-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../02-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../03-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../04-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../05-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../06-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../09-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.15_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../10-libgs9_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../11-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../12-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../13-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../14-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../15-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../16-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../17-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../18-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../19-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../20-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../21-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../22-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../23-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-lato fonts-texgyre javascript-common libjs-jquery libruby2.5\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration tex-gyre texlive-fonts-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  apache2 | lighttpd | httpd ri ruby-dev bundler texlive-fonts-recommended-doc\n",
            "  python-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc dot2tex prerex\n",
            "  ruby-tcltk | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  fonts-lato fonts-texgyre javascript-common libjs-jquery libruby2.5\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration tex-gyre texlive-fonts-recommended texlive-latex-extra\n",
            "  texlive-pictures texlive-plain-generic tipa\n",
            "0 upgraded, 21 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 66.6 MB of archives.\n",
            "After this operation, 216 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.11 [48.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.11 [3,072 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Fetched 66.6 MB in 3s (22.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 21.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-lato.\n",
            "(Reading database ... 162669 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../01-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../02-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../03-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../04-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../05-ruby2.5_2.5.1-1ubuntu1.11_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.11) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../06-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../07-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../08-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../09-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../10-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../11-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../12-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../13-libruby2.5_2.5.1-1ubuntu1.11_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.11) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../14-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../15-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../16-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../17-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../18-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../19-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../20-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.11) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.11) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running mktexlsr. This may take some time... done.\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  ghostscript gsfonts\n",
            "Suggested packages:\n",
            "  ghostscript-x\n",
            "The following NEW packages will be installed:\n",
            "  dvipng ghostscript gsfonts\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,250 kB of archives.\n",
            "After this operation, 4,947 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [51.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dvipng amd64 1.15-1 [78.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Fetched 3,250 kB in 1s (3,700 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ghostscript.\n",
            "(Reading database ... 181020 files and directories currently installed.)\n",
            "Preparing to unpack .../ghostscript_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cm-super-minimal pfb2t1c2pfb\n",
            "The following NEW packages will be installed:\n",
            "  cm-super cm-super-minimal pfb2t1c2pfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 24.5 MB of archives.\n",
            "After this operation, 59.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super-minimal all 0.3.4-11 [5,810 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pfb2t1c2pfb amd64 0.3-11 [9,342 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super all 0.3.4-11 [18.7 MB]\n",
            "Fetched 24.5 MB in 1s (16.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cm-super-minimal.\n",
            "(Reading database ... 181213 files and directories currently installed.)\n",
            "Preparing to unpack .../cm-super-minimal_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super-minimal (0.3.4-11) ...\n",
            "Selecting previously unselected package pfb2t1c2pfb.\n",
            "Preparing to unpack .../pfb2t1c2pfb_0.3-11_amd64.deb ...\n",
            "Unpacking pfb2t1c2pfb (0.3-11) ...\n",
            "Selecting previously unselected package cm-super.\n",
            "Preparing to unpack .../cm-super_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super (0.3.4-11) ...\n",
            "Setting up pfb2t1c2pfb (0.3-11) ...\n",
            "Setting up cm-super-minimal (0.3.4-11) ...\n",
            "Setting up cm-super (0.3.4-11) ...\n",
            "Creating fonts. This may take some time... done.\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running mktexlsr. This may take some time... done.\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "#devido aos plots no final do script devemos intalar latex na máquina\n",
        "! sudo apt-get install texlive-latex-recommended \n",
        "! sudo apt install texlive-latex-extra\n",
        "! sudo apt install dvipng\n",
        "! sudo apt install cm-super\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.15.2\n",
        "import tensorflow as tf   #machine learning framework\n",
        "#tf.disable_v2_behavior()\n",
        "print('TensorFlow version: ', tf.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBCUeYMwbyT4",
        "outputId": "091c716f-9f29-457a-eab7-b4baebff7cf4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.2`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "TensorFlow version:  <module 'tensorflow._api.v1.version' from '/tensorflow-1.15.2/python3.7/tensorflow_core/_api/v1/version/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@author: Maziar Raissi\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '../../Utilities/')\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from plotting import newfig, savefig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "import time\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)"
      ],
      "metadata": {
        "id": "ReZIGxTKnaNa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, X, u, layers, lb, ub):\n",
        "        \n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        \n",
        "        self.x = X[:,0:1]\n",
        "        self.t = X[:,1:2]\n",
        "        self.u = u\n",
        "        \n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NNs\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
        "        \n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
        "                \n",
        "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
        "        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n",
        "        \n",
        "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_pred))\n",
        "        \n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
        "                                                                method = 'L-BFGS-B', \n",
        "                                                                options = {'maxiter': 50000,\n",
        "                                                                           'maxfun': 50000,\n",
        "                                                                           'maxcor': 50,\n",
        "                                                                           'maxls': 50,\n",
        "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
        "    \n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "            \n",
        "    def net_u(self, x, t):  \n",
        "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
        "        return u\n",
        "    \n",
        "    def net_f(self, x, t):\n",
        "        lambda_1 = self.lambda_1        \n",
        "        lambda_2 = tf.exp(self.lambda_2)\n",
        "        u = self.net_u(x,t)\n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        f = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def callback(self, loss, lambda_1, lambda_2):\n",
        "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
        "        \n",
        "        \n",
        "    def train(self, nIter):\n",
        "        tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                lambda_1_value = self.sess.run(self.lambda_1)\n",
        "                lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
        "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
        "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict = tf_dict,\n",
        "                                fetches = [self.loss, self.lambda_1, self.lambda_2],\n",
        "                                loss_callback = self.callback)\n",
        "        \n",
        "        \n",
        "    def predict(self, X_star):\n",
        "        \n",
        "        tf_dict = {self.x_tf: X_star[:,0:1], self.t_tf: X_star[:,1:2]}\n",
        "        \n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
        "        \n",
        "        return u_star, f_star"
      ],
      "metadata": {
        "id": "bfhlzV7fnbJb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\": \n",
        "     \n",
        "    nu = 0.1/np.pi   #nu: valor de lambda_2\n",
        "\n",
        "    N_u = 2000  #N: número de elementos de treino\n",
        "    layers = [2, 20, 20, 20, 20, 20, 20, 1] #2 layers a menos\n",
        "    \n",
        "    data = scipy.io.loadmat('../content/un100.mat')\n",
        "    \n",
        "    t = data['t'].flatten()[:,None]\n",
        "    x = data['x'].flatten()[:,None]\n",
        "    Exact = np.real(data['usol']).T   #.T: transposição da matriz de dados usol\n",
        "    \n",
        "    X, T = np.meshgrid(x,t)\n",
        "    \n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "    u_star = Exact.flatten()[:,None]              \n",
        "\n",
        "    # Domain bounds\n",
        "    lb = X_star.min(0)\n",
        "    ub = X_star.max(0)    \n",
        "    \n",
        "    ######################################################################\n",
        "    ######################## Noiseless Data ###############################\n",
        "    ######################################################################\n",
        "    noise = 0.0            \n",
        "             \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "    \n",
        "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "    model.train(2000)\n",
        "    \n",
        "    u_pred, f_pred = model.predict(X_star)\n",
        "            \n",
        "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "    \n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "        \n",
        "    lambda_1_value = model.sess.run(model.lambda_1)\n",
        "    lambda_2_value = model.sess.run(model.lambda_2)\n",
        "    lambda_2_value = np.exp(lambda_2_value)\n",
        "    \n",
        "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
        "    error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
        "    \n",
        "    print('Error u: %e' % (error_u))    \n",
        "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
        "    print('Error l2: %.5f%%' % (error_lambda_2))  \n",
        "    \n",
        "                               "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWIWsRhBnzr2",
        "outputId": "eab2fbe1-fdd9-4a5e-d2e7-939eca4fe7b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "It: 0, Loss: 2.570e-01, Lambda_1: 0.001, Lambda_2: 0.002477, Time: 1.22\n",
            "It: 10, Loss: 1.777e-01, Lambda_1: -0.001, Lambda_2: 0.002489, Time: 0.22\n",
            "It: 20, Loss: 1.736e-01, Lambda_1: -0.012, Lambda_2: 0.002515, Time: 0.20\n",
            "It: 30, Loss: 1.635e-01, Lambda_1: -0.023, Lambda_2: 0.002545, Time: 0.23\n",
            "It: 40, Loss: 1.582e-01, Lambda_1: -0.034, Lambda_2: 0.002575, Time: 0.22\n",
            "It: 50, Loss: 1.506e-01, Lambda_1: -0.043, Lambda_2: 0.002604, Time: 0.21\n",
            "It: 60, Loss: 1.393e-01, Lambda_1: -0.051, Lambda_2: 0.002632, Time: 0.21\n",
            "It: 70, Loss: 1.210e-01, Lambda_1: -0.056, Lambda_2: 0.002653, Time: 0.23\n",
            "It: 80, Loss: 9.375e-02, Lambda_1: -0.056, Lambda_2: 0.002652, Time: 0.24\n",
            "It: 90, Loss: 6.295e-02, Lambda_1: -0.051, Lambda_2: 0.002611, Time: 0.21\n",
            "It: 100, Loss: 4.061e-02, Lambda_1: -0.035, Lambda_2: 0.002556, Time: 0.22\n",
            "It: 110, Loss: 3.121e-02, Lambda_1: -0.014, Lambda_2: 0.002498, Time: 0.21\n",
            "It: 120, Loss: 2.772e-02, Lambda_1: 0.008, Lambda_2: 0.002442, Time: 0.22\n",
            "It: 130, Loss: 2.503e-02, Lambda_1: 0.028, Lambda_2: 0.002395, Time: 0.22\n",
            "It: 140, Loss: 2.292e-02, Lambda_1: 0.041, Lambda_2: 0.002366, Time: 0.21\n",
            "It: 150, Loss: 2.149e-02, Lambda_1: 0.049, Lambda_2: 0.002355, Time: 0.23\n",
            "It: 160, Loss: 2.049e-02, Lambda_1: 0.054, Lambda_2: 0.002358, Time: 0.21\n",
            "It: 170, Loss: 1.976e-02, Lambda_1: 0.057, Lambda_2: 0.002369, Time: 0.22\n",
            "It: 180, Loss: 1.922e-02, Lambda_1: 0.059, Lambda_2: 0.002386, Time: 0.21\n",
            "It: 190, Loss: 1.882e-02, Lambda_1: 0.060, Lambda_2: 0.002405, Time: 0.19\n",
            "It: 200, Loss: 1.853e-02, Lambda_1: 0.060, Lambda_2: 0.002425, Time: 0.19\n",
            "It: 210, Loss: 1.831e-02, Lambda_1: 0.061, Lambda_2: 0.002445, Time: 0.21\n",
            "It: 220, Loss: 1.814e-02, Lambda_1: 0.062, Lambda_2: 0.002466, Time: 0.21\n",
            "It: 230, Loss: 1.800e-02, Lambda_1: 0.063, Lambda_2: 0.002487, Time: 0.21\n",
            "It: 240, Loss: 1.788e-02, Lambda_1: 0.064, Lambda_2: 0.002508, Time: 0.20\n",
            "It: 250, Loss: 1.777e-02, Lambda_1: 0.065, Lambda_2: 0.002530, Time: 0.21\n",
            "It: 260, Loss: 1.767e-02, Lambda_1: 0.066, Lambda_2: 0.002553, Time: 0.21\n",
            "It: 270, Loss: 1.757e-02, Lambda_1: 0.067, Lambda_2: 0.002576, Time: 0.20\n",
            "It: 280, Loss: 1.747e-02, Lambda_1: 0.068, Lambda_2: 0.002600, Time: 0.20\n",
            "It: 290, Loss: 1.737e-02, Lambda_1: 0.069, Lambda_2: 0.002625, Time: 0.20\n",
            "It: 300, Loss: 1.727e-02, Lambda_1: 0.071, Lambda_2: 0.002650, Time: 0.20\n",
            "It: 310, Loss: 1.717e-02, Lambda_1: 0.072, Lambda_2: 0.002677, Time: 0.22\n",
            "It: 320, Loss: 1.707e-02, Lambda_1: 0.074, Lambda_2: 0.002704, Time: 0.21\n",
            "It: 330, Loss: 1.696e-02, Lambda_1: 0.076, Lambda_2: 0.002732, Time: 0.21\n",
            "It: 340, Loss: 1.685e-02, Lambda_1: 0.078, Lambda_2: 0.002761, Time: 0.20\n",
            "It: 350, Loss: 1.673e-02, Lambda_1: 0.080, Lambda_2: 0.002791, Time: 0.20\n",
            "It: 360, Loss: 1.662e-02, Lambda_1: 0.082, Lambda_2: 0.002822, Time: 0.23\n",
            "It: 370, Loss: 1.650e-02, Lambda_1: 0.084, Lambda_2: 0.002854, Time: 0.21\n",
            "It: 380, Loss: 1.637e-02, Lambda_1: 0.086, Lambda_2: 0.002888, Time: 0.21\n",
            "It: 390, Loss: 1.624e-02, Lambda_1: 0.089, Lambda_2: 0.002922, Time: 0.21\n",
            "It: 400, Loss: 1.611e-02, Lambda_1: 0.092, Lambda_2: 0.002958, Time: 0.22\n",
            "It: 410, Loss: 1.597e-02, Lambda_1: 0.095, Lambda_2: 0.002995, Time: 0.21\n",
            "It: 420, Loss: 1.584e-02, Lambda_1: 0.098, Lambda_2: 0.003033, Time: 0.22\n",
            "It: 430, Loss: 1.569e-02, Lambda_1: 0.101, Lambda_2: 0.003072, Time: 0.22\n",
            "It: 440, Loss: 1.554e-02, Lambda_1: 0.105, Lambda_2: 0.003113, Time: 0.21\n",
            "It: 450, Loss: 1.539e-02, Lambda_1: 0.109, Lambda_2: 0.003154, Time: 0.23\n",
            "It: 460, Loss: 1.524e-02, Lambda_1: 0.113, Lambda_2: 0.003197, Time: 0.22\n",
            "It: 470, Loss: 1.508e-02, Lambda_1: 0.117, Lambda_2: 0.003241, Time: 0.22\n",
            "It: 480, Loss: 1.492e-02, Lambda_1: 0.122, Lambda_2: 0.003286, Time: 0.22\n",
            "It: 490, Loss: 1.477e-02, Lambda_1: 0.126, Lambda_2: 0.003331, Time: 0.20\n",
            "It: 500, Loss: 1.461e-02, Lambda_1: 0.131, Lambda_2: 0.003378, Time: 0.22\n",
            "It: 510, Loss: 1.446e-02, Lambda_1: 0.136, Lambda_2: 0.003426, Time: 0.22\n",
            "It: 520, Loss: 1.429e-02, Lambda_1: 0.141, Lambda_2: 0.003475, Time: 0.20\n",
            "It: 530, Loss: 1.413e-02, Lambda_1: 0.146, Lambda_2: 0.003525, Time: 0.21\n",
            "It: 540, Loss: 1.397e-02, Lambda_1: 0.152, Lambda_2: 0.003576, Time: 0.23\n",
            "It: 550, Loss: 1.384e-02, Lambda_1: 0.158, Lambda_2: 0.003627, Time: 0.21\n",
            "It: 560, Loss: 1.371e-02, Lambda_1: 0.163, Lambda_2: 0.003680, Time: 0.20\n",
            "It: 570, Loss: 1.352e-02, Lambda_1: 0.169, Lambda_2: 0.003734, Time: 0.21\n",
            "It: 580, Loss: 1.335e-02, Lambda_1: 0.175, Lambda_2: 0.003789, Time: 0.21\n",
            "It: 590, Loss: 1.319e-02, Lambda_1: 0.181, Lambda_2: 0.003845, Time: 0.24\n",
            "It: 600, Loss: 1.304e-02, Lambda_1: 0.187, Lambda_2: 0.003902, Time: 0.21\n",
            "It: 610, Loss: 1.290e-02, Lambda_1: 0.193, Lambda_2: 0.003960, Time: 0.19\n",
            "It: 620, Loss: 1.287e-02, Lambda_1: 0.199, Lambda_2: 0.004019, Time: 0.22\n",
            "It: 630, Loss: 1.261e-02, Lambda_1: 0.205, Lambda_2: 0.004078, Time: 0.21\n",
            "It: 640, Loss: 1.249e-02, Lambda_1: 0.211, Lambda_2: 0.004139, Time: 0.22\n",
            "It: 650, Loss: 1.234e-02, Lambda_1: 0.217, Lambda_2: 0.004201, Time: 0.21\n",
            "It: 660, Loss: 1.229e-02, Lambda_1: 0.223, Lambda_2: 0.004263, Time: 0.20\n",
            "It: 670, Loss: 1.209e-02, Lambda_1: 0.229, Lambda_2: 0.004327, Time: 0.20\n",
            "It: 680, Loss: 1.197e-02, Lambda_1: 0.235, Lambda_2: 0.004392, Time: 0.21\n",
            "It: 690, Loss: 1.185e-02, Lambda_1: 0.241, Lambda_2: 0.004457, Time: 0.21\n",
            "It: 700, Loss: 1.180e-02, Lambda_1: 0.247, Lambda_2: 0.004524, Time: 0.21\n",
            "It: 710, Loss: 1.161e-02, Lambda_1: 0.252, Lambda_2: 0.004592, Time: 0.21\n",
            "It: 720, Loss: 1.148e-02, Lambda_1: 0.258, Lambda_2: 0.004662, Time: 0.19\n",
            "It: 730, Loss: 1.137e-02, Lambda_1: 0.264, Lambda_2: 0.004733, Time: 0.22\n",
            "It: 740, Loss: 1.126e-02, Lambda_1: 0.270, Lambda_2: 0.004805, Time: 0.21\n",
            "It: 750, Loss: 1.126e-02, Lambda_1: 0.276, Lambda_2: 0.004878, Time: 0.21\n",
            "It: 760, Loss: 1.103e-02, Lambda_1: 0.281, Lambda_2: 0.004952, Time: 0.21\n",
            "It: 770, Loss: 1.093e-02, Lambda_1: 0.287, Lambda_2: 0.005028, Time: 0.19\n",
            "It: 780, Loss: 1.082e-02, Lambda_1: 0.292, Lambda_2: 0.005105, Time: 0.22\n",
            "It: 790, Loss: 1.075e-02, Lambda_1: 0.298, Lambda_2: 0.005184, Time: 0.21\n",
            "It: 800, Loss: 1.065e-02, Lambda_1: 0.304, Lambda_2: 0.005264, Time: 0.21\n",
            "It: 810, Loss: 1.052e-02, Lambda_1: 0.309, Lambda_2: 0.005345, Time: 0.19\n",
            "It: 820, Loss: 1.039e-02, Lambda_1: 0.315, Lambda_2: 0.005427, Time: 0.20\n",
            "It: 830, Loss: 1.027e-02, Lambda_1: 0.320, Lambda_2: 0.005510, Time: 0.23\n",
            "It: 840, Loss: 1.017e-02, Lambda_1: 0.326, Lambda_2: 0.005595, Time: 0.20\n",
            "It: 850, Loss: 1.009e-02, Lambda_1: 0.331, Lambda_2: 0.005681, Time: 0.19\n",
            "It: 860, Loss: 9.988e-03, Lambda_1: 0.337, Lambda_2: 0.005766, Time: 0.19\n",
            "It: 870, Loss: 9.903e-03, Lambda_1: 0.342, Lambda_2: 0.005854, Time: 0.22\n",
            "It: 880, Loss: 9.756e-03, Lambda_1: 0.348, Lambda_2: 0.005943, Time: 0.20\n",
            "It: 890, Loss: 9.647e-03, Lambda_1: 0.353, Lambda_2: 0.006033, Time: 0.20\n",
            "It: 900, Loss: 9.544e-03, Lambda_1: 0.359, Lambda_2: 0.006124, Time: 0.21\n",
            "It: 910, Loss: 9.440e-03, Lambda_1: 0.364, Lambda_2: 0.006217, Time: 0.20\n",
            "It: 920, Loss: 9.453e-03, Lambda_1: 0.370, Lambda_2: 0.006312, Time: 0.22\n",
            "It: 930, Loss: 9.370e-03, Lambda_1: 0.375, Lambda_2: 0.006407, Time: 0.20\n",
            "It: 940, Loss: 9.164e-03, Lambda_1: 0.381, Lambda_2: 0.006504, Time: 0.21\n",
            "It: 950, Loss: 9.266e-03, Lambda_1: 0.386, Lambda_2: 0.006602, Time: 0.21\n",
            "It: 960, Loss: 9.028e-03, Lambda_1: 0.392, Lambda_2: 0.006700, Time: 0.21\n",
            "It: 970, Loss: 8.821e-03, Lambda_1: 0.397, Lambda_2: 0.006800, Time: 0.21\n",
            "It: 980, Loss: 8.725e-03, Lambda_1: 0.403, Lambda_2: 0.006902, Time: 0.21\n",
            "It: 990, Loss: 8.625e-03, Lambda_1: 0.408, Lambda_2: 0.007006, Time: 0.22\n",
            "It: 1000, Loss: 8.537e-03, Lambda_1: 0.414, Lambda_2: 0.007111, Time: 0.21\n",
            "It: 1010, Loss: 8.457e-03, Lambda_1: 0.419, Lambda_2: 0.007217, Time: 0.20\n",
            "It: 1020, Loss: 8.329e-03, Lambda_1: 0.424, Lambda_2: 0.007324, Time: 0.21\n",
            "It: 1030, Loss: 8.218e-03, Lambda_1: 0.429, Lambda_2: 0.007433, Time: 0.20\n",
            "It: 1040, Loss: 8.115e-03, Lambda_1: 0.435, Lambda_2: 0.007544, Time: 0.22\n",
            "It: 1050, Loss: 8.022e-03, Lambda_1: 0.440, Lambda_2: 0.007657, Time: 0.22\n",
            "It: 1060, Loss: 7.911e-03, Lambda_1: 0.446, Lambda_2: 0.007772, Time: 0.22\n",
            "It: 1070, Loss: 7.811e-03, Lambda_1: 0.451, Lambda_2: 0.007889, Time: 0.21\n",
            "It: 1080, Loss: 7.863e-03, Lambda_1: 0.457, Lambda_2: 0.008008, Time: 0.20\n",
            "It: 1090, Loss: 8.085e-03, Lambda_1: 0.462, Lambda_2: 0.008126, Time: 0.21\n",
            "It: 1100, Loss: 7.692e-03, Lambda_1: 0.467, Lambda_2: 0.008247, Time: 0.21\n",
            "It: 1110, Loss: 7.483e-03, Lambda_1: 0.472, Lambda_2: 0.008369, Time: 0.23\n",
            "It: 1120, Loss: 7.340e-03, Lambda_1: 0.477, Lambda_2: 0.008494, Time: 0.21\n",
            "It: 1130, Loss: 7.214e-03, Lambda_1: 0.483, Lambda_2: 0.008621, Time: 0.22\n",
            "It: 1140, Loss: 7.114e-03, Lambda_1: 0.488, Lambda_2: 0.008750, Time: 0.21\n",
            "It: 1150, Loss: 7.019e-03, Lambda_1: 0.494, Lambda_2: 0.008882, Time: 0.20\n",
            "It: 1160, Loss: 7.084e-03, Lambda_1: 0.499, Lambda_2: 0.009015, Time: 0.21\n",
            "It: 1170, Loss: 7.614e-03, Lambda_1: 0.504, Lambda_2: 0.009147, Time: 0.20\n",
            "It: 1180, Loss: 6.918e-03, Lambda_1: 0.509, Lambda_2: 0.009279, Time: 0.22\n",
            "It: 1190, Loss: 6.655e-03, Lambda_1: 0.513, Lambda_2: 0.009415, Time: 0.22\n",
            "It: 1200, Loss: 6.585e-03, Lambda_1: 0.518, Lambda_2: 0.009552, Time: 0.23\n",
            "It: 1210, Loss: 6.458e-03, Lambda_1: 0.523, Lambda_2: 0.009691, Time: 0.22\n",
            "It: 1220, Loss: 6.355e-03, Lambda_1: 0.529, Lambda_2: 0.009832, Time: 0.22\n",
            "It: 1230, Loss: 6.260e-03, Lambda_1: 0.534, Lambda_2: 0.009976, Time: 0.22\n",
            "It: 1240, Loss: 6.164e-03, Lambda_1: 0.539, Lambda_2: 0.010121, Time: 0.21\n",
            "It: 1250, Loss: 6.070e-03, Lambda_1: 0.544, Lambda_2: 0.010269, Time: 0.23\n",
            "It: 1260, Loss: 8.451e-03, Lambda_1: 0.549, Lambda_2: 0.010419, Time: 0.21\n",
            "It: 1270, Loss: 6.246e-03, Lambda_1: 0.553, Lambda_2: 0.010558, Time: 0.21\n",
            "It: 1280, Loss: 6.003e-03, Lambda_1: 0.556, Lambda_2: 0.010709, Time: 0.23\n",
            "It: 1290, Loss: 5.882e-03, Lambda_1: 0.560, Lambda_2: 0.010859, Time: 0.22\n",
            "It: 1300, Loss: 5.663e-03, Lambda_1: 0.564, Lambda_2: 0.011009, Time: 0.20\n",
            "It: 1310, Loss: 5.595e-03, Lambda_1: 0.569, Lambda_2: 0.011161, Time: 0.22\n",
            "It: 1320, Loss: 5.494e-03, Lambda_1: 0.574, Lambda_2: 0.011316, Time: 0.20\n",
            "It: 1330, Loss: 5.403e-03, Lambda_1: 0.579, Lambda_2: 0.011472, Time: 0.20\n",
            "It: 1340, Loss: 5.318e-03, Lambda_1: 0.584, Lambda_2: 0.011631, Time: 0.22\n",
            "It: 1350, Loss: 5.232e-03, Lambda_1: 0.588, Lambda_2: 0.011792, Time: 0.21\n",
            "It: 1360, Loss: 5.148e-03, Lambda_1: 0.593, Lambda_2: 0.011954, Time: 0.22\n",
            "It: 1370, Loss: 5.064e-03, Lambda_1: 0.598, Lambda_2: 0.012118, Time: 0.21\n",
            "It: 1380, Loss: 4.980e-03, Lambda_1: 0.603, Lambda_2: 0.012284, Time: 0.20\n",
            "It: 1390, Loss: 4.896e-03, Lambda_1: 0.607, Lambda_2: 0.012451, Time: 0.21\n",
            "It: 1400, Loss: 4.813e-03, Lambda_1: 0.612, Lambda_2: 0.012620, Time: 0.21\n",
            "It: 1410, Loss: 4.731e-03, Lambda_1: 0.616, Lambda_2: 0.012791, Time: 0.20\n",
            "It: 1420, Loss: 5.086e-03, Lambda_1: 0.621, Lambda_2: 0.012963, Time: 0.20\n",
            "It: 1430, Loss: 4.707e-03, Lambda_1: 0.624, Lambda_2: 0.013125, Time: 0.22\n",
            "It: 1440, Loss: 4.747e-03, Lambda_1: 0.626, Lambda_2: 0.013296, Time: 0.20\n",
            "It: 1450, Loss: 4.535e-03, Lambda_1: 0.629, Lambda_2: 0.013465, Time: 0.20\n",
            "It: 1460, Loss: 4.413e-03, Lambda_1: 0.632, Lambda_2: 0.013635, Time: 0.20\n",
            "It: 1470, Loss: 4.321e-03, Lambda_1: 0.636, Lambda_2: 0.013807, Time: 0.18\n",
            "It: 1480, Loss: 4.229e-03, Lambda_1: 0.641, Lambda_2: 0.013979, Time: 0.22\n",
            "It: 1490, Loss: 4.148e-03, Lambda_1: 0.645, Lambda_2: 0.014151, Time: 0.20\n",
            "It: 1500, Loss: 4.074e-03, Lambda_1: 0.650, Lambda_2: 0.014326, Time: 0.20\n",
            "It: 1510, Loss: 4.000e-03, Lambda_1: 0.654, Lambda_2: 0.014502, Time: 0.22\n",
            "It: 1520, Loss: 3.927e-03, Lambda_1: 0.658, Lambda_2: 0.014679, Time: 0.20\n",
            "It: 1530, Loss: 3.854e-03, Lambda_1: 0.662, Lambda_2: 0.014857, Time: 0.21\n",
            "It: 1540, Loss: 3.783e-03, Lambda_1: 0.666, Lambda_2: 0.015037, Time: 0.21\n",
            "It: 1550, Loss: 3.711e-03, Lambda_1: 0.671, Lambda_2: 0.015217, Time: 0.22\n",
            "It: 1560, Loss: 3.641e-03, Lambda_1: 0.675, Lambda_2: 0.015398, Time: 0.21\n",
            "It: 1570, Loss: 3.571e-03, Lambda_1: 0.679, Lambda_2: 0.015581, Time: 0.20\n",
            "It: 1580, Loss: 3.501e-03, Lambda_1: 0.683, Lambda_2: 0.015764, Time: 0.22\n",
            "It: 1590, Loss: 3.433e-03, Lambda_1: 0.687, Lambda_2: 0.015948, Time: 0.21\n",
            "It: 1600, Loss: 3.375e-03, Lambda_1: 0.691, Lambda_2: 0.016132, Time: 0.20\n",
            "It: 1610, Loss: 6.913e-03, Lambda_1: 0.694, Lambda_2: 0.016315, Time: 0.20\n",
            "It: 1620, Loss: 3.265e-03, Lambda_1: 0.696, Lambda_2: 0.016495, Time: 0.22\n",
            "It: 1630, Loss: 3.353e-03, Lambda_1: 0.698, Lambda_2: 0.016684, Time: 0.21\n",
            "It: 1640, Loss: 3.248e-03, Lambda_1: 0.700, Lambda_2: 0.016866, Time: 0.19\n",
            "It: 1650, Loss: 3.119e-03, Lambda_1: 0.703, Lambda_2: 0.017051, Time: 0.21\n",
            "It: 1660, Loss: 3.017e-03, Lambda_1: 0.707, Lambda_2: 0.017232, Time: 0.20\n",
            "It: 1670, Loss: 2.961e-03, Lambda_1: 0.711, Lambda_2: 0.017412, Time: 0.21\n",
            "It: 1680, Loss: 2.897e-03, Lambda_1: 0.715, Lambda_2: 0.017593, Time: 0.20\n",
            "It: 1690, Loss: 2.840e-03, Lambda_1: 0.719, Lambda_2: 0.017774, Time: 0.22\n",
            "It: 1700, Loss: 2.782e-03, Lambda_1: 0.723, Lambda_2: 0.017955, Time: 0.19\n",
            "It: 1710, Loss: 2.725e-03, Lambda_1: 0.727, Lambda_2: 0.018137, Time: 0.19\n",
            "It: 1720, Loss: 2.670e-03, Lambda_1: 0.731, Lambda_2: 0.018319, Time: 0.21\n",
            "It: 1730, Loss: 2.615e-03, Lambda_1: 0.734, Lambda_2: 0.018501, Time: 0.20\n",
            "It: 1740, Loss: 2.562e-03, Lambda_1: 0.738, Lambda_2: 0.018683, Time: 0.20\n",
            "It: 1750, Loss: 2.509e-03, Lambda_1: 0.741, Lambda_2: 0.018864, Time: 0.21\n",
            "It: 1760, Loss: 2.457e-03, Lambda_1: 0.745, Lambda_2: 0.019045, Time: 0.21\n",
            "It: 1770, Loss: 2.406e-03, Lambda_1: 0.748, Lambda_2: 0.019226, Time: 0.22\n",
            "It: 1780, Loss: 2.356e-03, Lambda_1: 0.752, Lambda_2: 0.019407, Time: 0.21\n",
            "It: 1790, Loss: 2.307e-03, Lambda_1: 0.755, Lambda_2: 0.019587, Time: 0.22\n",
            "It: 1800, Loss: 2.445e-03, Lambda_1: 0.758, Lambda_2: 0.019767, Time: 0.23\n",
            "It: 1810, Loss: 4.188e-03, Lambda_1: 0.760, Lambda_2: 0.019936, Time: 0.23\n",
            "It: 1820, Loss: 2.232e-03, Lambda_1: 0.761, Lambda_2: 0.020120, Time: 0.22\n",
            "It: 1830, Loss: 2.317e-03, Lambda_1: 0.762, Lambda_2: 0.020296, Time: 0.21\n",
            "It: 1840, Loss: 2.203e-03, Lambda_1: 0.764, Lambda_2: 0.020469, Time: 0.21\n",
            "It: 1850, Loss: 2.082e-03, Lambda_1: 0.767, Lambda_2: 0.020638, Time: 0.22\n",
            "It: 1860, Loss: 2.028e-03, Lambda_1: 0.770, Lambda_2: 0.020800, Time: 0.22\n",
            "It: 1870, Loss: 1.991e-03, Lambda_1: 0.773, Lambda_2: 0.020962, Time: 0.21\n",
            "It: 1880, Loss: 1.948e-03, Lambda_1: 0.777, Lambda_2: 0.021124, Time: 0.21\n",
            "It: 1890, Loss: 1.909e-03, Lambda_1: 0.780, Lambda_2: 0.021287, Time: 0.22\n",
            "It: 1900, Loss: 1.872e-03, Lambda_1: 0.783, Lambda_2: 0.021448, Time: 0.20\n",
            "It: 1910, Loss: 1.834e-03, Lambda_1: 0.786, Lambda_2: 0.021610, Time: 0.23\n",
            "It: 1920, Loss: 1.798e-03, Lambda_1: 0.789, Lambda_2: 0.021770, Time: 0.20\n",
            "It: 1930, Loss: 1.763e-03, Lambda_1: 0.792, Lambda_2: 0.021930, Time: 0.21\n",
            "It: 1940, Loss: 1.728e-03, Lambda_1: 0.795, Lambda_2: 0.022089, Time: 0.21\n",
            "It: 1950, Loss: 1.694e-03, Lambda_1: 0.798, Lambda_2: 0.022247, Time: 0.22\n",
            "It: 1960, Loss: 1.660e-03, Lambda_1: 0.800, Lambda_2: 0.022404, Time: 0.22\n",
            "It: 1970, Loss: 1.627e-03, Lambda_1: 0.803, Lambda_2: 0.022561, Time: 0.21\n",
            "It: 1980, Loss: 1.605e-03, Lambda_1: 0.806, Lambda_2: 0.022716, Time: 0.19\n",
            "It: 1990, Loss: 2.330e-03, Lambda_1: 0.808, Lambda_2: 0.022867, Time: 0.20\n",
            "Loss: 1.767026e-03, l1: 0.80959, l2: 0.02299\n",
            "Loss: 2.706304e+01, l1: 0.78211, l2: 0.02346\n",
            "Loss: 2.192689e-03, l1: 0.80948, l2: 0.02300\n",
            "Loss: 1.568226e-03, l1: 0.80955, l2: 0.02299\n",
            "Loss: 1.546507e-03, l1: 0.80953, l2: 0.02299\n",
            "Loss: 1.542888e-03, l1: 0.80953, l2: 0.02300\n",
            "Loss: 1.542595e-03, l1: 0.80954, l2: 0.02300\n",
            "Loss: 1.540950e-03, l1: 0.80965, l2: 0.02300\n",
            "Loss: 1.539995e-03, l1: 0.80973, l2: 0.02300\n",
            "Loss: 1.536023e-03, l1: 0.81020, l2: 0.02303\n",
            "Loss: 1.532953e-03, l1: 0.81103, l2: 0.02309\n",
            "Loss: 1.527862e-03, l1: 0.81122, l2: 0.02310\n",
            "Loss: 1.519700e-03, l1: 0.81198, l2: 0.02316\n",
            "Loss: 1.511815e-03, l1: 0.81307, l2: 0.02323\n",
            "Loss: 1.489945e-03, l1: 0.81663, l2: 0.02343\n",
            "Loss: 1.454616e-03, l1: 0.82416, l2: 0.02383\n",
            "Loss: 1.415399e-03, l1: 0.83659, l2: 0.02445\n",
            "Loss: 1.372006e-03, l1: 0.84183, l2: 0.02468\n",
            "Loss: 1.326518e-03, l1: 0.84949, l2: 0.02499\n",
            "Loss: 1.293047e-03, l1: 0.85606, l2: 0.02527\n",
            "Loss: 1.187400e-03, l1: 0.88122, l2: 0.02645\n",
            "Loss: 1.166901e-03, l1: 0.91848, l2: 0.02837\n",
            "Loss: 1.064213e-03, l1: 0.90974, l2: 0.02792\n",
            "Loss: 1.041113e-03, l1: 0.91237, l2: 0.02810\n",
            "Loss: 1.014906e-03, l1: 0.91568, l2: 0.02832\n",
            "Loss: 9.778700e-04, l1: 0.92466, l2: 0.02887\n",
            "Loss: 9.354872e-04, l1: 0.93226, l2: 0.02934\n",
            "Loss: 9.020276e-04, l1: 0.93914, l2: 0.02976\n",
            "Loss: 8.690476e-04, l1: 0.94155, l2: 0.02993\n",
            "Loss: 8.527151e-04, l1: 0.94664, l2: 0.03024\n",
            "Loss: 8.399455e-04, l1: 0.94663, l2: 0.03027\n",
            "Loss: 8.242701e-04, l1: 0.95074, l2: 0.03055\n",
            "Loss: 8.127677e-04, l1: 0.95427, l2: 0.03082\n",
            "Loss: 8.067592e-04, l1: 0.95585, l2: 0.03094\n",
            "Loss: 7.985117e-04, l1: 0.95805, l2: 0.03112\n",
            "Loss: 7.825466e-04, l1: 0.96061, l2: 0.03137\n",
            "Loss: 7.619649e-04, l1: 0.96407, l2: 0.03178\n",
            "Loss: 7.486504e-04, l1: 0.96300, l2: 0.03186\n",
            "Loss: 7.391747e-04, l1: 0.96280, l2: 0.03196\n",
            "Loss: 7.302114e-04, l1: 0.96148, l2: 0.03199\n",
            "Loss: 7.136194e-04, l1: 0.96025, l2: 0.03214\n",
            "Loss: 6.847524e-04, l1: 0.95336, l2: 0.03222\n",
            "Loss: 6.658810e-04, l1: 0.94831, l2: 0.03227\n",
            "Loss: 6.427396e-04, l1: 0.94257, l2: 0.03226\n",
            "Loss: 5.989599e-04, l1: 0.93563, l2: 0.03249\n",
            "Loss: 5.615309e-04, l1: 0.93345, l2: 0.03300\n",
            "Loss: 5.353350e-04, l1: 0.93389, l2: 0.03346\n",
            "Loss: 5.207295e-04, l1: 0.93681, l2: 0.03397\n",
            "Loss: 5.114319e-04, l1: 0.93714, l2: 0.03409\n",
            "Loss: 5.052629e-04, l1: 0.94106, l2: 0.03443\n",
            "Loss: 4.977226e-04, l1: 0.93890, l2: 0.03423\n",
            "Loss: 4.887501e-04, l1: 0.93575, l2: 0.03393\n",
            "Loss: 4.810481e-04, l1: 0.93217, l2: 0.03372\n",
            "Loss: 4.698993e-04, l1: 0.92544, l2: 0.03351\n",
            "Loss: 4.699432e-04, l1: 0.92597, l2: 0.03373\n",
            "Loss: 4.668044e-04, l1: 0.92570, l2: 0.03362\n",
            "Loss: 4.594710e-04, l1: 0.92370, l2: 0.03368\n",
            "Loss: 4.527549e-04, l1: 0.92423, l2: 0.03390\n",
            "Loss: 4.485147e-04, l1: 0.92600, l2: 0.03409\n",
            "Loss: 4.494403e-04, l1: 0.92784, l2: 0.03427\n",
            "Loss: 4.466073e-04, l1: 0.92683, l2: 0.03417\n",
            "Loss: 4.439124e-04, l1: 0.92868, l2: 0.03428\n",
            "Loss: 4.419026e-04, l1: 0.92938, l2: 0.03434\n",
            "Loss: 4.383695e-04, l1: 0.92970, l2: 0.03434\n",
            "Loss: 4.332124e-04, l1: 0.92952, l2: 0.03437\n",
            "Loss: 4.270923e-04, l1: 0.92879, l2: 0.03437\n",
            "Loss: 4.193152e-04, l1: 0.92708, l2: 0.03442\n",
            "Loss: 4.141370e-04, l1: 0.92716, l2: 0.03442\n",
            "Loss: 4.086106e-04, l1: 0.92793, l2: 0.03443\n",
            "Loss: 4.045900e-04, l1: 0.92809, l2: 0.03438\n",
            "Loss: 3.994587e-04, l1: 0.92755, l2: 0.03431\n",
            "Loss: 4.105460e-04, l1: 0.92690, l2: 0.03403\n",
            "Loss: 3.970550e-04, l1: 0.92736, l2: 0.03423\n",
            "Loss: 3.926773e-04, l1: 0.92628, l2: 0.03416\n",
            "Loss: 3.879094e-04, l1: 0.92607, l2: 0.03412\n",
            "Loss: 3.803238e-04, l1: 0.92702, l2: 0.03408\n",
            "Loss: 3.715677e-04, l1: 0.93015, l2: 0.03402\n",
            "Loss: 3.694450e-04, l1: 0.93719, l2: 0.03418\n",
            "Loss: 3.587268e-04, l1: 0.93600, l2: 0.03400\n",
            "Loss: 3.564836e-04, l1: 0.93707, l2: 0.03414\n",
            "Loss: 3.543875e-04, l1: 0.93776, l2: 0.03410\n",
            "Loss: 3.525899e-04, l1: 0.93799, l2: 0.03410\n",
            "Loss: 3.481506e-04, l1: 0.93823, l2: 0.03398\n",
            "Loss: 3.410416e-04, l1: 0.93711, l2: 0.03389\n",
            "Loss: 3.270076e-04, l1: 0.93535, l2: 0.03366\n",
            "Loss: 3.204458e-04, l1: 0.93450, l2: 0.03353\n",
            "Loss: 3.167942e-04, l1: 0.93772, l2: 0.03357\n",
            "Loss: 3.151651e-04, l1: 0.93862, l2: 0.03355\n",
            "Loss: 3.142732e-04, l1: 0.93929, l2: 0.03356\n",
            "Loss: 3.124849e-04, l1: 0.94052, l2: 0.03355\n",
            "Loss: 3.104244e-04, l1: 0.94096, l2: 0.03350\n",
            "Loss: 3.053069e-04, l1: 0.94138, l2: 0.03334\n",
            "Loss: 2.985676e-04, l1: 0.94165, l2: 0.03315\n",
            "Loss: 2.909326e-04, l1: 0.94261, l2: 0.03298\n",
            "Loss: 2.900595e-04, l1: 0.94413, l2: 0.03271\n",
            "Loss: 2.865396e-04, l1: 0.94341, l2: 0.03284\n",
            "Loss: 2.823545e-04, l1: 0.94381, l2: 0.03288\n",
            "Loss: 2.778369e-04, l1: 0.94674, l2: 0.03305\n",
            "Loss: 2.754011e-04, l1: 0.94822, l2: 0.03308\n",
            "Loss: 2.713421e-04, l1: 0.95231, l2: 0.03310\n",
            "Loss: 2.646750e-04, l1: 0.95426, l2: 0.03295\n",
            "Loss: 2.555078e-04, l1: 0.95869, l2: 0.03271\n",
            "Loss: 2.512225e-04, l1: 0.95971, l2: 0.03259\n",
            "Loss: 2.466977e-04, l1: 0.96104, l2: 0.03259\n",
            "Loss: 2.476343e-04, l1: 0.96260, l2: 0.03253\n",
            "Loss: 2.434838e-04, l1: 0.96177, l2: 0.03256\n",
            "Loss: 2.391418e-04, l1: 0.96414, l2: 0.03271\n",
            "Loss: 2.357951e-04, l1: 0.96598, l2: 0.03287\n",
            "Loss: 2.320400e-04, l1: 0.96627, l2: 0.03278\n",
            "Loss: 2.293836e-04, l1: 0.96499, l2: 0.03264\n",
            "Loss: 2.280476e-04, l1: 0.96406, l2: 0.03245\n",
            "Loss: 2.269224e-04, l1: 0.96399, l2: 0.03240\n",
            "Loss: 2.261053e-04, l1: 0.96323, l2: 0.03230\n",
            "Loss: 2.252490e-04, l1: 0.96361, l2: 0.03229\n",
            "Loss: 2.231063e-04, l1: 0.96445, l2: 0.03225\n",
            "Loss: 2.207380e-04, l1: 0.96451, l2: 0.03221\n",
            "Loss: 2.405676e-04, l1: 0.96392, l2: 0.03169\n",
            "Loss: 2.194187e-04, l1: 0.96439, l2: 0.03210\n",
            "Loss: 2.164316e-04, l1: 0.96560, l2: 0.03204\n",
            "Loss: 2.133186e-04, l1: 0.96630, l2: 0.03203\n",
            "Loss: 2.110908e-04, l1: 0.96771, l2: 0.03197\n",
            "Loss: 2.088444e-04, l1: 0.96989, l2: 0.03198\n",
            "Loss: 2.059677e-04, l1: 0.97156, l2: 0.03199\n",
            "Loss: 2.038211e-04, l1: 0.97175, l2: 0.03200\n",
            "Loss: 2.017959e-04, l1: 0.97074, l2: 0.03200\n",
            "Loss: 2.003004e-04, l1: 0.96977, l2: 0.03197\n",
            "Loss: 1.978136e-04, l1: 0.96787, l2: 0.03190\n",
            "Loss: 2.185022e-04, l1: 0.96143, l2: 0.03166\n",
            "Loss: 1.971290e-04, l1: 0.96690, l2: 0.03186\n",
            "Loss: 1.950627e-04, l1: 0.96572, l2: 0.03180\n",
            "Loss: 1.919796e-04, l1: 0.96527, l2: 0.03178\n",
            "Loss: 1.883927e-04, l1: 0.96600, l2: 0.03177\n",
            "Loss: 1.841391e-04, l1: 0.96757, l2: 0.03176\n",
            "Loss: 1.840850e-04, l1: 0.97246, l2: 0.03170\n",
            "Loss: 1.823983e-04, l1: 0.97002, l2: 0.03173\n",
            "Loss: 1.802546e-04, l1: 0.97096, l2: 0.03172\n",
            "Loss: 1.793732e-04, l1: 0.97127, l2: 0.03171\n",
            "Loss: 1.781295e-04, l1: 0.97218, l2: 0.03166\n",
            "Loss: 1.767808e-04, l1: 0.97266, l2: 0.03161\n",
            "Loss: 1.749429e-04, l1: 0.97319, l2: 0.03151\n",
            "Loss: 1.733744e-04, l1: 0.97406, l2: 0.03145\n",
            "Loss: 1.707091e-04, l1: 0.97308, l2: 0.03145\n",
            "Loss: 1.686059e-04, l1: 0.97249, l2: 0.03145\n",
            "Loss: 1.649987e-04, l1: 0.97173, l2: 0.03147\n",
            "Loss: 1.632194e-04, l1: 0.97106, l2: 0.03150\n",
            "Loss: 1.618334e-04, l1: 0.97150, l2: 0.03142\n",
            "Loss: 1.611647e-04, l1: 0.97141, l2: 0.03139\n",
            "Loss: 1.608284e-04, l1: 0.97172, l2: 0.03135\n",
            "Loss: 1.604316e-04, l1: 0.97190, l2: 0.03134\n",
            "Loss: 1.598505e-04, l1: 0.97250, l2: 0.03132\n",
            "Loss: 1.589338e-04, l1: 0.97280, l2: 0.03133\n",
            "Loss: 1.573301e-04, l1: 0.97346, l2: 0.03137\n",
            "Loss: 1.550286e-04, l1: 0.97360, l2: 0.03145\n",
            "Loss: 1.519995e-04, l1: 0.97394, l2: 0.03163\n",
            "Loss: 1.488866e-04, l1: 0.97277, l2: 0.03166\n",
            "Loss: 1.457948e-04, l1: 0.97156, l2: 0.03162\n",
            "Loss: 1.438029e-04, l1: 0.97127, l2: 0.03156\n",
            "Loss: 1.421153e-04, l1: 0.97187, l2: 0.03151\n",
            "Loss: 1.398649e-04, l1: 0.97431, l2: 0.03148\n",
            "Loss: 1.377886e-04, l1: 0.97728, l2: 0.03151\n",
            "Loss: 1.365814e-04, l1: 0.98037, l2: 0.03159\n",
            "Loss: 1.356416e-04, l1: 0.98029, l2: 0.03163\n",
            "Loss: 1.345778e-04, l1: 0.97999, l2: 0.03165\n",
            "Loss: 1.329389e-04, l1: 0.98004, l2: 0.03163\n",
            "Loss: 1.369792e-04, l1: 0.97780, l2: 0.03184\n",
            "Loss: 1.317974e-04, l1: 0.97933, l2: 0.03170\n",
            "Loss: 1.293223e-04, l1: 0.97957, l2: 0.03157\n",
            "Loss: 1.274991e-04, l1: 0.97811, l2: 0.03142\n",
            "Loss: 1.263224e-04, l1: 0.97844, l2: 0.03131\n",
            "Loss: 1.255555e-04, l1: 0.97837, l2: 0.03130\n",
            "Loss: 1.242827e-04, l1: 0.97833, l2: 0.03131\n",
            "Loss: 1.234211e-04, l1: 0.97783, l2: 0.03134\n",
            "Loss: 1.244226e-04, l1: 0.97889, l2: 0.03133\n",
            "Loss: 1.227329e-04, l1: 0.97825, l2: 0.03133\n",
            "Loss: 1.222067e-04, l1: 0.97815, l2: 0.03134\n",
            "Loss: 1.216011e-04, l1: 0.97834, l2: 0.03135\n",
            "Loss: 1.211792e-04, l1: 0.97889, l2: 0.03137\n",
            "Loss: 1.204067e-04, l1: 0.97965, l2: 0.03138\n",
            "Loss: 1.189289e-04, l1: 0.98114, l2: 0.03142\n",
            "Loss: 1.172242e-04, l1: 0.98186, l2: 0.03146\n",
            "Loss: 1.149178e-04, l1: 0.98203, l2: 0.03149\n",
            "Loss: 1.117706e-04, l1: 0.98046, l2: 0.03163\n",
            "Loss: 1.103957e-04, l1: 0.97902, l2: 0.03157\n",
            "Loss: 1.097073e-04, l1: 0.97796, l2: 0.03154\n",
            "Loss: 1.090358e-04, l1: 0.97784, l2: 0.03153\n",
            "Loss: 1.082858e-04, l1: 0.97743, l2: 0.03149\n",
            "Loss: 1.069980e-04, l1: 0.97683, l2: 0.03140\n",
            "Loss: 1.063016e-04, l1: 0.97849, l2: 0.03132\n",
            "Loss: 1.057649e-04, l1: 0.97718, l2: 0.03124\n",
            "Loss: 1.051967e-04, l1: 0.97783, l2: 0.03126\n",
            "Loss: 1.046814e-04, l1: 0.97866, l2: 0.03125\n",
            "Loss: 1.037144e-04, l1: 0.97925, l2: 0.03129\n",
            "Loss: 1.018640e-04, l1: 0.97996, l2: 0.03135\n",
            "Loss: 9.993377e-05, l1: 0.98069, l2: 0.03145\n",
            "Loss: 9.945888e-05, l1: 0.97961, l2: 0.03160\n",
            "Loss: 9.702590e-05, l1: 0.98024, l2: 0.03158\n",
            "Loss: 9.610699e-05, l1: 0.98086, l2: 0.03158\n",
            "Loss: 9.523235e-05, l1: 0.98182, l2: 0.03160\n",
            "Loss: 9.481611e-05, l1: 0.98241, l2: 0.03155\n",
            "Loss: 9.426784e-05, l1: 0.98278, l2: 0.03158\n",
            "Loss: 9.394502e-05, l1: 0.98308, l2: 0.03156\n",
            "Loss: 9.367184e-05, l1: 0.98309, l2: 0.03154\n",
            "Loss: 9.314914e-05, l1: 0.98296, l2: 0.03150\n",
            "Loss: 9.322567e-05, l1: 0.98455, l2: 0.03146\n",
            "Loss: 9.276366e-05, l1: 0.98371, l2: 0.03148\n",
            "Loss: 9.186787e-05, l1: 0.98433, l2: 0.03150\n",
            "Loss: 9.117883e-05, l1: 0.98414, l2: 0.03152\n",
            "Loss: 9.024523e-05, l1: 0.98483, l2: 0.03155\n",
            "Loss: 8.940670e-05, l1: 0.98452, l2: 0.03157\n",
            "Loss: 8.884372e-05, l1: 0.98541, l2: 0.03162\n",
            "Loss: 8.849398e-05, l1: 0.98564, l2: 0.03163\n",
            "Loss: 8.790156e-05, l1: 0.98577, l2: 0.03164\n",
            "Loss: 8.663270e-05, l1: 0.98641, l2: 0.03165\n",
            "Loss: 8.503188e-05, l1: 0.98658, l2: 0.03164\n",
            "Loss: 8.377575e-05, l1: 0.98641, l2: 0.03149\n",
            "Loss: 8.229542e-05, l1: 0.98616, l2: 0.03155\n",
            "Loss: 8.143384e-05, l1: 0.98617, l2: 0.03153\n",
            "Loss: 8.075587e-05, l1: 0.98601, l2: 0.03154\n",
            "Loss: 8.044695e-05, l1: 0.98661, l2: 0.03155\n",
            "Loss: 8.015102e-05, l1: 0.98686, l2: 0.03157\n",
            "Loss: 7.990162e-05, l1: 0.98707, l2: 0.03159\n",
            "Loss: 7.972166e-05, l1: 0.98729, l2: 0.03161\n",
            "Loss: 7.951008e-05, l1: 0.98748, l2: 0.03161\n",
            "Loss: 7.968386e-05, l1: 0.98806, l2: 0.03167\n",
            "Loss: 7.941564e-05, l1: 0.98769, l2: 0.03163\n",
            "Loss: 7.928321e-05, l1: 0.98770, l2: 0.03162\n",
            "Loss: 7.915730e-05, l1: 0.98757, l2: 0.03162\n",
            "Loss: 7.905223e-05, l1: 0.98747, l2: 0.03161\n",
            "Loss: 7.891221e-05, l1: 0.98718, l2: 0.03162\n",
            "Loss: 7.872854e-05, l1: 0.98712, l2: 0.03159\n",
            "Loss: 7.850135e-05, l1: 0.98713, l2: 0.03159\n",
            "Loss: 7.800945e-05, l1: 0.98703, l2: 0.03162\n",
            "Loss: 7.764874e-05, l1: 0.98688, l2: 0.03162\n",
            "Loss: 7.681857e-05, l1: 0.98655, l2: 0.03168\n",
            "Loss: 7.641585e-05, l1: 0.98551, l2: 0.03164\n",
            "Loss: 7.768802e-05, l1: 0.98448, l2: 0.03158\n",
            "Loss: 7.606233e-05, l1: 0.98518, l2: 0.03162\n",
            "Loss: 7.544323e-05, l1: 0.98559, l2: 0.03164\n",
            "Loss: 7.434937e-05, l1: 0.98626, l2: 0.03167\n",
            "Loss: 7.296138e-05, l1: 0.98714, l2: 0.03168\n",
            "Loss: 7.108689e-05, l1: 0.98790, l2: 0.03168\n",
            "Loss: 6.947673e-05, l1: 0.98926, l2: 0.03167\n",
            "Loss: 6.858753e-05, l1: 0.98906, l2: 0.03170\n",
            "Loss: 6.815892e-05, l1: 0.98953, l2: 0.03172\n",
            "Loss: 6.769005e-05, l1: 0.98924, l2: 0.03172\n",
            "Loss: 6.719048e-05, l1: 0.98934, l2: 0.03174\n",
            "Loss: 6.664925e-05, l1: 0.98968, l2: 0.03175\n",
            "Loss: 6.643406e-05, l1: 0.99046, l2: 0.03175\n",
            "Loss: 6.627620e-05, l1: 0.99052, l2: 0.03174\n",
            "Loss: 6.609297e-05, l1: 0.99056, l2: 0.03172\n",
            "Loss: 6.595792e-05, l1: 0.99067, l2: 0.03170\n",
            "Loss: 6.576465e-05, l1: 0.99091, l2: 0.03169\n",
            "Loss: 6.574851e-05, l1: 0.99184, l2: 0.03169\n",
            "Loss: 6.566793e-05, l1: 0.99139, l2: 0.03169\n",
            "Loss: 6.556946e-05, l1: 0.99167, l2: 0.03170\n",
            "Loss: 6.546499e-05, l1: 0.99203, l2: 0.03173\n",
            "Loss: 6.529388e-05, l1: 0.99236, l2: 0.03177\n",
            "Loss: 6.501973e-05, l1: 0.99268, l2: 0.03181\n",
            "Loss: 6.442735e-05, l1: 0.99281, l2: 0.03189\n",
            "Loss: 6.395046e-05, l1: 0.99308, l2: 0.03190\n",
            "Loss: 6.328952e-05, l1: 0.99306, l2: 0.03194\n",
            "Loss: 6.273793e-05, l1: 0.99295, l2: 0.03192\n",
            "Loss: 6.167765e-05, l1: 0.99330, l2: 0.03186\n",
            "Loss: 6.113060e-05, l1: 0.99383, l2: 0.03185\n",
            "Loss: 6.018084e-05, l1: 0.99534, l2: 0.03189\n",
            "Loss: 5.951836e-05, l1: 0.99558, l2: 0.03189\n",
            "Loss: 5.908195e-05, l1: 0.99554, l2: 0.03189\n",
            "Loss: 5.856476e-05, l1: 0.99511, l2: 0.03189\n",
            "Loss: 5.806341e-05, l1: 0.99452, l2: 0.03186\n",
            "Loss: 5.727846e-05, l1: 0.99363, l2: 0.03186\n",
            "Loss: 5.747467e-05, l1: 0.99143, l2: 0.03169\n",
            "Loss: 5.683665e-05, l1: 0.99264, l2: 0.03178\n",
            "Loss: 5.631661e-05, l1: 0.99256, l2: 0.03181\n",
            "Loss: 5.506551e-05, l1: 0.99146, l2: 0.03187\n",
            "Loss: 5.502686e-05, l1: 0.99175, l2: 0.03198\n",
            "Loss: 5.451071e-05, l1: 0.99159, l2: 0.03192\n",
            "Loss: 5.396624e-05, l1: 0.99107, l2: 0.03196\n",
            "Loss: 5.359000e-05, l1: 0.99125, l2: 0.03199\n",
            "Loss: 5.312808e-05, l1: 0.99150, l2: 0.03203\n",
            "Loss: 5.267575e-05, l1: 0.99206, l2: 0.03203\n",
            "Loss: 5.230356e-05, l1: 0.99266, l2: 0.03206\n",
            "Loss: 5.202973e-05, l1: 0.99279, l2: 0.03204\n",
            "Loss: 5.171186e-05, l1: 0.99278, l2: 0.03203\n",
            "Loss: 5.144448e-05, l1: 0.99244, l2: 0.03203\n",
            "Loss: 5.122018e-05, l1: 0.99205, l2: 0.03205\n",
            "Loss: 5.113692e-05, l1: 0.99155, l2: 0.03207\n",
            "Loss: 5.089373e-05, l1: 0.99152, l2: 0.03207\n",
            "Loss: 5.063442e-05, l1: 0.99135, l2: 0.03206\n",
            "Loss: 5.033113e-05, l1: 0.99090, l2: 0.03203\n",
            "Loss: 5.005298e-05, l1: 0.99019, l2: 0.03199\n",
            "Loss: 5.124660e-05, l1: 0.99007, l2: 0.03197\n",
            "Loss: 4.999818e-05, l1: 0.99017, l2: 0.03198\n",
            "Loss: 4.978458e-05, l1: 0.98954, l2: 0.03193\n",
            "Loss: 4.939334e-05, l1: 0.98877, l2: 0.03191\n",
            "Loss: 4.890503e-05, l1: 0.98795, l2: 0.03188\n",
            "Loss: 4.955220e-05, l1: 0.98703, l2: 0.03197\n",
            "Loss: 4.863867e-05, l1: 0.98762, l2: 0.03191\n",
            "Loss: 4.821432e-05, l1: 0.98779, l2: 0.03191\n",
            "Loss: 4.796001e-05, l1: 0.98815, l2: 0.03194\n",
            "Loss: 4.767560e-05, l1: 0.98857, l2: 0.03195\n",
            "Loss: 4.762682e-05, l1: 0.98954, l2: 0.03195\n",
            "Loss: 4.720611e-05, l1: 0.98921, l2: 0.03193\n",
            "Loss: 4.706935e-05, l1: 0.98882, l2: 0.03190\n",
            "Loss: 4.688772e-05, l1: 0.98896, l2: 0.03188\n",
            "Loss: 4.670181e-05, l1: 0.98921, l2: 0.03184\n",
            "Loss: 4.657238e-05, l1: 0.98941, l2: 0.03183\n",
            "Loss: 4.660030e-05, l1: 0.98945, l2: 0.03180\n",
            "Loss: 4.650424e-05, l1: 0.98943, l2: 0.03182\n",
            "Loss: 4.639421e-05, l1: 0.98945, l2: 0.03183\n",
            "Loss: 4.631893e-05, l1: 0.98932, l2: 0.03184\n",
            "Loss: 4.618317e-05, l1: 0.98929, l2: 0.03187\n",
            "Loss: 4.598056e-05, l1: 0.98927, l2: 0.03188\n",
            "Loss: 4.562553e-05, l1: 0.98950, l2: 0.03189\n",
            "Loss: 4.514620e-05, l1: 0.98900, l2: 0.03188\n",
            "Loss: 4.454631e-05, l1: 0.99001, l2: 0.03185\n",
            "Loss: 4.408824e-05, l1: 0.99066, l2: 0.03179\n",
            "Loss: 4.377955e-05, l1: 0.99103, l2: 0.03177\n",
            "Loss: 4.355321e-05, l1: 0.99123, l2: 0.03174\n",
            "Loss: 4.328828e-05, l1: 0.99144, l2: 0.03174\n",
            "Loss: 4.288665e-05, l1: 0.99163, l2: 0.03176\n",
            "Loss: 4.242108e-05, l1: 0.99210, l2: 0.03178\n",
            "Loss: 4.211087e-05, l1: 0.99301, l2: 0.03184\n",
            "Loss: 4.170200e-05, l1: 0.99295, l2: 0.03185\n",
            "Loss: 4.146660e-05, l1: 0.99293, l2: 0.03183\n",
            "Loss: 4.124552e-05, l1: 0.99294, l2: 0.03183\n",
            "Loss: 4.100561e-05, l1: 0.99328, l2: 0.03182\n",
            "Loss: 4.155919e-05, l1: 0.99421, l2: 0.03191\n",
            "Loss: 4.091730e-05, l1: 0.99353, l2: 0.03184\n",
            "Loss: 4.071173e-05, l1: 0.99388, l2: 0.03186\n",
            "Loss: 4.052764e-05, l1: 0.99392, l2: 0.03187\n",
            "Loss: 4.040727e-05, l1: 0.99365, l2: 0.03188\n",
            "Loss: 4.029695e-05, l1: 0.99331, l2: 0.03189\n",
            "Loss: 4.017946e-05, l1: 0.99292, l2: 0.03190\n",
            "Loss: 4.001598e-05, l1: 0.99279, l2: 0.03189\n",
            "Loss: 3.980565e-05, l1: 0.99282, l2: 0.03190\n",
            "Loss: 3.959148e-05, l1: 0.99291, l2: 0.03189\n",
            "Loss: 3.938191e-05, l1: 0.99309, l2: 0.03192\n",
            "Loss: 3.922360e-05, l1: 0.99323, l2: 0.03193\n",
            "Loss: 3.896589e-05, l1: 0.99324, l2: 0.03195\n",
            "Loss: 3.929983e-05, l1: 0.99391, l2: 0.03205\n",
            "Loss: 3.883387e-05, l1: 0.99347, l2: 0.03199\n",
            "Loss: 3.860406e-05, l1: 0.99324, l2: 0.03198\n",
            "Loss: 3.846364e-05, l1: 0.99299, l2: 0.03197\n",
            "Loss: 3.835058e-05, l1: 0.99279, l2: 0.03196\n",
            "Loss: 3.824324e-05, l1: 0.99259, l2: 0.03195\n",
            "Loss: 4.187217e-05, l1: 0.99532, l2: 0.03204\n",
            "Loss: 3.819227e-05, l1: 0.99288, l2: 0.03196\n",
            "Loss: 3.801024e-05, l1: 0.99280, l2: 0.03195\n",
            "Loss: 3.789629e-05, l1: 0.99289, l2: 0.03190\n",
            "Loss: 3.766217e-05, l1: 0.99312, l2: 0.03191\n",
            "Loss: 3.757573e-05, l1: 0.99320, l2: 0.03191\n",
            "Loss: 3.745861e-05, l1: 0.99332, l2: 0.03190\n",
            "Loss: 3.735070e-05, l1: 0.99346, l2: 0.03188\n",
            "Loss: 3.718884e-05, l1: 0.99341, l2: 0.03186\n",
            "Loss: 3.693199e-05, l1: 0.99349, l2: 0.03183\n",
            "Loss: 3.668448e-05, l1: 0.99335, l2: 0.03183\n",
            "Loss: 3.642623e-05, l1: 0.99335, l2: 0.03183\n",
            "Loss: 3.622015e-05, l1: 0.99351, l2: 0.03182\n",
            "Loss: 3.584540e-05, l1: 0.99392, l2: 0.03177\n",
            "Loss: 3.583027e-05, l1: 0.99424, l2: 0.03169\n",
            "Loss: 3.564910e-05, l1: 0.99408, l2: 0.03173\n",
            "Loss: 3.542409e-05, l1: 0.99439, l2: 0.03169\n",
            "Loss: 3.527106e-05, l1: 0.99457, l2: 0.03168\n",
            "Loss: 3.512983e-05, l1: 0.99464, l2: 0.03169\n",
            "Loss: 3.494821e-05, l1: 0.99447, l2: 0.03171\n",
            "Loss: 3.478380e-05, l1: 0.99453, l2: 0.03176\n",
            "Loss: 3.465877e-05, l1: 0.99443, l2: 0.03176\n",
            "Loss: 3.454823e-05, l1: 0.99451, l2: 0.03178\n",
            "Loss: 3.447512e-05, l1: 0.99470, l2: 0.03177\n",
            "Loss: 3.442757e-05, l1: 0.99478, l2: 0.03177\n",
            "Loss: 3.431686e-05, l1: 0.99500, l2: 0.03177\n",
            "Loss: 3.409900e-05, l1: 0.99561, l2: 0.03177\n",
            "Loss: 3.397380e-05, l1: 0.99592, l2: 0.03178\n",
            "Loss: 3.385695e-05, l1: 0.99594, l2: 0.03181\n",
            "Loss: 3.380530e-05, l1: 0.99562, l2: 0.03182\n",
            "Loss: 3.373338e-05, l1: 0.99563, l2: 0.03182\n",
            "Loss: 3.368005e-05, l1: 0.99559, l2: 0.03183\n",
            "Loss: 3.363983e-05, l1: 0.99552, l2: 0.03182\n",
            "Loss: 3.358554e-05, l1: 0.99553, l2: 0.03183\n",
            "Loss: 3.354334e-05, l1: 0.99565, l2: 0.03184\n",
            "Loss: 3.347983e-05, l1: 0.99570, l2: 0.03185\n",
            "Loss: 3.335876e-05, l1: 0.99571, l2: 0.03185\n",
            "Loss: 3.397837e-05, l1: 0.99503, l2: 0.03178\n",
            "Loss: 3.330759e-05, l1: 0.99557, l2: 0.03184\n",
            "Loss: 3.319103e-05, l1: 0.99533, l2: 0.03183\n",
            "Loss: 3.293275e-05, l1: 0.99482, l2: 0.03181\n",
            "Loss: 3.274658e-05, l1: 0.99424, l2: 0.03179\n",
            "Loss: 3.263222e-05, l1: 0.99415, l2: 0.03178\n",
            "Loss: 3.251328e-05, l1: 0.99431, l2: 0.03178\n",
            "Loss: 3.238201e-05, l1: 0.99454, l2: 0.03179\n",
            "Loss: 3.223767e-05, l1: 0.99491, l2: 0.03180\n",
            "Loss: 3.215442e-05, l1: 0.99501, l2: 0.03181\n",
            "Loss: 3.208437e-05, l1: 0.99508, l2: 0.03182\n",
            "Loss: 3.200909e-05, l1: 0.99501, l2: 0.03183\n",
            "Loss: 3.195817e-05, l1: 0.99492, l2: 0.03184\n",
            "Loss: 3.189301e-05, l1: 0.99480, l2: 0.03186\n",
            "Loss: 3.183705e-05, l1: 0.99455, l2: 0.03185\n",
            "Loss: 3.193727e-05, l1: 0.99472, l2: 0.03193\n",
            "Loss: 3.176279e-05, l1: 0.99462, l2: 0.03188\n",
            "Loss: 3.168601e-05, l1: 0.99462, l2: 0.03187\n",
            "Loss: 3.158099e-05, l1: 0.99462, l2: 0.03185\n",
            "Loss: 3.153717e-05, l1: 0.99459, l2: 0.03185\n",
            "Loss: 3.133350e-05, l1: 0.99448, l2: 0.03185\n",
            "Loss: 3.108709e-05, l1: 0.99445, l2: 0.03186\n",
            "Loss: 3.089859e-05, l1: 0.99458, l2: 0.03189\n",
            "Loss: 3.081012e-05, l1: 0.99468, l2: 0.03191\n",
            "Loss: 3.069204e-05, l1: 0.99474, l2: 0.03191\n",
            "Loss: 3.061301e-05, l1: 0.99485, l2: 0.03191\n",
            "Loss: 3.049383e-05, l1: 0.99500, l2: 0.03191\n",
            "Loss: 3.034425e-05, l1: 0.99529, l2: 0.03191\n",
            "Loss: 3.027271e-05, l1: 0.99527, l2: 0.03190\n",
            "Loss: 3.022497e-05, l1: 0.99530, l2: 0.03189\n",
            "Loss: 3.019505e-05, l1: 0.99520, l2: 0.03190\n",
            "Loss: 3.016937e-05, l1: 0.99507, l2: 0.03189\n",
            "Loss: 3.018107e-05, l1: 0.99486, l2: 0.03189\n",
            "Loss: 3.015377e-05, l1: 0.99498, l2: 0.03189\n",
            "Loss: 3.011739e-05, l1: 0.99483, l2: 0.03189\n",
            "Loss: 3.007900e-05, l1: 0.99470, l2: 0.03187\n",
            "Loss: 3.005423e-05, l1: 0.99466, l2: 0.03187\n",
            "Loss: 3.002348e-05, l1: 0.99464, l2: 0.03187\n",
            "Loss: 2.997360e-05, l1: 0.99467, l2: 0.03187\n",
            "Loss: 2.990236e-05, l1: 0.99461, l2: 0.03189\n",
            "Loss: 2.981279e-05, l1: 0.99472, l2: 0.03190\n",
            "Loss: 2.975208e-05, l1: 0.99469, l2: 0.03190\n",
            "Loss: 2.962466e-05, l1: 0.99470, l2: 0.03191\n",
            "Loss: 3.023989e-05, l1: 0.99419, l2: 0.03187\n",
            "Loss: 2.960365e-05, l1: 0.99462, l2: 0.03190\n",
            "Loss: 2.954109e-05, l1: 0.99466, l2: 0.03190\n",
            "Loss: 2.947959e-05, l1: 0.99473, l2: 0.03190\n",
            "Loss: 2.941121e-05, l1: 0.99484, l2: 0.03189\n",
            "Loss: 2.932967e-05, l1: 0.99495, l2: 0.03189\n",
            "Loss: 2.923693e-05, l1: 0.99502, l2: 0.03188\n",
            "Loss: 2.911145e-05, l1: 0.99496, l2: 0.03188\n",
            "Loss: 2.911787e-05, l1: 0.99501, l2: 0.03187\n",
            "Loss: 2.905707e-05, l1: 0.99498, l2: 0.03188\n",
            "Loss: 2.893217e-05, l1: 0.99474, l2: 0.03189\n",
            "Loss: 2.885407e-05, l1: 0.99470, l2: 0.03189\n",
            "Loss: 2.866140e-05, l1: 0.99459, l2: 0.03186\n",
            "Loss: 2.852970e-05, l1: 0.99450, l2: 0.03184\n",
            "Loss: 2.841126e-05, l1: 0.99439, l2: 0.03179\n",
            "Loss: 2.833784e-05, l1: 0.99448, l2: 0.03178\n",
            "Loss: 2.827597e-05, l1: 0.99456, l2: 0.03178\n",
            "Loss: 2.820686e-05, l1: 0.99471, l2: 0.03179\n",
            "Loss: 2.813312e-05, l1: 0.99479, l2: 0.03179\n",
            "Loss: 2.803840e-05, l1: 0.99500, l2: 0.03181\n",
            "Loss: 2.795948e-05, l1: 0.99495, l2: 0.03182\n",
            "Loss: 2.786130e-05, l1: 0.99473, l2: 0.03184\n",
            "Loss: 2.780455e-05, l1: 0.99460, l2: 0.03186\n",
            "Loss: 2.775215e-05, l1: 0.99450, l2: 0.03185\n",
            "Loss: 2.771977e-05, l1: 0.99444, l2: 0.03185\n",
            "Loss: 2.768853e-05, l1: 0.99441, l2: 0.03185\n",
            "Loss: 2.762699e-05, l1: 0.99438, l2: 0.03187\n",
            "Loss: 2.753449e-05, l1: 0.99438, l2: 0.03190\n",
            "Loss: 2.743692e-05, l1: 0.99433, l2: 0.03194\n",
            "Loss: 2.743963e-05, l1: 0.99432, l2: 0.03194\n",
            "Loss: 2.737994e-05, l1: 0.99433, l2: 0.03194\n",
            "Loss: 2.729716e-05, l1: 0.99434, l2: 0.03197\n",
            "Loss: 2.721841e-05, l1: 0.99432, l2: 0.03196\n",
            "Loss: 2.707898e-05, l1: 0.99420, l2: 0.03195\n",
            "Loss: 2.689324e-05, l1: 0.99399, l2: 0.03193\n",
            "Loss: 2.668558e-05, l1: 0.99365, l2: 0.03190\n",
            "Loss: 2.652871e-05, l1: 0.99347, l2: 0.03188\n",
            "Loss: 2.644273e-05, l1: 0.99332, l2: 0.03188\n",
            "Loss: 2.636492e-05, l1: 0.99349, l2: 0.03189\n",
            "Loss: 2.630153e-05, l1: 0.99365, l2: 0.03189\n",
            "Loss: 2.619429e-05, l1: 0.99381, l2: 0.03189\n",
            "Loss: 2.604002e-05, l1: 0.99400, l2: 0.03188\n",
            "Loss: 2.590209e-05, l1: 0.99421, l2: 0.03187\n",
            "Loss: 2.573535e-05, l1: 0.99442, l2: 0.03186\n",
            "Loss: 2.556215e-05, l1: 0.99470, l2: 0.03186\n",
            "Loss: 2.540371e-05, l1: 0.99504, l2: 0.03186\n",
            "Loss: 2.524210e-05, l1: 0.99530, l2: 0.03186\n",
            "Loss: 2.532923e-05, l1: 0.99596, l2: 0.03186\n",
            "Loss: 2.515239e-05, l1: 0.99557, l2: 0.03186\n",
            "Loss: 2.502213e-05, l1: 0.99565, l2: 0.03186\n",
            "Loss: 2.489351e-05, l1: 0.99549, l2: 0.03185\n",
            "Loss: 2.477488e-05, l1: 0.99540, l2: 0.03185\n",
            "Loss: 2.466889e-05, l1: 0.99513, l2: 0.03183\n",
            "Loss: 2.457716e-05, l1: 0.99514, l2: 0.03184\n",
            "Loss: 2.452618e-05, l1: 0.99528, l2: 0.03185\n",
            "Loss: 2.445522e-05, l1: 0.99532, l2: 0.03185\n",
            "Loss: 2.434755e-05, l1: 0.99533, l2: 0.03185\n",
            "Loss: 2.447667e-05, l1: 0.99484, l2: 0.03181\n",
            "Loss: 2.430325e-05, l1: 0.99517, l2: 0.03184\n",
            "Loss: 2.415055e-05, l1: 0.99505, l2: 0.03184\n",
            "Loss: 2.397194e-05, l1: 0.99473, l2: 0.03182\n",
            "Loss: 2.381288e-05, l1: 0.99429, l2: 0.03179\n",
            "Loss: 2.362761e-05, l1: 0.99393, l2: 0.03178\n",
            "Loss: 2.339731e-05, l1: 0.99348, l2: 0.03176\n",
            "Loss: 2.322020e-05, l1: 0.99362, l2: 0.03176\n",
            "Loss: 2.303192e-05, l1: 0.99391, l2: 0.03177\n",
            "Loss: 2.275665e-05, l1: 0.99434, l2: 0.03177\n",
            "Loss: 2.254007e-05, l1: 0.99452, l2: 0.03181\n",
            "Loss: 2.375466e-05, l1: 0.99584, l2: 0.03178\n",
            "Loss: 2.235279e-05, l1: 0.99487, l2: 0.03180\n",
            "Loss: 2.199425e-05, l1: 0.99474, l2: 0.03176\n",
            "Loss: 2.165621e-05, l1: 0.99444, l2: 0.03172\n",
            "Loss: 2.157513e-05, l1: 0.99444, l2: 0.03171\n",
            "Loss: 2.144529e-05, l1: 0.99449, l2: 0.03170\n",
            "Loss: 2.137705e-05, l1: 0.99448, l2: 0.03171\n",
            "Loss: 2.130013e-05, l1: 0.99453, l2: 0.03171\n",
            "Loss: 2.121813e-05, l1: 0.99448, l2: 0.03171\n",
            "Loss: 2.115953e-05, l1: 0.99448, l2: 0.03170\n",
            "Loss: 2.114433e-05, l1: 0.99438, l2: 0.03170\n",
            "Loss: 2.107284e-05, l1: 0.99447, l2: 0.03170\n",
            "Loss: 2.103972e-05, l1: 0.99453, l2: 0.03171\n",
            "Loss: 2.098104e-05, l1: 0.99464, l2: 0.03173\n",
            "Loss: 2.094770e-05, l1: 0.99478, l2: 0.03176\n",
            "Loss: 2.090582e-05, l1: 0.99527, l2: 0.03181\n",
            "Loss: 2.075555e-05, l1: 0.99518, l2: 0.03176\n",
            "Loss: 2.071151e-05, l1: 0.99510, l2: 0.03176\n",
            "Loss: 2.063120e-05, l1: 0.99508, l2: 0.03175\n",
            "Loss: 2.054757e-05, l1: 0.99519, l2: 0.03174\n",
            "Loss: 2.060825e-05, l1: 0.99579, l2: 0.03176\n",
            "Loss: 2.051370e-05, l1: 0.99541, l2: 0.03175\n",
            "Loss: 2.047231e-05, l1: 0.99557, l2: 0.03174\n",
            "Loss: 2.043719e-05, l1: 0.99575, l2: 0.03175\n",
            "Loss: 2.040355e-05, l1: 0.99589, l2: 0.03175\n",
            "Loss: 2.034567e-05, l1: 0.99614, l2: 0.03177\n",
            "Loss: 2.030634e-05, l1: 0.99642, l2: 0.03178\n",
            "Loss: 2.023183e-05, l1: 0.99636, l2: 0.03178\n",
            "Loss: 2.018525e-05, l1: 0.99629, l2: 0.03178\n",
            "Loss: 2.015367e-05, l1: 0.99630, l2: 0.03178\n",
            "Loss: 2.011463e-05, l1: 0.99638, l2: 0.03177\n",
            "Loss: 2.007236e-05, l1: 0.99671, l2: 0.03176\n",
            "Loss: 2.003432e-05, l1: 0.99670, l2: 0.03175\n",
            "Loss: 1.999862e-05, l1: 0.99678, l2: 0.03175\n",
            "Loss: 1.994617e-05, l1: 0.99688, l2: 0.03176\n",
            "Loss: 1.990664e-05, l1: 0.99711, l2: 0.03176\n",
            "Loss: 1.985072e-05, l1: 0.99719, l2: 0.03176\n",
            "Loss: 1.976114e-05, l1: 0.99735, l2: 0.03177\n",
            "Loss: 1.969365e-05, l1: 0.99748, l2: 0.03177\n",
            "Loss: 1.977907e-05, l1: 0.99801, l2: 0.03179\n",
            "Loss: 1.965759e-05, l1: 0.99767, l2: 0.03178\n",
            "Loss: 1.957833e-05, l1: 0.99778, l2: 0.03177\n",
            "Loss: 1.954136e-05, l1: 0.99782, l2: 0.03178\n",
            "Loss: 1.949173e-05, l1: 0.99791, l2: 0.03178\n",
            "Loss: 1.942041e-05, l1: 0.99804, l2: 0.03179\n",
            "Loss: 1.965201e-05, l1: 0.99853, l2: 0.03182\n",
            "Loss: 1.938708e-05, l1: 0.99817, l2: 0.03180\n",
            "Loss: 1.932767e-05, l1: 0.99836, l2: 0.03180\n",
            "Loss: 1.927350e-05, l1: 0.99843, l2: 0.03180\n",
            "Loss: 1.916000e-05, l1: 0.99866, l2: 0.03180\n",
            "Loss: 1.903675e-05, l1: 0.99887, l2: 0.03180\n",
            "Loss: 1.899342e-05, l1: 0.99836, l2: 0.03176\n",
            "Loss: 1.882358e-05, l1: 0.99867, l2: 0.03179\n",
            "Loss: 1.877487e-05, l1: 0.99864, l2: 0.03181\n",
            "Loss: 1.872610e-05, l1: 0.99857, l2: 0.03181\n",
            "Loss: 1.865996e-05, l1: 0.99850, l2: 0.03182\n",
            "Loss: 1.859010e-05, l1: 0.99846, l2: 0.03182\n",
            "Loss: 1.858682e-05, l1: 0.99878, l2: 0.03181\n",
            "Loss: 1.855408e-05, l1: 0.99862, l2: 0.03181\n",
            "Loss: 1.849883e-05, l1: 0.99865, l2: 0.03181\n",
            "Loss: 1.843174e-05, l1: 0.99874, l2: 0.03181\n",
            "Loss: 1.833862e-05, l1: 0.99878, l2: 0.03179\n",
            "Loss: 1.822439e-05, l1: 0.99882, l2: 0.03178\n",
            "Loss: 1.811589e-05, l1: 0.99874, l2: 0.03177\n",
            "Loss: 1.804082e-05, l1: 0.99829, l2: 0.03177\n",
            "Loss: 1.797351e-05, l1: 0.99823, l2: 0.03176\n",
            "Loss: 1.791397e-05, l1: 0.99806, l2: 0.03175\n",
            "Loss: 1.784713e-05, l1: 0.99780, l2: 0.03174\n",
            "Loss: 1.776385e-05, l1: 0.99762, l2: 0.03173\n",
            "Loss: 1.765865e-05, l1: 0.99756, l2: 0.03173\n",
            "Loss: 1.746937e-05, l1: 0.99769, l2: 0.03174\n",
            "Loss: 1.730775e-05, l1: 0.99782, l2: 0.03174\n",
            "Loss: 1.719079e-05, l1: 0.99783, l2: 0.03173\n",
            "Loss: 1.711042e-05, l1: 0.99803, l2: 0.03177\n",
            "Loss: 1.705760e-05, l1: 0.99799, l2: 0.03178\n",
            "Loss: 1.698061e-05, l1: 0.99775, l2: 0.03179\n",
            "Loss: 1.692082e-05, l1: 0.99764, l2: 0.03181\n",
            "Loss: 1.689212e-05, l1: 0.99729, l2: 0.03184\n",
            "Loss: 1.683038e-05, l1: 0.99733, l2: 0.03184\n",
            "Loss: 1.679031e-05, l1: 0.99732, l2: 0.03183\n",
            "Loss: 1.675105e-05, l1: 0.99721, l2: 0.03183\n",
            "Loss: 1.670187e-05, l1: 0.99704, l2: 0.03183\n",
            "Loss: 1.667713e-05, l1: 0.99677, l2: 0.03183\n",
            "Loss: 1.663079e-05, l1: 0.99673, l2: 0.03183\n",
            "Loss: 1.658268e-05, l1: 0.99667, l2: 0.03184\n",
            "Loss: 1.654171e-05, l1: 0.99658, l2: 0.03184\n",
            "Loss: 1.649694e-05, l1: 0.99650, l2: 0.03183\n",
            "Loss: 1.646982e-05, l1: 0.99642, l2: 0.03183\n",
            "Loss: 1.644758e-05, l1: 0.99638, l2: 0.03183\n",
            "Loss: 1.641853e-05, l1: 0.99630, l2: 0.03183\n",
            "Loss: 1.637257e-05, l1: 0.99614, l2: 0.03182\n",
            "Loss: 1.650345e-05, l1: 0.99578, l2: 0.03186\n",
            "Loss: 1.635590e-05, l1: 0.99605, l2: 0.03183\n",
            "Loss: 1.630039e-05, l1: 0.99587, l2: 0.03183\n",
            "Loss: 1.623853e-05, l1: 0.99571, l2: 0.03184\n",
            "Loss: 1.620698e-05, l1: 0.99584, l2: 0.03185\n",
            "Loss: 1.615187e-05, l1: 0.99577, l2: 0.03185\n",
            "Loss: 1.611084e-05, l1: 0.99582, l2: 0.03186\n",
            "Loss: 1.608250e-05, l1: 0.99595, l2: 0.03186\n",
            "Loss: 1.605936e-05, l1: 0.99597, l2: 0.03186\n",
            "Loss: 1.601891e-05, l1: 0.99595, l2: 0.03186\n",
            "Loss: 1.598468e-05, l1: 0.99576, l2: 0.03185\n",
            "Loss: 1.595371e-05, l1: 0.99569, l2: 0.03185\n",
            "Loss: 1.591599e-05, l1: 0.99548, l2: 0.03183\n",
            "Loss: 1.588592e-05, l1: 0.99551, l2: 0.03182\n",
            "Loss: 1.585770e-05, l1: 0.99562, l2: 0.03183\n",
            "Loss: 1.598543e-05, l1: 0.99579, l2: 0.03178\n",
            "Loss: 1.583973e-05, l1: 0.99567, l2: 0.03182\n",
            "Loss: 1.581598e-05, l1: 0.99582, l2: 0.03182\n",
            "Loss: 1.580225e-05, l1: 0.99594, l2: 0.03181\n",
            "Loss: 1.576796e-05, l1: 0.99613, l2: 0.03181\n",
            "Loss: 1.571807e-05, l1: 0.99633, l2: 0.03180\n",
            "Loss: 1.576493e-05, l1: 0.99700, l2: 0.03178\n",
            "Loss: 1.569890e-05, l1: 0.99656, l2: 0.03179\n",
            "Loss: 1.566628e-05, l1: 0.99654, l2: 0.03178\n",
            "Loss: 1.562166e-05, l1: 0.99645, l2: 0.03178\n",
            "Loss: 1.557870e-05, l1: 0.99639, l2: 0.03177\n",
            "Loss: 1.552270e-05, l1: 0.99634, l2: 0.03176\n",
            "Loss: 1.546228e-05, l1: 0.99643, l2: 0.03176\n",
            "Loss: 1.539567e-05, l1: 0.99653, l2: 0.03174\n",
            "Loss: 1.533952e-05, l1: 0.99664, l2: 0.03174\n",
            "Loss: 1.529871e-05, l1: 0.99678, l2: 0.03174\n",
            "Loss: 1.526695e-05, l1: 0.99684, l2: 0.03175\n",
            "Loss: 1.524247e-05, l1: 0.99683, l2: 0.03175\n",
            "Loss: 1.522435e-05, l1: 0.99685, l2: 0.03175\n",
            "Loss: 1.521200e-05, l1: 0.99685, l2: 0.03175\n",
            "Loss: 1.517498e-05, l1: 0.99691, l2: 0.03175\n",
            "Loss: 1.526641e-05, l1: 0.99688, l2: 0.03172\n",
            "Loss: 1.515500e-05, l1: 0.99690, l2: 0.03174\n",
            "Loss: 1.509317e-05, l1: 0.99707, l2: 0.03175\n",
            "Loss: 1.504830e-05, l1: 0.99720, l2: 0.03175\n",
            "Loss: 1.498944e-05, l1: 0.99733, l2: 0.03176\n",
            "Loss: 1.491207e-05, l1: 0.99743, l2: 0.03177\n",
            "Loss: 1.481814e-05, l1: 0.99753, l2: 0.03178\n",
            "Loss: 1.496983e-05, l1: 0.99778, l2: 0.03184\n",
            "Loss: 1.478177e-05, l1: 0.99761, l2: 0.03180\n",
            "Loss: 1.472295e-05, l1: 0.99760, l2: 0.03180\n",
            "Loss: 1.467188e-05, l1: 0.99756, l2: 0.03181\n",
            "Loss: 1.463815e-05, l1: 0.99760, l2: 0.03181\n",
            "Loss: 1.461923e-05, l1: 0.99775, l2: 0.03181\n",
            "Loss: 1.456015e-05, l1: 0.99787, l2: 0.03181\n",
            "Loss: 1.452710e-05, l1: 0.99797, l2: 0.03180\n",
            "Loss: 1.448110e-05, l1: 0.99818, l2: 0.03179\n",
            "Loss: 1.443487e-05, l1: 0.99834, l2: 0.03178\n",
            "Loss: 1.438748e-05, l1: 0.99837, l2: 0.03178\n",
            "Loss: 1.435103e-05, l1: 0.99847, l2: 0.03179\n",
            "Loss: 1.430636e-05, l1: 0.99837, l2: 0.03181\n",
            "Loss: 1.427234e-05, l1: 0.99856, l2: 0.03181\n",
            "Loss: 1.421647e-05, l1: 0.99838, l2: 0.03182\n",
            "Loss: 1.414568e-05, l1: 0.99819, l2: 0.03183\n",
            "Loss: 1.409342e-05, l1: 0.99812, l2: 0.03183\n",
            "Loss: 1.406383e-05, l1: 0.99816, l2: 0.03183\n",
            "Loss: 1.411061e-05, l1: 0.99807, l2: 0.03182\n",
            "Loss: 1.404290e-05, l1: 0.99813, l2: 0.03183\n",
            "Loss: 1.402077e-05, l1: 0.99812, l2: 0.03183\n",
            "Loss: 1.400169e-05, l1: 0.99815, l2: 0.03183\n",
            "Loss: 1.397471e-05, l1: 0.99813, l2: 0.03184\n",
            "Loss: 1.394970e-05, l1: 0.99811, l2: 0.03184\n",
            "Loss: 1.391092e-05, l1: 0.99792, l2: 0.03185\n",
            "Loss: 1.399765e-05, l1: 0.99829, l2: 0.03187\n",
            "Loss: 1.389866e-05, l1: 0.99802, l2: 0.03185\n",
            "Loss: 1.387494e-05, l1: 0.99782, l2: 0.03185\n",
            "Loss: 1.385646e-05, l1: 0.99768, l2: 0.03185\n",
            "Loss: 1.384236e-05, l1: 0.99758, l2: 0.03184\n",
            "Loss: 1.382611e-05, l1: 0.99754, l2: 0.03185\n",
            "Loss: 1.380255e-05, l1: 0.99752, l2: 0.03184\n",
            "Loss: 1.378419e-05, l1: 0.99755, l2: 0.03184\n",
            "Loss: 1.377102e-05, l1: 0.99757, l2: 0.03184\n",
            "Loss: 1.375606e-05, l1: 0.99756, l2: 0.03184\n",
            "Loss: 1.390768e-05, l1: 0.99791, l2: 0.03184\n",
            "Loss: 1.374959e-05, l1: 0.99762, l2: 0.03184\n",
            "Loss: 1.372891e-05, l1: 0.99757, l2: 0.03184\n",
            "Loss: 1.368895e-05, l1: 0.99745, l2: 0.03184\n",
            "Loss: 1.364818e-05, l1: 0.99732, l2: 0.03185\n",
            "Loss: 1.361847e-05, l1: 0.99715, l2: 0.03185\n",
            "Loss: 1.358925e-05, l1: 0.99716, l2: 0.03185\n",
            "Loss: 1.354823e-05, l1: 0.99721, l2: 0.03186\n",
            "Loss: 1.351595e-05, l1: 0.99723, l2: 0.03186\n",
            "Loss: 1.355194e-05, l1: 0.99738, l2: 0.03185\n",
            "Loss: 1.349768e-05, l1: 0.99729, l2: 0.03185\n",
            "Loss: 1.346253e-05, l1: 0.99732, l2: 0.03186\n",
            "Loss: 1.344209e-05, l1: 0.99732, l2: 0.03185\n",
            "Loss: 1.342238e-05, l1: 0.99737, l2: 0.03185\n",
            "Loss: 1.340693e-05, l1: 0.99747, l2: 0.03184\n",
            "Loss: 1.339269e-05, l1: 0.99755, l2: 0.03184\n",
            "Loss: 1.337982e-05, l1: 0.99762, l2: 0.03184\n",
            "Loss: 1.339441e-05, l1: 0.99780, l2: 0.03186\n",
            "Loss: 1.337350e-05, l1: 0.99768, l2: 0.03185\n",
            "Loss: 1.335878e-05, l1: 0.99769, l2: 0.03185\n",
            "Loss: 1.333327e-05, l1: 0.99768, l2: 0.03185\n",
            "Loss: 1.330562e-05, l1: 0.99773, l2: 0.03185\n",
            "Loss: 1.328423e-05, l1: 0.99765, l2: 0.03185\n",
            "Loss: 1.326604e-05, l1: 0.99766, l2: 0.03185\n",
            "Loss: 1.322955e-05, l1: 0.99770, l2: 0.03186\n",
            "Loss: 1.319024e-05, l1: 0.99766, l2: 0.03186\n",
            "Loss: 1.314284e-05, l1: 0.99760, l2: 0.03185\n",
            "Loss: 1.309939e-05, l1: 0.99751, l2: 0.03185\n",
            "Loss: 1.310857e-05, l1: 0.99740, l2: 0.03185\n",
            "Loss: 1.307383e-05, l1: 0.99746, l2: 0.03185\n",
            "Loss: 1.302899e-05, l1: 0.99727, l2: 0.03185\n",
            "Loss: 1.299140e-05, l1: 0.99710, l2: 0.03185\n",
            "Loss: 1.296180e-05, l1: 0.99697, l2: 0.03184\n",
            "Loss: 1.293100e-05, l1: 0.99689, l2: 0.03183\n",
            "Loss: 1.289780e-05, l1: 0.99684, l2: 0.03182\n",
            "Loss: 1.287458e-05, l1: 0.99680, l2: 0.03182\n",
            "Loss: 1.284745e-05, l1: 0.99675, l2: 0.03182\n",
            "Loss: 1.280135e-05, l1: 0.99660, l2: 0.03182\n",
            "Loss: 1.306030e-05, l1: 0.99664, l2: 0.03183\n",
            "Loss: 1.278261e-05, l1: 0.99661, l2: 0.03182\n",
            "Loss: 1.274478e-05, l1: 0.99646, l2: 0.03181\n",
            "Loss: 1.269802e-05, l1: 0.99627, l2: 0.03180\n",
            "Loss: 1.267883e-05, l1: 0.99614, l2: 0.03179\n",
            "Loss: 1.264889e-05, l1: 0.99623, l2: 0.03178\n",
            "Loss: 1.263047e-05, l1: 0.99630, l2: 0.03179\n",
            "Loss: 1.259681e-05, l1: 0.99642, l2: 0.03179\n",
            "Loss: 1.255154e-05, l1: 0.99642, l2: 0.03180\n",
            "Loss: 1.243907e-05, l1: 0.99629, l2: 0.03181\n",
            "Loss: 1.233141e-05, l1: 0.99626, l2: 0.03183\n",
            "Loss: 1.224558e-05, l1: 0.99613, l2: 0.03185\n",
            "Loss: 1.218052e-05, l1: 0.99596, l2: 0.03185\n",
            "Loss: 1.216451e-05, l1: 0.99630, l2: 0.03187\n",
            "Loss: 1.208957e-05, l1: 0.99622, l2: 0.03187\n",
            "Loss: 1.204602e-05, l1: 0.99624, l2: 0.03187\n",
            "Loss: 1.201755e-05, l1: 0.99640, l2: 0.03187\n",
            "Loss: 1.197484e-05, l1: 0.99655, l2: 0.03188\n",
            "Loss: 1.191454e-05, l1: 0.99687, l2: 0.03187\n",
            "Loss: 1.210734e-05, l1: 0.99718, l2: 0.03188\n",
            "Loss: 1.188752e-05, l1: 0.99695, l2: 0.03188\n",
            "Loss: 1.185783e-05, l1: 0.99691, l2: 0.03187\n",
            "Loss: 1.181499e-05, l1: 0.99690, l2: 0.03185\n",
            "Loss: 1.177855e-05, l1: 0.99683, l2: 0.03184\n",
            "Loss: 1.173356e-05, l1: 0.99685, l2: 0.03183\n",
            "Loss: 1.168445e-05, l1: 0.99693, l2: 0.03184\n",
            "Loss: 1.164965e-05, l1: 0.99701, l2: 0.03185\n",
            "Loss: 1.161867e-05, l1: 0.99706, l2: 0.03186\n",
            "Loss: 1.158264e-05, l1: 0.99706, l2: 0.03187\n",
            "Loss: 1.153416e-05, l1: 0.99702, l2: 0.03188\n",
            "Loss: 1.154988e-05, l1: 0.99686, l2: 0.03189\n",
            "Loss: 1.150709e-05, l1: 0.99695, l2: 0.03189\n",
            "Loss: 1.147226e-05, l1: 0.99690, l2: 0.03188\n",
            "Loss: 1.140686e-05, l1: 0.99678, l2: 0.03188\n",
            "Loss: 1.135296e-05, l1: 0.99674, l2: 0.03188\n",
            "Loss: 1.124915e-05, l1: 0.99688, l2: 0.03190\n",
            "Loss: 1.137563e-05, l1: 0.99657, l2: 0.03190\n",
            "Loss: 1.120652e-05, l1: 0.99678, l2: 0.03190\n",
            "Loss: 1.114557e-05, l1: 0.99691, l2: 0.03191\n",
            "Loss: 1.110146e-05, l1: 0.99711, l2: 0.03193\n",
            "Loss: 1.107569e-05, l1: 0.99712, l2: 0.03192\n",
            "Loss: 1.103798e-05, l1: 0.99710, l2: 0.03191\n",
            "Loss: 1.097069e-05, l1: 0.99709, l2: 0.03190\n",
            "Loss: 1.089930e-05, l1: 0.99718, l2: 0.03190\n",
            "Loss: 1.083857e-05, l1: 0.99727, l2: 0.03190\n",
            "Loss: 1.080469e-05, l1: 0.99747, l2: 0.03191\n",
            "Loss: 1.076705e-05, l1: 0.99750, l2: 0.03191\n",
            "Loss: 1.074915e-05, l1: 0.99752, l2: 0.03191\n",
            "Loss: 1.072435e-05, l1: 0.99753, l2: 0.03191\n",
            "Loss: 1.068703e-05, l1: 0.99751, l2: 0.03191\n",
            "Loss: 1.064885e-05, l1: 0.99744, l2: 0.03190\n",
            "Loss: 1.060412e-05, l1: 0.99740, l2: 0.03190\n",
            "Loss: 1.056784e-05, l1: 0.99729, l2: 0.03191\n",
            "Loss: 1.055324e-05, l1: 0.99724, l2: 0.03191\n",
            "Loss: 1.052759e-05, l1: 0.99716, l2: 0.03191\n",
            "Loss: 1.049231e-05, l1: 0.99712, l2: 0.03191\n",
            "Loss: 1.052559e-05, l1: 0.99703, l2: 0.03191\n",
            "Loss: 1.047671e-05, l1: 0.99709, l2: 0.03191\n",
            "Loss: 1.044642e-05, l1: 0.99716, l2: 0.03191\n",
            "Loss: 1.041898e-05, l1: 0.99725, l2: 0.03192\n",
            "Loss: 1.039013e-05, l1: 0.99738, l2: 0.03191\n",
            "Loss: 1.035540e-05, l1: 0.99740, l2: 0.03192\n",
            "Loss: 1.030910e-05, l1: 0.99734, l2: 0.03192\n",
            "Loss: 1.025244e-05, l1: 0.99715, l2: 0.03193\n",
            "Loss: 1.022883e-05, l1: 0.99707, l2: 0.03194\n",
            "Loss: 1.019649e-05, l1: 0.99696, l2: 0.03195\n",
            "Loss: 1.021911e-05, l1: 0.99689, l2: 0.03196\n",
            "Loss: 1.018257e-05, l1: 0.99693, l2: 0.03195\n",
            "Loss: 1.015673e-05, l1: 0.99695, l2: 0.03196\n",
            "Loss: 1.013216e-05, l1: 0.99702, l2: 0.03196\n",
            "Loss: 1.011235e-05, l1: 0.99714, l2: 0.03196\n",
            "Loss: 1.008934e-05, l1: 0.99727, l2: 0.03195\n",
            "Loss: 1.006352e-05, l1: 0.99745, l2: 0.03194\n",
            "Loss: 1.004589e-05, l1: 0.99748, l2: 0.03194\n",
            "Loss: 1.002781e-05, l1: 0.99749, l2: 0.03194\n",
            "Loss: 1.001576e-05, l1: 0.99749, l2: 0.03193\n",
            "Loss: 9.993535e-06, l1: 0.99755, l2: 0.03193\n",
            "Loss: 1.008964e-05, l1: 0.99770, l2: 0.03192\n",
            "Loss: 9.979856e-06, l1: 0.99759, l2: 0.03193\n",
            "Loss: 9.940825e-06, l1: 0.99772, l2: 0.03192\n",
            "Loss: 9.916348e-06, l1: 0.99783, l2: 0.03193\n",
            "Loss: 9.897108e-06, l1: 0.99788, l2: 0.03193\n",
            "Loss: 9.870166e-06, l1: 0.99795, l2: 0.03192\n",
            "Loss: 9.825385e-06, l1: 0.99816, l2: 0.03192\n",
            "Loss: 9.785696e-06, l1: 0.99840, l2: 0.03192\n",
            "Loss: 9.754252e-06, l1: 0.99843, l2: 0.03192\n",
            "Loss: 9.728267e-06, l1: 0.99852, l2: 0.03192\n",
            "Loss: 9.712417e-06, l1: 0.99858, l2: 0.03192\n",
            "Loss: 9.656040e-06, l1: 0.99879, l2: 0.03191\n",
            "Loss: 9.980248e-06, l1: 0.99877, l2: 0.03191\n",
            "Loss: 9.641823e-06, l1: 0.99879, l2: 0.03191\n",
            "Loss: 9.617405e-06, l1: 0.99879, l2: 0.03191\n",
            "Loss: 9.591128e-06, l1: 0.99869, l2: 0.03191\n",
            "Loss: 9.574669e-06, l1: 0.99859, l2: 0.03191\n",
            "Loss: 9.560737e-06, l1: 0.99847, l2: 0.03190\n",
            "Loss: 9.549159e-06, l1: 0.99843, l2: 0.03190\n",
            "Loss: 9.532154e-06, l1: 0.99841, l2: 0.03190\n",
            "Loss: 9.521133e-06, l1: 0.99843, l2: 0.03190\n",
            "Loss: 9.502285e-06, l1: 0.99844, l2: 0.03189\n",
            "Loss: 9.479358e-06, l1: 0.99845, l2: 0.03189\n",
            "Loss: 9.495713e-06, l1: 0.99831, l2: 0.03190\n",
            "Loss: 9.467627e-06, l1: 0.99839, l2: 0.03189\n",
            "Loss: 9.453078e-06, l1: 0.99838, l2: 0.03189\n",
            "Loss: 9.442441e-06, l1: 0.99835, l2: 0.03190\n",
            "Loss: 9.434231e-06, l1: 0.99833, l2: 0.03190\n",
            "Loss: 9.420794e-06, l1: 0.99828, l2: 0.03190\n",
            "Loss: 9.409545e-06, l1: 0.99823, l2: 0.03190\n",
            "Loss: 9.395989e-06, l1: 0.99821, l2: 0.03190\n",
            "Loss: 9.384699e-06, l1: 0.99821, l2: 0.03190\n",
            "Loss: 9.374991e-06, l1: 0.99820, l2: 0.03190\n",
            "Loss: 9.361067e-06, l1: 0.99818, l2: 0.03191\n",
            "Loss: 9.348227e-06, l1: 0.99816, l2: 0.03192\n",
            "Loss: 9.354279e-06, l1: 0.99816, l2: 0.03192\n",
            "Loss: 9.339788e-06, l1: 0.99816, l2: 0.03192\n",
            "Loss: 9.325589e-06, l1: 0.99821, l2: 0.03192\n",
            "Loss: 9.312913e-06, l1: 0.99818, l2: 0.03192\n",
            "Loss: 9.294542e-06, l1: 0.99812, l2: 0.03192\n",
            "Loss: 9.282215e-06, l1: 0.99809, l2: 0.03191\n",
            "Loss: 9.261285e-06, l1: 0.99800, l2: 0.03190\n",
            "Loss: 9.239838e-06, l1: 0.99802, l2: 0.03188\n",
            "Loss: 9.219046e-06, l1: 0.99802, l2: 0.03188\n",
            "Loss: 9.205768e-06, l1: 0.99808, l2: 0.03189\n",
            "Loss: 9.195612e-06, l1: 0.99815, l2: 0.03189\n",
            "Loss: 9.192479e-06, l1: 0.99822, l2: 0.03189\n",
            "Loss: 9.174511e-06, l1: 0.99827, l2: 0.03189\n",
            "Loss: 9.167019e-06, l1: 0.99828, l2: 0.03189\n",
            "Loss: 9.156730e-06, l1: 0.99830, l2: 0.03189\n",
            "Loss: 9.145158e-06, l1: 0.99834, l2: 0.03189\n",
            "Loss: 9.131737e-06, l1: 0.99840, l2: 0.03189\n",
            "Loss: 9.128717e-06, l1: 0.99864, l2: 0.03189\n",
            "Loss: 9.109857e-06, l1: 0.99854, l2: 0.03189\n",
            "Loss: 9.100004e-06, l1: 0.99851, l2: 0.03189\n",
            "Loss: 9.085107e-06, l1: 0.99854, l2: 0.03189\n",
            "Loss: 9.066262e-06, l1: 0.99866, l2: 0.03189\n",
            "Loss: 9.170749e-06, l1: 0.99912, l2: 0.03188\n",
            "Loss: 9.053048e-06, l1: 0.99877, l2: 0.03189\n",
            "Loss: 9.030868e-06, l1: 0.99891, l2: 0.03189\n",
            "Loss: 8.971961e-06, l1: 0.99936, l2: 0.03190\n",
            "Loss: 8.928285e-06, l1: 0.99963, l2: 0.03191\n",
            "Loss: 8.888826e-06, l1: 0.99992, l2: 0.03192\n",
            "Loss: 9.038967e-06, l1: 1.00013, l2: 0.03194\n",
            "Loss: 8.867439e-06, l1: 0.99997, l2: 0.03193\n",
            "Loss: 8.843119e-06, l1: 0.99987, l2: 0.03192\n",
            "Loss: 8.784192e-06, l1: 0.99962, l2: 0.03192\n",
            "Loss: 8.761026e-06, l1: 0.99955, l2: 0.03192\n",
            "Loss: 8.736128e-06, l1: 0.99966, l2: 0.03191\n",
            "Loss: 8.707323e-06, l1: 0.99966, l2: 0.03191\n",
            "Loss: 8.693691e-06, l1: 0.99961, l2: 0.03191\n",
            "Loss: 8.681144e-06, l1: 0.99962, l2: 0.03190\n",
            "Loss: 8.669688e-06, l1: 0.99963, l2: 0.03190\n",
            "Loss: 8.653416e-06, l1: 0.99963, l2: 0.03189\n",
            "Loss: 8.615864e-06, l1: 0.99964, l2: 0.03188\n",
            "Loss: 8.650582e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 8.593661e-06, l1: 0.99955, l2: 0.03187\n",
            "Loss: 8.549601e-06, l1: 0.99960, l2: 0.03186\n",
            "Loss: 8.513707e-06, l1: 0.99957, l2: 0.03185\n",
            "Loss: 8.466071e-06, l1: 0.99948, l2: 0.03185\n",
            "Loss: 8.485106e-06, l1: 0.99934, l2: 0.03186\n",
            "Loss: 8.456319e-06, l1: 0.99943, l2: 0.03185\n",
            "Loss: 8.433868e-06, l1: 0.99940, l2: 0.03185\n",
            "Loss: 8.403646e-06, l1: 0.99938, l2: 0.03185\n",
            "Loss: 8.362679e-06, l1: 0.99939, l2: 0.03185\n",
            "Loss: 8.305668e-06, l1: 0.99939, l2: 0.03184\n",
            "Loss: 8.297447e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 8.245916e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 8.232862e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 8.214859e-06, l1: 0.99936, l2: 0.03183\n",
            "Loss: 8.187987e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 8.160394e-06, l1: 0.99916, l2: 0.03183\n",
            "Loss: 8.232347e-06, l1: 0.99901, l2: 0.03183\n",
            "Loss: 8.152015e-06, l1: 0.99912, l2: 0.03183\n",
            "Loss: 8.124161e-06, l1: 0.99900, l2: 0.03183\n",
            "Loss: 8.097866e-06, l1: 0.99883, l2: 0.03182\n",
            "Loss: 8.079057e-06, l1: 0.99871, l2: 0.03182\n",
            "Loss: 8.052783e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 8.032927e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 7.998652e-06, l1: 0.99854, l2: 0.03183\n",
            "Loss: 7.962029e-06, l1: 0.99837, l2: 0.03185\n",
            "Loss: 7.920185e-06, l1: 0.99812, l2: 0.03186\n",
            "Loss: 7.896094e-06, l1: 0.99800, l2: 0.03187\n",
            "Loss: 7.848726e-06, l1: 0.99781, l2: 0.03188\n",
            "Loss: 7.841129e-06, l1: 0.99815, l2: 0.03189\n",
            "Loss: 7.795162e-06, l1: 0.99794, l2: 0.03189\n",
            "Loss: 7.779760e-06, l1: 0.99793, l2: 0.03189\n",
            "Loss: 7.752948e-06, l1: 0.99795, l2: 0.03188\n",
            "Loss: 7.733297e-06, l1: 0.99785, l2: 0.03188\n",
            "Loss: 7.710079e-06, l1: 0.99794, l2: 0.03188\n",
            "Loss: 7.691200e-06, l1: 0.99791, l2: 0.03188\n",
            "Loss: 7.678952e-06, l1: 0.99783, l2: 0.03189\n",
            "Loss: 7.670855e-06, l1: 0.99777, l2: 0.03189\n",
            "Loss: 7.658210e-06, l1: 0.99772, l2: 0.03189\n",
            "Loss: 7.646640e-06, l1: 0.99773, l2: 0.03189\n",
            "Loss: 7.633436e-06, l1: 0.99773, l2: 0.03189\n",
            "Loss: 7.616305e-06, l1: 0.99776, l2: 0.03189\n",
            "Loss: 7.597348e-06, l1: 0.99773, l2: 0.03189\n",
            "Loss: 7.584304e-06, l1: 0.99768, l2: 0.03189\n",
            "Loss: 7.571381e-06, l1: 0.99754, l2: 0.03189\n",
            "Loss: 7.556719e-06, l1: 0.99761, l2: 0.03189\n",
            "Loss: 7.545575e-06, l1: 0.99763, l2: 0.03188\n",
            "Loss: 7.528133e-06, l1: 0.99770, l2: 0.03188\n",
            "Loss: 7.516275e-06, l1: 0.99775, l2: 0.03187\n",
            "Loss: 7.505735e-06, l1: 0.99802, l2: 0.03186\n",
            "Loss: 7.469458e-06, l1: 0.99796, l2: 0.03186\n",
            "Loss: 7.446061e-06, l1: 0.99793, l2: 0.03187\n",
            "Loss: 7.409826e-06, l1: 0.99795, l2: 0.03188\n",
            "Loss: 7.382692e-06, l1: 0.99804, l2: 0.03188\n",
            "Loss: 7.354605e-06, l1: 0.99812, l2: 0.03189\n",
            "Loss: 7.338653e-06, l1: 0.99822, l2: 0.03188\n",
            "Loss: 7.326881e-06, l1: 0.99823, l2: 0.03188\n",
            "Loss: 7.312198e-06, l1: 0.99823, l2: 0.03187\n",
            "Loss: 7.649425e-06, l1: 0.99776, l2: 0.03189\n",
            "Loss: 7.309510e-06, l1: 0.99819, l2: 0.03187\n",
            "Loss: 7.297344e-06, l1: 0.99813, l2: 0.03187\n",
            "Loss: 7.280734e-06, l1: 0.99807, l2: 0.03186\n",
            "Loss: 7.268678e-06, l1: 0.99803, l2: 0.03186\n",
            "Loss: 7.254410e-06, l1: 0.99801, l2: 0.03186\n",
            "Loss: 7.240401e-06, l1: 0.99795, l2: 0.03186\n",
            "Loss: 7.245578e-06, l1: 0.99788, l2: 0.03186\n",
            "Loss: 7.231645e-06, l1: 0.99792, l2: 0.03186\n",
            "Loss: 7.215483e-06, l1: 0.99796, l2: 0.03186\n",
            "Loss: 7.201421e-06, l1: 0.99799, l2: 0.03186\n",
            "Loss: 7.183933e-06, l1: 0.99802, l2: 0.03186\n",
            "Loss: 7.159249e-06, l1: 0.99807, l2: 0.03186\n",
            "Loss: 7.160642e-06, l1: 0.99831, l2: 0.03186\n",
            "Loss: 7.148744e-06, l1: 0.99819, l2: 0.03186\n",
            "Loss: 7.139805e-06, l1: 0.99829, l2: 0.03186\n",
            "Loss: 7.130801e-06, l1: 0.99822, l2: 0.03186\n",
            "Loss: 7.127998e-06, l1: 0.99819, l2: 0.03186\n",
            "Loss: 7.124975e-06, l1: 0.99816, l2: 0.03186\n",
            "Loss: 7.118398e-06, l1: 0.99811, l2: 0.03186\n",
            "Loss: 7.111086e-06, l1: 0.99814, l2: 0.03185\n",
            "Loss: 7.212264e-06, l1: 0.99780, l2: 0.03184\n",
            "Loss: 7.104916e-06, l1: 0.99807, l2: 0.03185\n",
            "Loss: 7.098174e-06, l1: 0.99807, l2: 0.03185\n",
            "Loss: 7.086573e-06, l1: 0.99809, l2: 0.03185\n",
            "Loss: 7.079487e-06, l1: 0.99809, l2: 0.03185\n",
            "Loss: 7.069932e-06, l1: 0.99814, l2: 0.03185\n",
            "Loss: 7.089027e-06, l1: 0.99786, l2: 0.03185\n",
            "Loss: 7.066486e-06, l1: 0.99806, l2: 0.03185\n",
            "Loss: 7.057410e-06, l1: 0.99810, l2: 0.03185\n",
            "Loss: 7.040831e-06, l1: 0.99812, l2: 0.03185\n",
            "Loss: 7.023378e-06, l1: 0.99812, l2: 0.03184\n",
            "Loss: 7.006233e-06, l1: 0.99806, l2: 0.03184\n",
            "Loss: 7.014711e-06, l1: 0.99800, l2: 0.03183\n",
            "Loss: 6.992653e-06, l1: 0.99803, l2: 0.03183\n",
            "Loss: 6.969849e-06, l1: 0.99797, l2: 0.03183\n",
            "Loss: 6.941510e-06, l1: 0.99792, l2: 0.03182\n",
            "Loss: 6.927058e-06, l1: 0.99795, l2: 0.03182\n",
            "Loss: 6.911115e-06, l1: 0.99804, l2: 0.03182\n",
            "Loss: 6.895220e-06, l1: 0.99816, l2: 0.03182\n",
            "Loss: 6.886136e-06, l1: 0.99816, l2: 0.03183\n",
            "Loss: 6.873452e-06, l1: 0.99844, l2: 0.03182\n",
            "Loss: 6.857598e-06, l1: 0.99828, l2: 0.03182\n",
            "Loss: 6.848140e-06, l1: 0.99819, l2: 0.03182\n",
            "Loss: 6.841221e-06, l1: 0.99815, l2: 0.03182\n",
            "Loss: 6.835361e-06, l1: 0.99814, l2: 0.03182\n",
            "Loss: 6.827601e-06, l1: 0.99824, l2: 0.03181\n",
            "Loss: 6.814246e-06, l1: 0.99820, l2: 0.03180\n",
            "Loss: 6.803604e-06, l1: 0.99821, l2: 0.03180\n",
            "Loss: 6.793628e-06, l1: 0.99821, l2: 0.03179\n",
            "Loss: 6.787895e-06, l1: 0.99823, l2: 0.03179\n",
            "Loss: 6.781070e-06, l1: 0.99822, l2: 0.03179\n",
            "Loss: 6.764261e-06, l1: 0.99815, l2: 0.03179\n",
            "Loss: 6.754977e-06, l1: 0.99815, l2: 0.03179\n",
            "Loss: 6.746877e-06, l1: 0.99813, l2: 0.03178\n",
            "Loss: 6.738343e-06, l1: 0.99813, l2: 0.03178\n",
            "Loss: 6.724545e-06, l1: 0.99817, l2: 0.03178\n",
            "Loss: 6.719675e-06, l1: 0.99802, l2: 0.03177\n",
            "Loss: 6.719307e-06, l1: 0.99821, l2: 0.03177\n",
            "Loss: 6.709252e-06, l1: 0.99812, l2: 0.03177\n",
            "Loss: 6.688996e-06, l1: 0.99819, l2: 0.03177\n",
            "Loss: 6.677210e-06, l1: 0.99818, l2: 0.03177\n",
            "Loss: 6.656158e-06, l1: 0.99817, l2: 0.03177\n",
            "Loss: 6.636916e-06, l1: 0.99822, l2: 0.03177\n",
            "Loss: 6.637751e-06, l1: 0.99847, l2: 0.03177\n",
            "Loss: 6.625798e-06, l1: 0.99834, l2: 0.03177\n",
            "Loss: 6.609454e-06, l1: 0.99846, l2: 0.03177\n",
            "Loss: 6.597883e-06, l1: 0.99850, l2: 0.03177\n",
            "Loss: 6.579251e-06, l1: 0.99863, l2: 0.03177\n",
            "Loss: 6.568155e-06, l1: 0.99871, l2: 0.03177\n",
            "Loss: 6.551595e-06, l1: 0.99879, l2: 0.03178\n",
            "Loss: 6.548819e-06, l1: 0.99898, l2: 0.03178\n",
            "Loss: 6.542516e-06, l1: 0.99894, l2: 0.03178\n",
            "Loss: 6.532293e-06, l1: 0.99888, l2: 0.03178\n",
            "Loss: 6.523309e-06, l1: 0.99888, l2: 0.03178\n",
            "Loss: 6.503258e-06, l1: 0.99894, l2: 0.03178\n",
            "Loss: 6.488267e-06, l1: 0.99903, l2: 0.03178\n",
            "Loss: 6.468726e-06, l1: 0.99927, l2: 0.03179\n",
            "Loss: 6.465002e-06, l1: 0.99943, l2: 0.03179\n",
            "Loss: 6.445527e-06, l1: 0.99937, l2: 0.03178\n",
            "Loss: 6.431897e-06, l1: 0.99932, l2: 0.03178\n",
            "Loss: 6.420843e-06, l1: 0.99933, l2: 0.03179\n",
            "Loss: 6.406420e-06, l1: 0.99935, l2: 0.03179\n",
            "Loss: 6.393387e-06, l1: 0.99946, l2: 0.03179\n",
            "Loss: 6.383047e-06, l1: 0.99928, l2: 0.03179\n",
            "Loss: 6.372024e-06, l1: 0.99934, l2: 0.03179\n",
            "Loss: 6.361263e-06, l1: 0.99932, l2: 0.03179\n",
            "Loss: 6.353891e-06, l1: 0.99929, l2: 0.03180\n",
            "Loss: 6.347209e-06, l1: 0.99924, l2: 0.03180\n",
            "Loss: 6.338327e-06, l1: 0.99919, l2: 0.03181\n",
            "Loss: 6.330925e-06, l1: 0.99911, l2: 0.03182\n",
            "Loss: 6.323424e-06, l1: 0.99910, l2: 0.03182\n",
            "Loss: 6.317819e-06, l1: 0.99912, l2: 0.03182\n",
            "Loss: 6.313389e-06, l1: 0.99912, l2: 0.03182\n",
            "Loss: 6.300686e-06, l1: 0.99916, l2: 0.03183\n",
            "Loss: 6.287546e-06, l1: 0.99916, l2: 0.03183\n",
            "Loss: 6.269838e-06, l1: 0.99923, l2: 0.03184\n",
            "Loss: 6.266281e-06, l1: 0.99910, l2: 0.03184\n",
            "Loss: 6.237532e-06, l1: 0.99918, l2: 0.03184\n",
            "Loss: 6.225761e-06, l1: 0.99919, l2: 0.03184\n",
            "Loss: 6.215225e-06, l1: 0.99917, l2: 0.03184\n",
            "Loss: 6.203258e-06, l1: 0.99916, l2: 0.03184\n",
            "Loss: 6.368497e-06, l1: 0.99945, l2: 0.03184\n",
            "Loss: 6.198889e-06, l1: 0.99920, l2: 0.03184\n",
            "Loss: 6.184214e-06, l1: 0.99920, l2: 0.03185\n",
            "Loss: 6.173930e-06, l1: 0.99922, l2: 0.03185\n",
            "Loss: 6.161595e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 6.148320e-06, l1: 0.99923, l2: 0.03184\n",
            "Loss: 6.137270e-06, l1: 0.99929, l2: 0.03184\n",
            "Loss: 6.126152e-06, l1: 0.99921, l2: 0.03184\n",
            "Loss: 6.120550e-06, l1: 0.99915, l2: 0.03184\n",
            "Loss: 6.115275e-06, l1: 0.99910, l2: 0.03185\n",
            "Loss: 6.108316e-06, l1: 0.99904, l2: 0.03185\n",
            "Loss: 6.106972e-06, l1: 0.99892, l2: 0.03185\n",
            "Loss: 6.096265e-06, l1: 0.99891, l2: 0.03185\n",
            "Loss: 6.092043e-06, l1: 0.99892, l2: 0.03185\n",
            "Loss: 6.087914e-06, l1: 0.99892, l2: 0.03185\n",
            "Loss: 6.083881e-06, l1: 0.99891, l2: 0.03185\n",
            "Loss: 6.072734e-06, l1: 0.99885, l2: 0.03185\n",
            "Loss: 6.072683e-06, l1: 0.99887, l2: 0.03185\n",
            "Loss: 6.068905e-06, l1: 0.99886, l2: 0.03185\n",
            "Loss: 6.063051e-06, l1: 0.99886, l2: 0.03185\n",
            "Loss: 6.053854e-06, l1: 0.99885, l2: 0.03185\n",
            "Loss: 6.045175e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 6.033148e-06, l1: 0.99884, l2: 0.03185\n",
            "Loss: 6.316767e-06, l1: 0.99921, l2: 0.03183\n",
            "Loss: 6.028822e-06, l1: 0.99888, l2: 0.03185\n",
            "Loss: 6.013879e-06, l1: 0.99897, l2: 0.03185\n",
            "Loss: 6.006332e-06, l1: 0.99903, l2: 0.03185\n",
            "Loss: 5.991370e-06, l1: 0.99914, l2: 0.03185\n",
            "Loss: 5.987563e-06, l1: 0.99913, l2: 0.03185\n",
            "Loss: 5.982738e-06, l1: 0.99912, l2: 0.03185\n",
            "Loss: 5.975421e-06, l1: 0.99907, l2: 0.03185\n",
            "Loss: 5.966041e-06, l1: 0.99899, l2: 0.03185\n",
            "Loss: 5.961651e-06, l1: 0.99891, l2: 0.03185\n",
            "Loss: 5.953812e-06, l1: 0.99889, l2: 0.03185\n",
            "Loss: 5.962124e-06, l1: 0.99894, l2: 0.03185\n",
            "Loss: 5.948457e-06, l1: 0.99891, l2: 0.03185\n",
            "Loss: 5.939911e-06, l1: 0.99889, l2: 0.03185\n",
            "Loss: 5.913311e-06, l1: 0.99879, l2: 0.03184\n",
            "Loss: 5.897399e-06, l1: 0.99870, l2: 0.03184\n",
            "Loss: 5.915670e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 5.887757e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 5.871842e-06, l1: 0.99859, l2: 0.03184\n",
            "Loss: 5.869515e-06, l1: 0.99850, l2: 0.03185\n",
            "Loss: 5.857004e-06, l1: 0.99848, l2: 0.03185\n",
            "Loss: 5.850694e-06, l1: 0.99850, l2: 0.03185\n",
            "Loss: 5.835854e-06, l1: 0.99854, l2: 0.03185\n",
            "Loss: 5.826888e-06, l1: 0.99855, l2: 0.03185\n",
            "Loss: 5.817061e-06, l1: 0.99856, l2: 0.03185\n",
            "Loss: 5.802908e-06, l1: 0.99864, l2: 0.03185\n",
            "Loss: 5.783784e-06, l1: 0.99873, l2: 0.03184\n",
            "Loss: 5.768497e-06, l1: 0.99874, l2: 0.03183\n",
            "Loss: 5.754526e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 5.744488e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 5.735759e-06, l1: 0.99858, l2: 0.03184\n",
            "Loss: 5.731858e-06, l1: 0.99857, l2: 0.03184\n",
            "Loss: 5.728195e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 5.722649e-06, l1: 0.99864, l2: 0.03185\n",
            "Loss: 5.717754e-06, l1: 0.99867, l2: 0.03185\n",
            "Loss: 5.730593e-06, l1: 0.99865, l2: 0.03186\n",
            "Loss: 5.715270e-06, l1: 0.99866, l2: 0.03185\n",
            "Loss: 5.712816e-06, l1: 0.99865, l2: 0.03185\n",
            "Loss: 5.709442e-06, l1: 0.99862, l2: 0.03185\n",
            "Loss: 5.705306e-06, l1: 0.99857, l2: 0.03185\n",
            "Loss: 5.698198e-06, l1: 0.99845, l2: 0.03185\n",
            "Loss: 5.733447e-06, l1: 0.99851, l2: 0.03184\n",
            "Loss: 5.695195e-06, l1: 0.99847, l2: 0.03185\n",
            "Loss: 5.688493e-06, l1: 0.99840, l2: 0.03185\n",
            "Loss: 5.680447e-06, l1: 0.99836, l2: 0.03185\n",
            "Loss: 5.675672e-06, l1: 0.99837, l2: 0.03185\n",
            "Loss: 5.669611e-06, l1: 0.99839, l2: 0.03184\n",
            "Loss: 5.731998e-06, l1: 0.99832, l2: 0.03183\n",
            "Loss: 5.668003e-06, l1: 0.99838, l2: 0.03184\n",
            "Loss: 5.665210e-06, l1: 0.99837, l2: 0.03184\n",
            "Loss: 5.661103e-06, l1: 0.99838, l2: 0.03183\n",
            "Loss: 5.646404e-06, l1: 0.99834, l2: 0.03183\n",
            "Loss: 5.678826e-06, l1: 0.99854, l2: 0.03183\n",
            "Loss: 5.639811e-06, l1: 0.99840, l2: 0.03183\n",
            "Loss: 5.629970e-06, l1: 0.99834, l2: 0.03183\n",
            "Loss: 5.614888e-06, l1: 0.99831, l2: 0.03183\n",
            "Loss: 5.600715e-06, l1: 0.99833, l2: 0.03183\n",
            "Loss: 5.594840e-06, l1: 0.99836, l2: 0.03183\n",
            "Loss: 5.579275e-06, l1: 0.99843, l2: 0.03183\n",
            "Loss: 5.577308e-06, l1: 0.99870, l2: 0.03184\n",
            "Loss: 5.568871e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 5.559543e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 5.565820e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 5.553287e-06, l1: 0.99858, l2: 0.03184\n",
            "Loss: 5.541999e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 5.536607e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 5.525817e-06, l1: 0.99863, l2: 0.03183\n",
            "Loss: 5.518263e-06, l1: 0.99864, l2: 0.03183\n",
            "Loss: 5.504694e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 5.510230e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 5.497121e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 5.485502e-06, l1: 0.99869, l2: 0.03184\n",
            "Loss: 5.471541e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 5.462724e-06, l1: 0.99883, l2: 0.03185\n",
            "Loss: 5.452984e-06, l1: 0.99883, l2: 0.03185\n",
            "Loss: 5.417919e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 5.398152e-06, l1: 0.99896, l2: 0.03185\n",
            "Loss: 5.378355e-06, l1: 0.99909, l2: 0.03186\n",
            "Loss: 5.421425e-06, l1: 0.99912, l2: 0.03185\n",
            "Loss: 5.374860e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 5.365059e-06, l1: 0.99916, l2: 0.03186\n",
            "Loss: 5.346262e-06, l1: 0.99921, l2: 0.03186\n",
            "Loss: 5.332172e-06, l1: 0.99925, l2: 0.03186\n",
            "Loss: 5.343048e-06, l1: 0.99906, l2: 0.03186\n",
            "Loss: 5.325277e-06, l1: 0.99918, l2: 0.03186\n",
            "Loss: 5.318451e-06, l1: 0.99912, l2: 0.03186\n",
            "Loss: 5.306831e-06, l1: 0.99898, l2: 0.03186\n",
            "Loss: 5.298938e-06, l1: 0.99890, l2: 0.03186\n",
            "Loss: 5.286338e-06, l1: 0.99880, l2: 0.03186\n",
            "Loss: 5.276372e-06, l1: 0.99879, l2: 0.03187\n",
            "Loss: 5.282049e-06, l1: 0.99878, l2: 0.03187\n",
            "Loss: 5.271561e-06, l1: 0.99879, l2: 0.03187\n",
            "Loss: 5.265024e-06, l1: 0.99885, l2: 0.03187\n",
            "Loss: 5.258041e-06, l1: 0.99891, l2: 0.03187\n",
            "Loss: 5.250905e-06, l1: 0.99896, l2: 0.03187\n",
            "Loss: 5.241471e-06, l1: 0.99897, l2: 0.03186\n",
            "Loss: 5.232090e-06, l1: 0.99899, l2: 0.03186\n",
            "Loss: 5.237420e-06, l1: 0.99894, l2: 0.03186\n",
            "Loss: 5.228237e-06, l1: 0.99897, l2: 0.03186\n",
            "Loss: 5.222347e-06, l1: 0.99895, l2: 0.03186\n",
            "Loss: 5.217131e-06, l1: 0.99894, l2: 0.03187\n",
            "Loss: 5.212245e-06, l1: 0.99894, l2: 0.03187\n",
            "Loss: 5.202337e-06, l1: 0.99898, l2: 0.03187\n",
            "Loss: 5.190799e-06, l1: 0.99909, l2: 0.03188\n",
            "Loss: 5.236696e-06, l1: 0.99900, l2: 0.03190\n",
            "Loss: 5.184980e-06, l1: 0.99907, l2: 0.03188\n",
            "Loss: 5.175096e-06, l1: 0.99914, l2: 0.03188\n",
            "Loss: 5.165815e-06, l1: 0.99920, l2: 0.03189\n",
            "Loss: 5.160910e-06, l1: 0.99922, l2: 0.03189\n",
            "Loss: 5.164182e-06, l1: 0.99920, l2: 0.03189\n",
            "Loss: 5.158058e-06, l1: 0.99921, l2: 0.03189\n",
            "Loss: 5.152594e-06, l1: 0.99925, l2: 0.03189\n",
            "Loss: 5.149388e-06, l1: 0.99927, l2: 0.03189\n",
            "Loss: 5.146711e-06, l1: 0.99928, l2: 0.03189\n",
            "Loss: 5.142991e-06, l1: 0.99928, l2: 0.03189\n",
            "Loss: 5.142143e-06, l1: 0.99932, l2: 0.03188\n",
            "Loss: 5.135287e-06, l1: 0.99930, l2: 0.03188\n",
            "Loss: 5.131340e-06, l1: 0.99928, l2: 0.03188\n",
            "Loss: 5.125251e-06, l1: 0.99927, l2: 0.03188\n",
            "Loss: 5.118951e-06, l1: 0.99924, l2: 0.03188\n",
            "Loss: 5.115714e-06, l1: 0.99930, l2: 0.03187\n",
            "Loss: 5.125525e-06, l1: 0.99928, l2: 0.03187\n",
            "Loss: 5.112761e-06, l1: 0.99929, l2: 0.03187\n",
            "Loss: 5.108740e-06, l1: 0.99926, l2: 0.03187\n",
            "Loss: 5.099083e-06, l1: 0.99922, l2: 0.03187\n",
            "Loss: 5.078722e-06, l1: 0.99903, l2: 0.03188\n",
            "Loss: 5.059188e-06, l1: 0.99903, l2: 0.03188\n",
            "Loss: 5.040787e-06, l1: 0.99904, l2: 0.03187\n",
            "Loss: 5.040379e-06, l1: 0.99912, l2: 0.03187\n",
            "Loss: 5.025848e-06, l1: 0.99908, l2: 0.03187\n",
            "Loss: 4.998993e-06, l1: 0.99912, l2: 0.03187\n",
            "Loss: 4.984055e-06, l1: 0.99914, l2: 0.03187\n",
            "Loss: 4.967577e-06, l1: 0.99914, l2: 0.03187\n",
            "Loss: 4.951508e-06, l1: 0.99910, l2: 0.03187\n",
            "Loss: 4.954079e-06, l1: 0.99900, l2: 0.03187\n",
            "Loss: 4.938468e-06, l1: 0.99906, l2: 0.03187\n",
            "Loss: 4.919488e-06, l1: 0.99897, l2: 0.03187\n",
            "Loss: 4.898338e-06, l1: 0.99887, l2: 0.03187\n",
            "Loss: 4.884901e-06, l1: 0.99872, l2: 0.03186\n",
            "Loss: 4.891934e-06, l1: 0.99865, l2: 0.03186\n",
            "Loss: 4.878339e-06, l1: 0.99869, l2: 0.03186\n",
            "Loss: 4.873800e-06, l1: 0.99870, l2: 0.03186\n",
            "Loss: 4.864835e-06, l1: 0.99871, l2: 0.03186\n",
            "Loss: 4.851654e-06, l1: 0.99867, l2: 0.03186\n",
            "Loss: 4.932923e-06, l1: 0.99814, l2: 0.03185\n",
            "Loss: 4.844600e-06, l1: 0.99855, l2: 0.03186\n",
            "Loss: 4.826095e-06, l1: 0.99841, l2: 0.03185\n",
            "Loss: 4.818560e-06, l1: 0.99832, l2: 0.03185\n",
            "Loss: 4.810131e-06, l1: 0.99823, l2: 0.03185\n",
            "Loss: 4.796454e-06, l1: 0.99811, l2: 0.03185\n",
            "Loss: 4.807445e-06, l1: 0.99798, l2: 0.03185\n",
            "Loss: 4.792118e-06, l1: 0.99807, l2: 0.03185\n",
            "Loss: 4.781750e-06, l1: 0.99802, l2: 0.03185\n",
            "Loss: 4.772663e-06, l1: 0.99802, l2: 0.03186\n",
            "Loss: 4.766096e-06, l1: 0.99802, l2: 0.03186\n",
            "Loss: 4.754850e-06, l1: 0.99798, l2: 0.03186\n",
            "Loss: 4.736539e-06, l1: 0.99788, l2: 0.03186\n",
            "Loss: 4.736014e-06, l1: 0.99758, l2: 0.03187\n",
            "Loss: 4.726725e-06, l1: 0.99773, l2: 0.03187\n",
            "Loss: 4.718704e-06, l1: 0.99768, l2: 0.03187\n",
            "Loss: 4.712607e-06, l1: 0.99761, l2: 0.03187\n",
            "Loss: 4.709045e-06, l1: 0.99756, l2: 0.03187\n",
            "Loss: 4.703866e-06, l1: 0.99750, l2: 0.03187\n",
            "Loss: 4.695572e-06, l1: 0.99738, l2: 0.03187\n",
            "Loss: 4.685063e-06, l1: 0.99725, l2: 0.03187\n",
            "Loss: 4.677252e-06, l1: 0.99716, l2: 0.03187\n",
            "Loss: 4.669768e-06, l1: 0.99714, l2: 0.03187\n",
            "Loss: 4.724456e-06, l1: 0.99667, l2: 0.03187\n",
            "Loss: 4.667807e-06, l1: 0.99706, l2: 0.03187\n",
            "Loss: 4.662360e-06, l1: 0.99708, l2: 0.03187\n",
            "Loss: 4.693781e-06, l1: 0.99727, l2: 0.03186\n",
            "Loss: 4.660294e-06, l1: 0.99712, l2: 0.03187\n",
            "Loss: 4.652000e-06, l1: 0.99715, l2: 0.03186\n",
            "Loss: 4.642797e-06, l1: 0.99716, l2: 0.03186\n",
            "Loss: 4.631044e-06, l1: 0.99719, l2: 0.03186\n",
            "Loss: 4.617306e-06, l1: 0.99727, l2: 0.03185\n",
            "Loss: 5.035718e-06, l1: 0.99731, l2: 0.03186\n",
            "Loss: 4.615517e-06, l1: 0.99728, l2: 0.03185\n",
            "Loss: 4.602903e-06, l1: 0.99739, l2: 0.03185\n",
            "Loss: 4.594352e-06, l1: 0.99748, l2: 0.03185\n",
            "Loss: 4.583794e-06, l1: 0.99758, l2: 0.03185\n",
            "Loss: 4.573444e-06, l1: 0.99768, l2: 0.03185\n",
            "Loss: 4.581773e-06, l1: 0.99769, l2: 0.03185\n",
            "Loss: 4.565973e-06, l1: 0.99768, l2: 0.03185\n",
            "Loss: 4.553300e-06, l1: 0.99784, l2: 0.03185\n",
            "Loss: 4.543226e-06, l1: 0.99789, l2: 0.03185\n",
            "Loss: 4.537933e-06, l1: 0.99783, l2: 0.03185\n",
            "Loss: 4.535340e-06, l1: 0.99782, l2: 0.03185\n",
            "Loss: 4.532417e-06, l1: 0.99783, l2: 0.03185\n",
            "Loss: 4.524928e-06, l1: 0.99789, l2: 0.03185\n",
            "Loss: 4.517850e-06, l1: 0.99800, l2: 0.03185\n",
            "Loss: 4.510933e-06, l1: 0.99803, l2: 0.03185\n",
            "Loss: 4.499367e-06, l1: 0.99805, l2: 0.03185\n",
            "Loss: 4.490528e-06, l1: 0.99801, l2: 0.03184\n",
            "Loss: 4.504815e-06, l1: 0.99797, l2: 0.03184\n",
            "Loss: 4.485772e-06, l1: 0.99800, l2: 0.03184\n",
            "Loss: 4.477988e-06, l1: 0.99794, l2: 0.03184\n",
            "Loss: 4.471313e-06, l1: 0.99789, l2: 0.03184\n",
            "Loss: 4.458847e-06, l1: 0.99784, l2: 0.03183\n",
            "Loss: 4.451020e-06, l1: 0.99784, l2: 0.03182\n",
            "Loss: 4.440634e-06, l1: 0.99785, l2: 0.03181\n",
            "Loss: 4.435731e-06, l1: 0.99790, l2: 0.03182\n",
            "Loss: 4.425435e-06, l1: 0.99800, l2: 0.03181\n",
            "Loss: 4.414655e-06, l1: 0.99803, l2: 0.03181\n",
            "Loss: 4.414518e-06, l1: 0.99822, l2: 0.03180\n",
            "Loss: 4.404649e-06, l1: 0.99812, l2: 0.03180\n",
            "Loss: 4.392700e-06, l1: 0.99805, l2: 0.03180\n",
            "Loss: 4.384734e-06, l1: 0.99793, l2: 0.03179\n",
            "Loss: 4.380680e-06, l1: 0.99790, l2: 0.03179\n",
            "Loss: 4.370525e-06, l1: 0.99789, l2: 0.03179\n",
            "Loss: 4.363568e-06, l1: 0.99787, l2: 0.03179\n",
            "Loss: 4.357763e-06, l1: 0.99793, l2: 0.03179\n",
            "Loss: 4.349799e-06, l1: 0.99806, l2: 0.03179\n",
            "Loss: 4.341176e-06, l1: 0.99816, l2: 0.03179\n",
            "Loss: 4.319971e-06, l1: 0.99840, l2: 0.03179\n",
            "Loss: 4.306798e-06, l1: 0.99861, l2: 0.03178\n",
            "Loss: 4.296664e-06, l1: 0.99869, l2: 0.03178\n",
            "Loss: 4.285583e-06, l1: 0.99867, l2: 0.03177\n",
            "Loss: 4.279249e-06, l1: 0.99862, l2: 0.03178\n",
            "Loss: 4.272228e-06, l1: 0.99860, l2: 0.03178\n",
            "Loss: 4.265899e-06, l1: 0.99863, l2: 0.03178\n",
            "Loss: 4.253543e-06, l1: 0.99869, l2: 0.03178\n",
            "Loss: 4.240009e-06, l1: 0.99880, l2: 0.03178\n",
            "Loss: 4.232514e-06, l1: 0.99880, l2: 0.03178\n",
            "Loss: 4.222797e-06, l1: 0.99880, l2: 0.03178\n",
            "Loss: 4.216763e-06, l1: 0.99879, l2: 0.03178\n",
            "Loss: 4.219219e-06, l1: 0.99878, l2: 0.03178\n",
            "Loss: 4.214268e-06, l1: 0.99878, l2: 0.03178\n",
            "Loss: 4.209771e-06, l1: 0.99878, l2: 0.03178\n",
            "Loss: 4.202169e-06, l1: 0.99879, l2: 0.03178\n",
            "Loss: 4.192524e-06, l1: 0.99885, l2: 0.03178\n",
            "Loss: 4.181375e-06, l1: 0.99888, l2: 0.03178\n",
            "Loss: 4.204286e-06, l1: 0.99875, l2: 0.03178\n",
            "Loss: 4.177113e-06, l1: 0.99885, l2: 0.03178\n",
            "Loss: 4.165355e-06, l1: 0.99882, l2: 0.03178\n",
            "Loss: 4.158106e-06, l1: 0.99872, l2: 0.03179\n",
            "Loss: 4.154477e-06, l1: 0.99863, l2: 0.03179\n",
            "Loss: 4.148721e-06, l1: 0.99857, l2: 0.03179\n",
            "Loss: 4.142798e-06, l1: 0.99846, l2: 0.03179\n",
            "Loss: 4.140672e-06, l1: 0.99856, l2: 0.03180\n",
            "Loss: 4.136573e-06, l1: 0.99857, l2: 0.03179\n",
            "Loss: 4.133444e-06, l1: 0.99866, l2: 0.03179\n",
            "Loss: 4.128758e-06, l1: 0.99876, l2: 0.03179\n",
            "Loss: 4.120365e-06, l1: 0.99891, l2: 0.03179\n",
            "Loss: 4.111164e-06, l1: 0.99895, l2: 0.03180\n",
            "Loss: 4.106029e-06, l1: 0.99886, l2: 0.03180\n",
            "Loss: 4.103344e-06, l1: 0.99883, l2: 0.03180\n",
            "Loss: 4.101510e-06, l1: 0.99880, l2: 0.03180\n",
            "Loss: 4.098950e-06, l1: 0.99879, l2: 0.03180\n",
            "Loss: 4.093256e-06, l1: 0.99878, l2: 0.03180\n",
            "Loss: 4.094522e-06, l1: 0.99881, l2: 0.03180\n",
            "Loss: 4.090404e-06, l1: 0.99880, l2: 0.03180\n",
            "Loss: 4.084339e-06, l1: 0.99885, l2: 0.03181\n",
            "Loss: 4.080739e-06, l1: 0.99885, l2: 0.03180\n",
            "Loss: 4.077642e-06, l1: 0.99887, l2: 0.03180\n",
            "Loss: 4.075116e-06, l1: 0.99888, l2: 0.03180\n",
            "Loss: 4.069098e-06, l1: 0.99891, l2: 0.03180\n",
            "Loss: 4.065730e-06, l1: 0.99893, l2: 0.03180\n",
            "Loss: 4.060442e-06, l1: 0.99893, l2: 0.03180\n",
            "Loss: 4.056986e-06, l1: 0.99890, l2: 0.03181\n",
            "Loss: 4.052622e-06, l1: 0.99889, l2: 0.03181\n",
            "Loss: 4.068204e-06, l1: 0.99867, l2: 0.03181\n",
            "Loss: 4.050518e-06, l1: 0.99884, l2: 0.03181\n",
            "Loss: 4.046906e-06, l1: 0.99883, l2: 0.03181\n",
            "Loss: 4.043635e-06, l1: 0.99883, l2: 0.03181\n",
            "Loss: 4.040963e-06, l1: 0.99882, l2: 0.03181\n",
            "Loss: 4.040724e-06, l1: 0.99880, l2: 0.03182\n",
            "Loss: 4.036888e-06, l1: 0.99880, l2: 0.03182\n",
            "Loss: 4.034818e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 4.032687e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 4.030585e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 4.024178e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 4.017613e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 4.066700e-06, l1: 0.99882, l2: 0.03183\n",
            "Loss: 4.015533e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 4.011803e-06, l1: 0.99882, l2: 0.03183\n",
            "Loss: 4.008671e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 4.005849e-06, l1: 0.99894, l2: 0.03183\n",
            "Loss: 4.001014e-06, l1: 0.99902, l2: 0.03183\n",
            "Loss: 3.994913e-06, l1: 0.99916, l2: 0.03183\n",
            "Loss: 3.989939e-06, l1: 0.99927, l2: 0.03183\n",
            "Loss: 3.988226e-06, l1: 0.99942, l2: 0.03184\n",
            "Loss: 3.984747e-06, l1: 0.99941, l2: 0.03184\n",
            "Loss: 3.981431e-06, l1: 0.99934, l2: 0.03184\n",
            "Loss: 3.980447e-06, l1: 0.99934, l2: 0.03184\n",
            "Loss: 3.977412e-06, l1: 0.99932, l2: 0.03184\n",
            "Loss: 3.984303e-06, l1: 0.99929, l2: 0.03184\n",
            "Loss: 3.975516e-06, l1: 0.99931, l2: 0.03184\n",
            "Loss: 3.972688e-06, l1: 0.99928, l2: 0.03184\n",
            "Loss: 3.969639e-06, l1: 0.99928, l2: 0.03184\n",
            "Loss: 3.968270e-06, l1: 0.99930, l2: 0.03184\n",
            "Loss: 3.965514e-06, l1: 0.99933, l2: 0.03184\n",
            "Loss: 3.959203e-06, l1: 0.99936, l2: 0.03184\n",
            "Loss: 3.946544e-06, l1: 0.99946, l2: 0.03184\n",
            "Loss: 3.935135e-06, l1: 0.99932, l2: 0.03185\n",
            "Loss: 3.928893e-06, l1: 0.99940, l2: 0.03185\n",
            "Loss: 3.921942e-06, l1: 0.99934, l2: 0.03185\n",
            "Loss: 3.916907e-06, l1: 0.99928, l2: 0.03184\n",
            "Loss: 3.914311e-06, l1: 0.99926, l2: 0.03184\n",
            "Loss: 3.908696e-06, l1: 0.99920, l2: 0.03184\n",
            "Loss: 3.933640e-06, l1: 0.99950, l2: 0.03185\n",
            "Loss: 3.907445e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 3.903895e-06, l1: 0.99922, l2: 0.03185\n",
            "Loss: 3.899832e-06, l1: 0.99919, l2: 0.03185\n",
            "Loss: 3.894114e-06, l1: 0.99921, l2: 0.03185\n",
            "Loss: 3.889721e-06, l1: 0.99916, l2: 0.03185\n",
            "Loss: 3.884868e-06, l1: 0.99920, l2: 0.03184\n",
            "Loss: 3.881364e-06, l1: 0.99917, l2: 0.03184\n",
            "Loss: 3.876049e-06, l1: 0.99918, l2: 0.03184\n",
            "Loss: 3.871136e-06, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.863418e-06, l1: 0.99914, l2: 0.03185\n",
            "Loss: 3.855168e-06, l1: 0.99917, l2: 0.03185\n",
            "Loss: 3.845963e-06, l1: 0.99918, l2: 0.03185\n",
            "Loss: 3.841919e-06, l1: 0.99922, l2: 0.03186\n",
            "Loss: 3.834067e-06, l1: 0.99918, l2: 0.03186\n",
            "Loss: 3.829234e-06, l1: 0.99914, l2: 0.03186\n",
            "Loss: 3.826251e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 3.822334e-06, l1: 0.99909, l2: 0.03186\n",
            "Loss: 3.818591e-06, l1: 0.99900, l2: 0.03186\n",
            "Loss: 3.811536e-06, l1: 0.99905, l2: 0.03186\n",
            "Loss: 3.806264e-06, l1: 0.99913, l2: 0.03186\n",
            "Loss: 3.802327e-06, l1: 0.99917, l2: 0.03186\n",
            "Loss: 3.803188e-06, l1: 0.99921, l2: 0.03186\n",
            "Loss: 3.800887e-06, l1: 0.99919, l2: 0.03186\n",
            "Loss: 3.798736e-06, l1: 0.99921, l2: 0.03186\n",
            "Loss: 3.796732e-06, l1: 0.99921, l2: 0.03186\n",
            "Loss: 3.794110e-06, l1: 0.99921, l2: 0.03187\n",
            "Loss: 3.788973e-06, l1: 0.99918, l2: 0.03187\n",
            "Loss: 3.783052e-06, l1: 0.99910, l2: 0.03187\n",
            "Loss: 3.776854e-06, l1: 0.99904, l2: 0.03187\n",
            "Loss: 3.772555e-06, l1: 0.99899, l2: 0.03187\n",
            "Loss: 3.767566e-06, l1: 0.99895, l2: 0.03187\n",
            "Loss: 3.759782e-06, l1: 0.99888, l2: 0.03187\n",
            "Loss: 3.768547e-06, l1: 0.99888, l2: 0.03187\n",
            "Loss: 3.755599e-06, l1: 0.99888, l2: 0.03187\n",
            "Loss: 3.749422e-06, l1: 0.99885, l2: 0.03187\n",
            "Loss: 3.744481e-06, l1: 0.99885, l2: 0.03187\n",
            "Loss: 3.741528e-06, l1: 0.99885, l2: 0.03187\n",
            "Loss: 3.737008e-06, l1: 0.99884, l2: 0.03187\n",
            "Loss: 3.735868e-06, l1: 0.99882, l2: 0.03187\n",
            "Loss: 3.730387e-06, l1: 0.99884, l2: 0.03187\n",
            "Loss: 3.728508e-06, l1: 0.99884, l2: 0.03187\n",
            "Loss: 3.725143e-06, l1: 0.99882, l2: 0.03186\n",
            "Loss: 3.746288e-06, l1: 0.99890, l2: 0.03186\n",
            "Loss: 3.723598e-06, l1: 0.99884, l2: 0.03186\n",
            "Loss: 3.719398e-06, l1: 0.99882, l2: 0.03186\n",
            "Loss: 3.711147e-06, l1: 0.99878, l2: 0.03185\n",
            "Loss: 3.698877e-06, l1: 0.99875, l2: 0.03184\n",
            "Loss: 3.692212e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 3.682333e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 3.677806e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 3.674483e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 3.670049e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.665975e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.658213e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 3.654506e-06, l1: 0.99890, l2: 0.03184\n",
            "Loss: 3.648391e-06, l1: 0.99899, l2: 0.03184\n",
            "Loss: 3.656266e-06, l1: 0.99873, l2: 0.03185\n",
            "Loss: 3.645249e-06, l1: 0.99890, l2: 0.03185\n",
            "Loss: 3.639869e-06, l1: 0.99894, l2: 0.03185\n",
            "Loss: 3.632117e-06, l1: 0.99899, l2: 0.03185\n",
            "Loss: 3.626515e-06, l1: 0.99893, l2: 0.03186\n",
            "Loss: 3.620330e-06, l1: 0.99894, l2: 0.03186\n",
            "Loss: 3.616050e-06, l1: 0.99891, l2: 0.03185\n",
            "Loss: 3.608816e-06, l1: 0.99886, l2: 0.03185\n",
            "Loss: 3.603940e-06, l1: 0.99882, l2: 0.03185\n",
            "Loss: 3.597093e-06, l1: 0.99882, l2: 0.03184\n",
            "Loss: 3.593635e-06, l1: 0.99874, l2: 0.03185\n",
            "Loss: 3.591028e-06, l1: 0.99879, l2: 0.03185\n",
            "Loss: 3.588736e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 3.586592e-06, l1: 0.99883, l2: 0.03185\n",
            "Loss: 3.582123e-06, l1: 0.99880, l2: 0.03185\n",
            "Loss: 3.573429e-06, l1: 0.99871, l2: 0.03185\n",
            "Loss: 3.565115e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 3.554761e-06, l1: 0.99856, l2: 0.03184\n",
            "Loss: 3.543907e-06, l1: 0.99850, l2: 0.03183\n",
            "Loss: 3.535506e-06, l1: 0.99860, l2: 0.03182\n",
            "Loss: 3.530891e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 3.528540e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 3.527546e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 3.526244e-06, l1: 0.99873, l2: 0.03183\n",
            "Loss: 3.524986e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 3.523333e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 3.520516e-06, l1: 0.99867, l2: 0.03183\n",
            "Loss: 3.516829e-06, l1: 0.99867, l2: 0.03183\n",
            "Loss: 3.514764e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 3.510851e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 3.508424e-06, l1: 0.99869, l2: 0.03183\n",
            "Loss: 3.505124e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 3.497356e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 3.562469e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.495760e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 3.490599e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.487327e-06, l1: 0.99863, l2: 0.03183\n",
            "Loss: 3.486874e-06, l1: 0.99864, l2: 0.03183\n",
            "Loss: 3.484587e-06, l1: 0.99864, l2: 0.03183\n",
            "Loss: 3.483452e-06, l1: 0.99863, l2: 0.03183\n",
            "Loss: 3.481064e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.478406e-06, l1: 0.99861, l2: 0.03183\n",
            "Loss: 3.491301e-06, l1: 0.99859, l2: 0.03183\n",
            "Loss: 3.477454e-06, l1: 0.99861, l2: 0.03183\n",
            "Loss: 3.474571e-06, l1: 0.99861, l2: 0.03183\n",
            "Loss: 3.472675e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.471193e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.470034e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.467851e-06, l1: 0.99860, l2: 0.03183\n",
            "Loss: 3.465314e-06, l1: 0.99853, l2: 0.03184\n",
            "Loss: 3.462489e-06, l1: 0.99852, l2: 0.03183\n",
            "Loss: 3.459109e-06, l1: 0.99852, l2: 0.03183\n",
            "Loss: 3.456366e-06, l1: 0.99852, l2: 0.03183\n",
            "Loss: 3.450651e-06, l1: 0.99849, l2: 0.03182\n",
            "Loss: 3.447532e-06, l1: 0.99857, l2: 0.03182\n",
            "Loss: 3.443968e-06, l1: 0.99854, l2: 0.03182\n",
            "Loss: 3.440846e-06, l1: 0.99855, l2: 0.03183\n",
            "Loss: 3.436294e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 3.429023e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 3.431667e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 3.425863e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.419775e-06, l1: 0.99864, l2: 0.03183\n",
            "Loss: 3.414289e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.407126e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 3.398258e-06, l1: 0.99855, l2: 0.03183\n",
            "Loss: 3.397389e-06, l1: 0.99844, l2: 0.03183\n",
            "Loss: 3.388328e-06, l1: 0.99850, l2: 0.03183\n",
            "Loss: 3.385584e-06, l1: 0.99853, l2: 0.03183\n",
            "Loss: 3.381839e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 3.379396e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 3.377793e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 3.375508e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 3.373357e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 3.368807e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 3.364536e-06, l1: 0.99864, l2: 0.03184\n",
            "Loss: 3.362052e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 3.360337e-06, l1: 0.99859, l2: 0.03184\n",
            "Loss: 3.359076e-06, l1: 0.99859, l2: 0.03184\n",
            "Loss: 3.357213e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 3.353646e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 3.355610e-06, l1: 0.99869, l2: 0.03183\n",
            "Loss: 3.351139e-06, l1: 0.99865, l2: 0.03184\n",
            "Loss: 3.348367e-06, l1: 0.99865, l2: 0.03184\n",
            "Loss: 3.345228e-06, l1: 0.99865, l2: 0.03184\n",
            "Loss: 3.342709e-06, l1: 0.99866, l2: 0.03184\n",
            "Loss: 3.338589e-06, l1: 0.99864, l2: 0.03184\n",
            "Loss: 3.331254e-06, l1: 0.99859, l2: 0.03184\n",
            "Loss: 3.325329e-06, l1: 0.99853, l2: 0.03184\n",
            "Loss: 3.322210e-06, l1: 0.99852, l2: 0.03184\n",
            "Loss: 3.333917e-06, l1: 0.99850, l2: 0.03184\n",
            "Loss: 3.321311e-06, l1: 0.99852, l2: 0.03184\n",
            "Loss: 3.318941e-06, l1: 0.99853, l2: 0.03184\n",
            "Loss: 3.316253e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 3.313965e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 3.310916e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 3.309766e-06, l1: 0.99864, l2: 0.03184\n",
            "Loss: 3.308943e-06, l1: 0.99865, l2: 0.03184\n",
            "Loss: 3.307444e-06, l1: 0.99866, l2: 0.03184\n",
            "Loss: 3.305332e-06, l1: 0.99869, l2: 0.03184\n",
            "Loss: 3.301560e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 3.298654e-06, l1: 0.99874, l2: 0.03184\n",
            "Loss: 3.296171e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 3.293658e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 3.285845e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.286957e-06, l1: 0.99900, l2: 0.03184\n",
            "Loss: 3.283236e-06, l1: 0.99893, l2: 0.03184\n",
            "Loss: 3.279017e-06, l1: 0.99893, l2: 0.03184\n",
            "Loss: 3.277470e-06, l1: 0.99890, l2: 0.03184\n",
            "Loss: 3.276242e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.274233e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 3.271669e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 3.267550e-06, l1: 0.99888, l2: 0.03184\n",
            "Loss: 3.265507e-06, l1: 0.99892, l2: 0.03184\n",
            "Loss: 3.263330e-06, l1: 0.99898, l2: 0.03184\n",
            "Loss: 3.261752e-06, l1: 0.99900, l2: 0.03184\n",
            "Loss: 3.259271e-06, l1: 0.99905, l2: 0.03184\n",
            "Loss: 3.259808e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.257498e-06, l1: 0.99905, l2: 0.03184\n",
            "Loss: 3.255630e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.253067e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 3.251508e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 3.247071e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.241269e-06, l1: 0.99906, l2: 0.03184\n",
            "Loss: 3.249193e-06, l1: 0.99930, l2: 0.03184\n",
            "Loss: 3.238286e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 3.234183e-06, l1: 0.99913, l2: 0.03184\n",
            "Loss: 3.231251e-06, l1: 0.99913, l2: 0.03184\n",
            "Loss: 3.228574e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 3.224786e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 3.221247e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 3.217229e-06, l1: 0.99911, l2: 0.03185\n",
            "Loss: 3.215203e-06, l1: 0.99913, l2: 0.03185\n",
            "Loss: 3.260615e-06, l1: 0.99906, l2: 0.03184\n",
            "Loss: 3.214768e-06, l1: 0.99913, l2: 0.03184\n",
            "Loss: 3.213592e-06, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.210773e-06, l1: 0.99918, l2: 0.03185\n",
            "Loss: 3.208326e-06, l1: 0.99924, l2: 0.03185\n",
            "Loss: 3.205038e-06, l1: 0.99922, l2: 0.03185\n",
            "Loss: 3.202180e-06, l1: 0.99920, l2: 0.03185\n",
            "Loss: 3.199887e-06, l1: 0.99919, l2: 0.03185\n",
            "Loss: 3.197857e-06, l1: 0.99915, l2: 0.03185\n",
            "Loss: 3.195036e-06, l1: 0.99916, l2: 0.03184\n",
            "Loss: 3.191089e-06, l1: 0.99918, l2: 0.03184\n",
            "Loss: 3.188855e-06, l1: 0.99918, l2: 0.03184\n",
            "Loss: 3.185842e-06, l1: 0.99915, l2: 0.03183\n",
            "Loss: 3.183649e-06, l1: 0.99913, l2: 0.03183\n",
            "Loss: 3.182132e-06, l1: 0.99908, l2: 0.03183\n",
            "Loss: 3.181117e-06, l1: 0.99908, l2: 0.03183\n",
            "Loss: 3.179688e-06, l1: 0.99908, l2: 0.03183\n",
            "Loss: 3.180023e-06, l1: 0.99907, l2: 0.03183\n",
            "Loss: 3.179165e-06, l1: 0.99908, l2: 0.03183\n",
            "Loss: 3.178173e-06, l1: 0.99907, l2: 0.03183\n",
            "Loss: 3.176145e-06, l1: 0.99908, l2: 0.03184\n",
            "Loss: 3.173216e-06, l1: 0.99909, l2: 0.03184\n",
            "Loss: 3.169111e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 3.179392e-06, l1: 0.99912, l2: 0.03185\n",
            "Loss: 3.168173e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 3.165293e-06, l1: 0.99913, l2: 0.03184\n",
            "Loss: 3.162068e-06, l1: 0.99913, l2: 0.03185\n",
            "Loss: 3.158974e-06, l1: 0.99912, l2: 0.03185\n",
            "Loss: 3.155066e-06, l1: 0.99910, l2: 0.03185\n",
            "Loss: 3.152358e-06, l1: 0.99899, l2: 0.03185\n",
            "Loss: 3.150567e-06, l1: 0.99900, l2: 0.03184\n",
            "Loss: 3.148570e-06, l1: 0.99900, l2: 0.03184\n",
            "Loss: 3.147015e-06, l1: 0.99899, l2: 0.03185\n",
            "Loss: 3.145291e-06, l1: 0.99897, l2: 0.03185\n",
            "Loss: 3.142700e-06, l1: 0.99896, l2: 0.03185\n",
            "Loss: 3.141299e-06, l1: 0.99890, l2: 0.03185\n",
            "Loss: 3.135801e-06, l1: 0.99895, l2: 0.03185\n",
            "Loss: 3.132686e-06, l1: 0.99897, l2: 0.03185\n",
            "Loss: 3.128685e-06, l1: 0.99901, l2: 0.03185\n",
            "Loss: 3.124902e-06, l1: 0.99904, l2: 0.03185\n",
            "Loss: 3.124646e-06, l1: 0.99896, l2: 0.03184\n",
            "Loss: 3.121471e-06, l1: 0.99899, l2: 0.03185\n",
            "Loss: 3.115251e-06, l1: 0.99900, l2: 0.03185\n",
            "Loss: 3.111097e-06, l1: 0.99896, l2: 0.03185\n",
            "Loss: 3.105373e-06, l1: 0.99885, l2: 0.03185\n",
            "Loss: 3.099137e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 3.095007e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 3.090533e-06, l1: 0.99880, l2: 0.03185\n",
            "Loss: 3.086617e-06, l1: 0.99882, l2: 0.03185\n",
            "Loss: 3.099404e-06, l1: 0.99871, l2: 0.03186\n",
            "Loss: 3.083391e-06, l1: 0.99879, l2: 0.03185\n",
            "Loss: 3.077483e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 3.071766e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 3.068021e-06, l1: 0.99878, l2: 0.03185\n",
            "Loss: 3.067188e-06, l1: 0.99871, l2: 0.03186\n",
            "Loss: 3.061889e-06, l1: 0.99870, l2: 0.03185\n",
            "Loss: 3.059698e-06, l1: 0.99869, l2: 0.03185\n",
            "Loss: 3.056408e-06, l1: 0.99867, l2: 0.03185\n",
            "Loss: 3.054545e-06, l1: 0.99864, l2: 0.03185\n",
            "Loss: 3.052425e-06, l1: 0.99865, l2: 0.03185\n",
            "Loss: 3.049132e-06, l1: 0.99866, l2: 0.03185\n",
            "Loss: 3.046878e-06, l1: 0.99866, l2: 0.03185\n",
            "Loss: 3.041288e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 3.039550e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 3.032835e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 3.031135e-06, l1: 0.99870, l2: 0.03184\n",
            "Loss: 3.029234e-06, l1: 0.99868, l2: 0.03184\n",
            "Loss: 3.024360e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 3.014816e-06, l1: 0.99867, l2: 0.03185\n",
            "Loss: 3.014843e-06, l1: 0.99866, l2: 0.03184\n",
            "Loss: 3.011107e-06, l1: 0.99866, l2: 0.03185\n",
            "Loss: 3.006387e-06, l1: 0.99870, l2: 0.03184\n",
            "Loss: 3.000818e-06, l1: 0.99873, l2: 0.03184\n",
            "Loss: 2.997376e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 2.995273e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 2.992508e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.991083e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.988340e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 2.985960e-06, l1: 0.99884, l2: 0.03184\n",
            "Loss: 2.983009e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 2.980572e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 2.978644e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 2.977206e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 2.974369e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 2.970240e-06, l1: 0.99885, l2: 0.03183\n",
            "Loss: 2.989711e-06, l1: 0.99879, l2: 0.03183\n",
            "Loss: 2.968497e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 2.964253e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 2.958750e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 2.956900e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 2.953146e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 2.951476e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 2.948489e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 2.946300e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 2.942542e-06, l1: 0.99858, l2: 0.03184\n",
            "Loss: 2.938680e-06, l1: 0.99854, l2: 0.03184\n",
            "Loss: 2.936480e-06, l1: 0.99852, l2: 0.03184\n",
            "Loss: 2.935460e-06, l1: 0.99848, l2: 0.03184\n",
            "Loss: 2.933564e-06, l1: 0.99848, l2: 0.03184\n",
            "Loss: 2.928645e-06, l1: 0.99849, l2: 0.03184\n",
            "Loss: 2.924376e-06, l1: 0.99850, l2: 0.03184\n",
            "Loss: 2.920153e-06, l1: 0.99849, l2: 0.03184\n",
            "Loss: 2.915695e-06, l1: 0.99855, l2: 0.03185\n",
            "Loss: 2.914017e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 2.912550e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 2.910970e-06, l1: 0.99856, l2: 0.03184\n",
            "Loss: 2.911465e-06, l1: 0.99852, l2: 0.03184\n",
            "Loss: 2.909500e-06, l1: 0.99854, l2: 0.03184\n",
            "Loss: 2.906439e-06, l1: 0.99854, l2: 0.03184\n",
            "Loss: 2.903947e-06, l1: 0.99853, l2: 0.03184\n",
            "Loss: 2.901340e-06, l1: 0.99851, l2: 0.03184\n",
            "Loss: 2.903024e-06, l1: 0.99840, l2: 0.03184\n",
            "Loss: 2.900073e-06, l1: 0.99847, l2: 0.03184\n",
            "Loss: 2.899000e-06, l1: 0.99847, l2: 0.03184\n",
            "Loss: 2.897307e-06, l1: 0.99846, l2: 0.03184\n",
            "Loss: 2.893219e-06, l1: 0.99846, l2: 0.03184\n",
            "Loss: 2.890771e-06, l1: 0.99845, l2: 0.03184\n",
            "Loss: 2.887341e-06, l1: 0.99844, l2: 0.03184\n",
            "Loss: 2.886137e-06, l1: 0.99845, l2: 0.03184\n",
            "Loss: 2.882896e-06, l1: 0.99846, l2: 0.03184\n",
            "Loss: 2.879383e-06, l1: 0.99848, l2: 0.03184\n",
            "Loss: 2.875788e-06, l1: 0.99850, l2: 0.03184\n",
            "Loss: 2.870093e-06, l1: 0.99853, l2: 0.03184\n",
            "Loss: 2.864757e-06, l1: 0.99857, l2: 0.03184\n",
            "Loss: 2.863073e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 2.856535e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 2.854085e-06, l1: 0.99855, l2: 0.03184\n",
            "Loss: 2.850462e-06, l1: 0.99857, l2: 0.03184\n",
            "Loss: 2.847017e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 2.843559e-06, l1: 0.99864, l2: 0.03184\n",
            "Loss: 2.840135e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 2.836856e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.835367e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 2.834857e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 2.831988e-06, l1: 0.99882, l2: 0.03184\n",
            "Loss: 2.829766e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 2.827581e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.826171e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 2.822200e-06, l1: 0.99875, l2: 0.03184\n",
            "Loss: 2.820119e-06, l1: 0.99874, l2: 0.03184\n",
            "Loss: 2.815779e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 2.808705e-06, l1: 0.99866, l2: 0.03184\n",
            "Loss: 2.807220e-06, l1: 0.99857, l2: 0.03184\n",
            "Loss: 2.799004e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 2.794076e-06, l1: 0.99861, l2: 0.03184\n",
            "Loss: 2.790436e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 2.788488e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 2.786256e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 2.783522e-06, l1: 0.99865, l2: 0.03184\n",
            "Loss: 2.780287e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 2.775241e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 2.767051e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 2.757771e-06, l1: 0.99869, l2: 0.03183\n",
            "Loss: 2.751324e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 2.742398e-06, l1: 0.99874, l2: 0.03183\n",
            "Loss: 2.735745e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 2.746105e-06, l1: 0.99869, l2: 0.03184\n",
            "Loss: 2.734458e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 2.730630e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.727993e-06, l1: 0.99879, l2: 0.03184\n",
            "Loss: 2.724207e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.720661e-06, l1: 0.99875, l2: 0.03184\n",
            "Loss: 2.716462e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 2.709655e-06, l1: 0.99870, l2: 0.03185\n",
            "Loss: 2.705540e-06, l1: 0.99873, l2: 0.03185\n",
            "Loss: 2.698726e-06, l1: 0.99873, l2: 0.03185\n",
            "Loss: 2.694648e-06, l1: 0.99877, l2: 0.03185\n",
            "Loss: 2.698507e-06, l1: 0.99879, l2: 0.03186\n",
            "Loss: 2.693133e-06, l1: 0.99878, l2: 0.03185\n",
            "Loss: 2.690442e-06, l1: 0.99883, l2: 0.03186\n",
            "Loss: 2.688844e-06, l1: 0.99886, l2: 0.03186\n",
            "Loss: 2.685718e-06, l1: 0.99888, l2: 0.03186\n",
            "Loss: 2.682814e-06, l1: 0.99891, l2: 0.03186\n",
            "Loss: 2.678504e-06, l1: 0.99896, l2: 0.03186\n",
            "Loss: 2.674517e-06, l1: 0.99901, l2: 0.03186\n",
            "Loss: 2.670133e-06, l1: 0.99909, l2: 0.03186\n",
            "Loss: 2.666623e-06, l1: 0.99913, l2: 0.03186\n",
            "Loss: 2.664249e-06, l1: 0.99915, l2: 0.03186\n",
            "Loss: 2.662555e-06, l1: 0.99916, l2: 0.03186\n",
            "Loss: 2.676913e-06, l1: 0.99909, l2: 0.03186\n",
            "Loss: 2.661587e-06, l1: 0.99914, l2: 0.03186\n",
            "Loss: 2.658796e-06, l1: 0.99916, l2: 0.03186\n",
            "Loss: 2.654588e-06, l1: 0.99919, l2: 0.03186\n",
            "Loss: 2.648995e-06, l1: 0.99925, l2: 0.03186\n",
            "Loss: 2.648555e-06, l1: 0.99930, l2: 0.03186\n",
            "Loss: 2.646725e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 2.642698e-06, l1: 0.99932, l2: 0.03186\n",
            "Loss: 2.640513e-06, l1: 0.99933, l2: 0.03187\n",
            "Loss: 2.637551e-06, l1: 0.99933, l2: 0.03187\n",
            "Loss: 2.633987e-06, l1: 0.99932, l2: 0.03187\n",
            "Loss: 2.633884e-06, l1: 0.99933, l2: 0.03187\n",
            "Loss: 2.631703e-06, l1: 0.99933, l2: 0.03187\n",
            "Loss: 2.627961e-06, l1: 0.99929, l2: 0.03187\n",
            "Loss: 2.625017e-06, l1: 0.99926, l2: 0.03187\n",
            "Loss: 2.621314e-06, l1: 0.99920, l2: 0.03187\n",
            "Loss: 2.618211e-06, l1: 0.99913, l2: 0.03187\n",
            "Loss: 2.615751e-06, l1: 0.99913, l2: 0.03187\n",
            "Loss: 2.613195e-06, l1: 0.99914, l2: 0.03187\n",
            "Loss: 2.611137e-06, l1: 0.99914, l2: 0.03187\n",
            "Loss: 2.606917e-06, l1: 0.99913, l2: 0.03187\n",
            "Loss: 2.613455e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 2.605313e-06, l1: 0.99912, l2: 0.03186\n",
            "Loss: 2.603442e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 2.601472e-06, l1: 0.99911, l2: 0.03186\n",
            "Loss: 2.599340e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 2.596280e-06, l1: 0.99911, l2: 0.03186\n",
            "Loss: 2.592580e-06, l1: 0.99909, l2: 0.03185\n",
            "Loss: 2.589175e-06, l1: 0.99908, l2: 0.03185\n",
            "Loss: 2.587458e-06, l1: 0.99908, l2: 0.03185\n",
            "Loss: 2.586642e-06, l1: 0.99908, l2: 0.03185\n",
            "Loss: 2.585705e-06, l1: 0.99908, l2: 0.03185\n",
            "Loss: 2.584022e-06, l1: 0.99907, l2: 0.03185\n",
            "Loss: 2.581678e-06, l1: 0.99905, l2: 0.03185\n",
            "Loss: 2.584492e-06, l1: 0.99894, l2: 0.03185\n",
            "Loss: 2.580649e-06, l1: 0.99901, l2: 0.03185\n",
            "Loss: 2.578632e-06, l1: 0.99898, l2: 0.03185\n",
            "Loss: 2.577239e-06, l1: 0.99895, l2: 0.03184\n",
            "Loss: 2.575876e-06, l1: 0.99892, l2: 0.03184\n",
            "Loss: 2.573469e-06, l1: 0.99888, l2: 0.03184\n",
            "Loss: 2.570288e-06, l1: 0.99882, l2: 0.03184\n",
            "Loss: 2.568967e-06, l1: 0.99882, l2: 0.03184\n",
            "Loss: 2.566449e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 2.565758e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 2.564446e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 2.563600e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 2.561383e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 2.559257e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 2.557754e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 2.557470e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 2.555788e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 2.553639e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 2.551187e-06, l1: 0.99876, l2: 0.03183\n",
            "Loss: 2.549640e-06, l1: 0.99876, l2: 0.03183\n",
            "Loss: 2.547228e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 2.545020e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.543635e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.542200e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.542136e-06, l1: 0.99882, l2: 0.03184\n",
            "Loss: 2.541570e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 2.540713e-06, l1: 0.99879, l2: 0.03184\n",
            "Loss: 2.539005e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.536169e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.535177e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.532672e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.531682e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.531069e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 2.530208e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 2.528959e-06, l1: 0.99872, l2: 0.03183\n",
            "Loss: 2.527180e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 2.525732e-06, l1: 0.99863, l2: 0.03183\n",
            "Loss: 2.523974e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 2.522643e-06, l1: 0.99869, l2: 0.03183\n",
            "Loss: 2.522227e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 2.520923e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 2.519890e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 2.518838e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 2.517798e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 2.515378e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 2.512554e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 2.510579e-06, l1: 0.99874, l2: 0.03183\n",
            "Loss: 2.509589e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 2.507897e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 2.508165e-06, l1: 0.99882, l2: 0.03183\n",
            "Loss: 2.506741e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 2.504477e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 2.501962e-06, l1: 0.99886, l2: 0.03182\n",
            "Loss: 2.499791e-06, l1: 0.99888, l2: 0.03182\n",
            "Loss: 2.497790e-06, l1: 0.99888, l2: 0.03182\n",
            "Loss: 2.496433e-06, l1: 0.99896, l2: 0.03182\n",
            "Loss: 2.506900e-06, l1: 0.99894, l2: 0.03183\n",
            "Loss: 2.494913e-06, l1: 0.99896, l2: 0.03183\n",
            "Loss: 2.492824e-06, l1: 0.99893, l2: 0.03183\n",
            "Loss: 2.490975e-06, l1: 0.99893, l2: 0.03183\n",
            "Loss: 2.489342e-06, l1: 0.99895, l2: 0.03183\n",
            "Loss: 2.487076e-06, l1: 0.99898, l2: 0.03183\n",
            "Loss: 2.482140e-06, l1: 0.99897, l2: 0.03182\n",
            "Loss: 2.509651e-06, l1: 0.99919, l2: 0.03183\n",
            "Loss: 2.480777e-06, l1: 0.99901, l2: 0.03182\n",
            "Loss: 2.476999e-06, l1: 0.99901, l2: 0.03182\n",
            "Loss: 2.474728e-06, l1: 0.99902, l2: 0.03182\n",
            "Loss: 2.472555e-06, l1: 0.99903, l2: 0.03182\n",
            "Loss: 2.492476e-06, l1: 0.99908, l2: 0.03182\n",
            "Loss: 2.471829e-06, l1: 0.99904, l2: 0.03182\n",
            "Loss: 2.470046e-06, l1: 0.99905, l2: 0.03182\n",
            "Loss: 2.468540e-06, l1: 0.99907, l2: 0.03183\n",
            "Loss: 2.466812e-06, l1: 0.99910, l2: 0.03183\n",
            "Loss: 2.464722e-06, l1: 0.99911, l2: 0.03183\n",
            "Loss: 2.461933e-06, l1: 0.99911, l2: 0.03183\n",
            "Loss: 2.458298e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 2.456193e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 2.454881e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 2.453771e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 2.452332e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 2.451231e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 2.449695e-06, l1: 0.99916, l2: 0.03184\n",
            "Loss: 2.448581e-06, l1: 0.99917, l2: 0.03184\n",
            "Loss: 2.445037e-06, l1: 0.99918, l2: 0.03184\n",
            "Loss: 2.442568e-06, l1: 0.99921, l2: 0.03184\n",
            "Loss: 2.440871e-06, l1: 0.99921, l2: 0.03184\n",
            "Loss: 2.439918e-06, l1: 0.99922, l2: 0.03184\n",
            "Loss: 2.438908e-06, l1: 0.99922, l2: 0.03185\n",
            "Loss: 2.436256e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 2.431121e-06, l1: 0.99933, l2: 0.03185\n",
            "Loss: 2.426929e-06, l1: 0.99938, l2: 0.03185\n",
            "Loss: 2.424304e-06, l1: 0.99942, l2: 0.03185\n",
            "Loss: 2.448830e-06, l1: 0.99939, l2: 0.03185\n",
            "Loss: 2.423899e-06, l1: 0.99942, l2: 0.03185\n",
            "Loss: 2.422739e-06, l1: 0.99942, l2: 0.03185\n",
            "Loss: 2.420834e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 2.419152e-06, l1: 0.99939, l2: 0.03185\n",
            "Loss: 2.420657e-06, l1: 0.99933, l2: 0.03185\n",
            "Loss: 2.418663e-06, l1: 0.99937, l2: 0.03185\n",
            "Loss: 2.417387e-06, l1: 0.99936, l2: 0.03185\n",
            "Loss: 2.416065e-06, l1: 0.99935, l2: 0.03185\n",
            "Loss: 2.414601e-06, l1: 0.99935, l2: 0.03185\n",
            "Loss: 2.412879e-06, l1: 0.99936, l2: 0.03185\n",
            "Loss: 2.411158e-06, l1: 0.99935, l2: 0.03185\n",
            "Loss: 2.409288e-06, l1: 0.99938, l2: 0.03185\n",
            "Loss: 2.407427e-06, l1: 0.99944, l2: 0.03185\n",
            "Loss: 2.405491e-06, l1: 0.99944, l2: 0.03185\n",
            "Loss: 2.402961e-06, l1: 0.99946, l2: 0.03186\n",
            "Loss: 2.401625e-06, l1: 0.99947, l2: 0.03186\n",
            "Loss: 2.398228e-06, l1: 0.99948, l2: 0.03186\n",
            "Loss: 2.394268e-06, l1: 0.99949, l2: 0.03186\n",
            "Loss: 2.392803e-06, l1: 0.99948, l2: 0.03186\n",
            "Loss: 2.389433e-06, l1: 0.99947, l2: 0.03186\n",
            "Loss: 2.388172e-06, l1: 0.99946, l2: 0.03186\n",
            "Loss: 2.387399e-06, l1: 0.99945, l2: 0.03186\n",
            "Loss: 2.386793e-06, l1: 0.99945, l2: 0.03186\n",
            "Loss: 2.385246e-06, l1: 0.99944, l2: 0.03186\n",
            "Loss: 2.383224e-06, l1: 0.99943, l2: 0.03186\n",
            "Loss: 2.383358e-06, l1: 0.99937, l2: 0.03185\n",
            "Loss: 2.381566e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 2.378972e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 2.377131e-06, l1: 0.99943, l2: 0.03186\n",
            "Loss: 2.375663e-06, l1: 0.99944, l2: 0.03186\n",
            "Loss: 2.381607e-06, l1: 0.99944, l2: 0.03186\n",
            "Loss: 2.375380e-06, l1: 0.99944, l2: 0.03186\n",
            "Loss: 2.374177e-06, l1: 0.99944, l2: 0.03186\n",
            "Loss: 2.372691e-06, l1: 0.99943, l2: 0.03186\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
            "  Objective function value: 0.000002\n",
            "  Number of iterations: 1714\n",
            "  Number of functions evaluations: 1864\n",
            "Error u: 8.580012e-04\n",
            "Error l1: 0.05705%\n",
            "Error l2: 0.08497%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "########################### Noisy Data ###############################\n",
        "######################################################################\n",
        "noise = 0.01        \n",
        "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "        \n",
        "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "model.train(2000)\n",
        "    \n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "        \n",
        "lambda_1_value_noisy = model.sess.run(model.lambda_1)\n",
        "lambda_2_value_noisy = model.sess.run(model.lambda_2)\n",
        "lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n",
        "            \n",
        "error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
        "error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu)/nu * 100\n",
        "    \n",
        "print('Error lambda_1: %f%%' % (error_lambda_1_noisy))\n",
        "print('Error lambda_2: %f%%' % (error_lambda_2_noisy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WSYpFgNogBy",
        "outputId": "08ddbfb6-eb14-4a40-d396-01c361c96270"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n",
            "It: 0, Loss: 2.764e-01, Lambda_1: -0.001, Lambda_2: 0.002478, Time: 1.12\n",
            "It: 10, Loss: 1.949e-01, Lambda_1: -0.008, Lambda_2: 0.002492, Time: 0.23\n",
            "It: 20, Loss: 1.765e-01, Lambda_1: -0.018, Lambda_2: 0.002516, Time: 0.21\n",
            "It: 30, Loss: 1.647e-01, Lambda_1: -0.030, Lambda_2: 0.002547, Time: 0.22\n",
            "It: 40, Loss: 1.579e-01, Lambda_1: -0.041, Lambda_2: 0.002579, Time: 0.22\n",
            "It: 50, Loss: 1.477e-01, Lambda_1: -0.051, Lambda_2: 0.002610, Time: 0.22\n",
            "It: 60, Loss: 1.338e-01, Lambda_1: -0.059, Lambda_2: 0.002637, Time: 0.25\n",
            "It: 70, Loss: 1.135e-01, Lambda_1: -0.063, Lambda_2: 0.002652, Time: 0.22\n",
            "It: 80, Loss: 8.625e-02, Lambda_1: -0.062, Lambda_2: 0.002629, Time: 0.22\n",
            "It: 90, Loss: 6.011e-02, Lambda_1: -0.053, Lambda_2: 0.002578, Time: 0.21\n",
            "It: 100, Loss: 4.147e-02, Lambda_1: -0.035, Lambda_2: 0.002522, Time: 0.23\n",
            "It: 110, Loss: 3.167e-02, Lambda_1: -0.013, Lambda_2: 0.002465, Time: 0.22\n",
            "It: 120, Loss: 2.725e-02, Lambda_1: 0.010, Lambda_2: 0.002411, Time: 0.22\n",
            "It: 130, Loss: 2.407e-02, Lambda_1: 0.028, Lambda_2: 0.002367, Time: 0.21\n",
            "It: 140, Loss: 2.183e-02, Lambda_1: 0.041, Lambda_2: 0.002342, Time: 0.21\n",
            "It: 150, Loss: 2.068e-02, Lambda_1: 0.048, Lambda_2: 0.002334, Time: 0.23\n",
            "It: 160, Loss: 1.990e-02, Lambda_1: 0.053, Lambda_2: 0.002339, Time: 0.21\n",
            "It: 170, Loss: 1.933e-02, Lambda_1: 0.056, Lambda_2: 0.002350, Time: 0.22\n",
            "It: 180, Loss: 1.891e-02, Lambda_1: 0.056, Lambda_2: 0.002365, Time: 0.21\n",
            "It: 190, Loss: 1.859e-02, Lambda_1: 0.057, Lambda_2: 0.002380, Time: 0.23\n",
            "It: 200, Loss: 1.834e-02, Lambda_1: 0.058, Lambda_2: 0.002395, Time: 0.22\n",
            "It: 210, Loss: 1.815e-02, Lambda_1: 0.059, Lambda_2: 0.002410, Time: 0.20\n",
            "It: 220, Loss: 1.801e-02, Lambda_1: 0.060, Lambda_2: 0.002425, Time: 0.23\n",
            "It: 230, Loss: 1.788e-02, Lambda_1: 0.061, Lambda_2: 0.002440, Time: 0.20\n",
            "It: 240, Loss: 1.778e-02, Lambda_1: 0.062, Lambda_2: 0.002457, Time: 0.31\n",
            "It: 250, Loss: 1.768e-02, Lambda_1: 0.063, Lambda_2: 0.002474, Time: 0.46\n",
            "It: 260, Loss: 1.759e-02, Lambda_1: 0.064, Lambda_2: 0.002491, Time: 0.38\n",
            "It: 270, Loss: 1.750e-02, Lambda_1: 0.065, Lambda_2: 0.002509, Time: 0.21\n",
            "It: 280, Loss: 1.741e-02, Lambda_1: 0.066, Lambda_2: 0.002528, Time: 0.35\n",
            "It: 290, Loss: 1.732e-02, Lambda_1: 0.067, Lambda_2: 0.002548, Time: 0.49\n",
            "It: 300, Loss: 1.723e-02, Lambda_1: 0.069, Lambda_2: 0.002568, Time: 0.27\n",
            "It: 310, Loss: 1.713e-02, Lambda_1: 0.070, Lambda_2: 0.002589, Time: 0.21\n",
            "It: 320, Loss: 1.704e-02, Lambda_1: 0.071, Lambda_2: 0.002610, Time: 0.21\n",
            "It: 330, Loss: 1.694e-02, Lambda_1: 0.073, Lambda_2: 0.002633, Time: 0.22\n",
            "It: 340, Loss: 1.683e-02, Lambda_1: 0.075, Lambda_2: 0.002656, Time: 0.21\n",
            "It: 350, Loss: 1.673e-02, Lambda_1: 0.077, Lambda_2: 0.002680, Time: 0.19\n",
            "It: 360, Loss: 1.662e-02, Lambda_1: 0.079, Lambda_2: 0.002704, Time: 0.20\n",
            "It: 370, Loss: 1.650e-02, Lambda_1: 0.081, Lambda_2: 0.002730, Time: 0.21\n",
            "It: 380, Loss: 1.639e-02, Lambda_1: 0.083, Lambda_2: 0.002756, Time: 0.23\n",
            "It: 390, Loss: 1.626e-02, Lambda_1: 0.086, Lambda_2: 0.002783, Time: 0.21\n",
            "It: 400, Loss: 1.613e-02, Lambda_1: 0.089, Lambda_2: 0.002811, Time: 0.22\n",
            "It: 410, Loss: 1.600e-02, Lambda_1: 0.092, Lambda_2: 0.002840, Time: 0.21\n",
            "It: 420, Loss: 1.586e-02, Lambda_1: 0.095, Lambda_2: 0.002870, Time: 0.22\n",
            "It: 430, Loss: 1.572e-02, Lambda_1: 0.099, Lambda_2: 0.002900, Time: 0.22\n",
            "It: 440, Loss: 1.557e-02, Lambda_1: 0.103, Lambda_2: 0.002932, Time: 0.19\n",
            "It: 450, Loss: 1.541e-02, Lambda_1: 0.107, Lambda_2: 0.002964, Time: 0.21\n",
            "It: 460, Loss: 1.526e-02, Lambda_1: 0.111, Lambda_2: 0.002998, Time: 0.21\n",
            "It: 470, Loss: 1.510e-02, Lambda_1: 0.116, Lambda_2: 0.003032, Time: 0.22\n",
            "It: 480, Loss: 1.493e-02, Lambda_1: 0.121, Lambda_2: 0.003067, Time: 0.20\n",
            "It: 490, Loss: 1.478e-02, Lambda_1: 0.127, Lambda_2: 0.003103, Time: 0.21\n",
            "It: 500, Loss: 1.460e-02, Lambda_1: 0.132, Lambda_2: 0.003141, Time: 0.21\n",
            "It: 510, Loss: 1.444e-02, Lambda_1: 0.138, Lambda_2: 0.003179, Time: 0.20\n",
            "It: 520, Loss: 1.427e-02, Lambda_1: 0.144, Lambda_2: 0.003219, Time: 0.23\n",
            "It: 530, Loss: 1.411e-02, Lambda_1: 0.149, Lambda_2: 0.003259, Time: 0.21\n",
            "It: 540, Loss: 1.395e-02, Lambda_1: 0.155, Lambda_2: 0.003301, Time: 0.20\n",
            "It: 550, Loss: 1.379e-02, Lambda_1: 0.162, Lambda_2: 0.003345, Time: 0.22\n",
            "It: 560, Loss: 1.363e-02, Lambda_1: 0.168, Lambda_2: 0.003389, Time: 0.20\n",
            "It: 570, Loss: 1.347e-02, Lambda_1: 0.174, Lambda_2: 0.003435, Time: 0.22\n",
            "It: 580, Loss: 1.333e-02, Lambda_1: 0.180, Lambda_2: 0.003482, Time: 0.22\n",
            "It: 590, Loss: 1.318e-02, Lambda_1: 0.186, Lambda_2: 0.003530, Time: 0.20\n",
            "It: 600, Loss: 1.302e-02, Lambda_1: 0.192, Lambda_2: 0.003579, Time: 0.20\n",
            "It: 610, Loss: 1.287e-02, Lambda_1: 0.198, Lambda_2: 0.003630, Time: 0.22\n",
            "It: 620, Loss: 1.272e-02, Lambda_1: 0.205, Lambda_2: 0.003682, Time: 0.21\n",
            "It: 630, Loss: 1.257e-02, Lambda_1: 0.211, Lambda_2: 0.003735, Time: 0.21\n",
            "It: 640, Loss: 1.243e-02, Lambda_1: 0.217, Lambda_2: 0.003789, Time: 0.21\n",
            "It: 650, Loss: 1.228e-02, Lambda_1: 0.223, Lambda_2: 0.003844, Time: 0.20\n",
            "It: 660, Loss: 1.222e-02, Lambda_1: 0.230, Lambda_2: 0.003900, Time: 0.22\n",
            "It: 670, Loss: 1.200e-02, Lambda_1: 0.236, Lambda_2: 0.003957, Time: 0.21\n",
            "It: 680, Loss: 1.186e-02, Lambda_1: 0.242, Lambda_2: 0.004015, Time: 0.20\n",
            "It: 690, Loss: 1.173e-02, Lambda_1: 0.248, Lambda_2: 0.004074, Time: 0.22\n",
            "It: 700, Loss: 1.159e-02, Lambda_1: 0.255, Lambda_2: 0.004134, Time: 0.20\n",
            "It: 710, Loss: 1.146e-02, Lambda_1: 0.261, Lambda_2: 0.004195, Time: 0.24\n",
            "It: 720, Loss: 1.133e-02, Lambda_1: 0.267, Lambda_2: 0.004258, Time: 0.21\n",
            "It: 730, Loss: 1.120e-02, Lambda_1: 0.274, Lambda_2: 0.004322, Time: 0.20\n",
            "It: 740, Loss: 1.109e-02, Lambda_1: 0.280, Lambda_2: 0.004387, Time: 0.19\n",
            "It: 750, Loss: 1.097e-02, Lambda_1: 0.286, Lambda_2: 0.004453, Time: 0.23\n",
            "It: 760, Loss: 1.083e-02, Lambda_1: 0.292, Lambda_2: 0.004521, Time: 0.20\n",
            "It: 770, Loss: 1.071e-02, Lambda_1: 0.298, Lambda_2: 0.004590, Time: 0.21\n",
            "It: 780, Loss: 1.060e-02, Lambda_1: 0.305, Lambda_2: 0.004661, Time: 0.21\n",
            "It: 790, Loss: 1.048e-02, Lambda_1: 0.311, Lambda_2: 0.004732, Time: 0.22\n",
            "It: 800, Loss: 1.036e-02, Lambda_1: 0.317, Lambda_2: 0.004806, Time: 0.23\n",
            "It: 810, Loss: 1.025e-02, Lambda_1: 0.323, Lambda_2: 0.004880, Time: 0.22\n",
            "It: 820, Loss: 1.020e-02, Lambda_1: 0.329, Lambda_2: 0.004956, Time: 0.20\n",
            "It: 830, Loss: 1.014e-02, Lambda_1: 0.335, Lambda_2: 0.005032, Time: 0.20\n",
            "It: 840, Loss: 9.952e-03, Lambda_1: 0.341, Lambda_2: 0.005110, Time: 0.21\n",
            "It: 850, Loss: 9.812e-03, Lambda_1: 0.347, Lambda_2: 0.005189, Time: 0.22\n",
            "It: 860, Loss: 9.702e-03, Lambda_1: 0.354, Lambda_2: 0.005269, Time: 0.21\n",
            "It: 870, Loss: 9.590e-03, Lambda_1: 0.360, Lambda_2: 0.005350, Time: 0.20\n",
            "It: 880, Loss: 9.486e-03, Lambda_1: 0.366, Lambda_2: 0.005432, Time: 0.20\n",
            "It: 890, Loss: 9.416e-03, Lambda_1: 0.372, Lambda_2: 0.005515, Time: 0.22\n",
            "It: 900, Loss: 9.319e-03, Lambda_1: 0.378, Lambda_2: 0.005599, Time: 0.20\n",
            "It: 910, Loss: 9.175e-03, Lambda_1: 0.384, Lambda_2: 0.005684, Time: 0.21\n",
            "It: 920, Loss: 9.086e-03, Lambda_1: 0.390, Lambda_2: 0.005770, Time: 0.20\n",
            "It: 930, Loss: 8.971e-03, Lambda_1: 0.396, Lambda_2: 0.005857, Time: 0.19\n",
            "It: 940, Loss: 8.871e-03, Lambda_1: 0.402, Lambda_2: 0.005946, Time: 0.22\n",
            "It: 950, Loss: 8.768e-03, Lambda_1: 0.408, Lambda_2: 0.006036, Time: 0.20\n",
            "It: 960, Loss: 8.736e-03, Lambda_1: 0.414, Lambda_2: 0.006128, Time: 0.19\n",
            "It: 970, Loss: 8.846e-03, Lambda_1: 0.420, Lambda_2: 0.006220, Time: 0.21\n",
            "It: 980, Loss: 8.574e-03, Lambda_1: 0.426, Lambda_2: 0.006312, Time: 0.21\n",
            "It: 990, Loss: 8.411e-03, Lambda_1: 0.431, Lambda_2: 0.006407, Time: 0.22\n",
            "It: 1000, Loss: 8.300e-03, Lambda_1: 0.437, Lambda_2: 0.006504, Time: 0.20\n",
            "It: 1010, Loss: 8.193e-03, Lambda_1: 0.442, Lambda_2: 0.006602, Time: 0.20\n",
            "It: 1020, Loss: 8.097e-03, Lambda_1: 0.448, Lambda_2: 0.006702, Time: 0.20\n",
            "It: 1030, Loss: 8.000e-03, Lambda_1: 0.454, Lambda_2: 0.006804, Time: 0.22\n",
            "It: 1040, Loss: 7.935e-03, Lambda_1: 0.460, Lambda_2: 0.006908, Time: 0.25\n",
            "It: 1050, Loss: 7.828e-03, Lambda_1: 0.465, Lambda_2: 0.007012, Time: 0.22\n",
            "It: 1060, Loss: 7.782e-03, Lambda_1: 0.470, Lambda_2: 0.007117, Time: 0.20\n",
            "It: 1070, Loss: 7.670e-03, Lambda_1: 0.475, Lambda_2: 0.007226, Time: 0.22\n",
            "It: 1080, Loss: 7.549e-03, Lambda_1: 0.481, Lambda_2: 0.007336, Time: 0.23\n",
            "It: 1090, Loss: 7.447e-03, Lambda_1: 0.486, Lambda_2: 0.007448, Time: 0.21\n",
            "It: 1100, Loss: 7.351e-03, Lambda_1: 0.492, Lambda_2: 0.007562, Time: 0.22\n",
            "It: 1110, Loss: 7.255e-03, Lambda_1: 0.497, Lambda_2: 0.007678, Time: 0.21\n",
            "It: 1120, Loss: 7.410e-03, Lambda_1: 0.502, Lambda_2: 0.007796, Time: 0.22\n",
            "It: 1130, Loss: 7.365e-03, Lambda_1: 0.507, Lambda_2: 0.007913, Time: 0.23\n",
            "It: 1140, Loss: 7.030e-03, Lambda_1: 0.512, Lambda_2: 0.008033, Time: 0.21\n",
            "It: 1150, Loss: 6.897e-03, Lambda_1: 0.517, Lambda_2: 0.008157, Time: 0.20\n",
            "It: 1160, Loss: 6.807e-03, Lambda_1: 0.521, Lambda_2: 0.008282, Time: 0.21\n",
            "It: 1170, Loss: 6.721e-03, Lambda_1: 0.526, Lambda_2: 0.008408, Time: 0.21\n",
            "It: 1180, Loss: 6.625e-03, Lambda_1: 0.531, Lambda_2: 0.008537, Time: 0.21\n",
            "It: 1190, Loss: 6.534e-03, Lambda_1: 0.536, Lambda_2: 0.008668, Time: 0.20\n",
            "It: 1200, Loss: 6.443e-03, Lambda_1: 0.541, Lambda_2: 0.008801, Time: 0.21\n",
            "It: 1210, Loss: 6.352e-03, Lambda_1: 0.546, Lambda_2: 0.008936, Time: 0.19\n",
            "It: 1220, Loss: 6.289e-03, Lambda_1: 0.551, Lambda_2: 0.009073, Time: 0.22\n",
            "It: 1230, Loss: 6.618e-03, Lambda_1: 0.556, Lambda_2: 0.009209, Time: 0.19\n",
            "It: 1240, Loss: 6.157e-03, Lambda_1: 0.560, Lambda_2: 0.009343, Time: 0.20\n",
            "It: 1250, Loss: 6.073e-03, Lambda_1: 0.563, Lambda_2: 0.009485, Time: 0.21\n",
            "It: 1260, Loss: 5.968e-03, Lambda_1: 0.567, Lambda_2: 0.009629, Time: 0.22\n",
            "It: 1270, Loss: 5.854e-03, Lambda_1: 0.572, Lambda_2: 0.009772, Time: 0.22\n",
            "It: 1280, Loss: 5.753e-03, Lambda_1: 0.576, Lambda_2: 0.009918, Time: 0.22\n",
            "It: 1290, Loss: 5.668e-03, Lambda_1: 0.581, Lambda_2: 0.010066, Time: 0.21\n",
            "It: 1300, Loss: 5.579e-03, Lambda_1: 0.586, Lambda_2: 0.010216, Time: 0.20\n",
            "It: 1310, Loss: 5.492e-03, Lambda_1: 0.590, Lambda_2: 0.010368, Time: 0.21\n",
            "It: 1320, Loss: 5.405e-03, Lambda_1: 0.595, Lambda_2: 0.010521, Time: 0.22\n",
            "It: 1330, Loss: 5.318e-03, Lambda_1: 0.599, Lambda_2: 0.010677, Time: 0.21\n",
            "It: 1340, Loss: 5.231e-03, Lambda_1: 0.604, Lambda_2: 0.010834, Time: 0.21\n",
            "It: 1350, Loss: 5.144e-03, Lambda_1: 0.608, Lambda_2: 0.010994, Time: 0.21\n",
            "It: 1360, Loss: 5.057e-03, Lambda_1: 0.613, Lambda_2: 0.011155, Time: 0.21\n",
            "It: 1370, Loss: 4.970e-03, Lambda_1: 0.617, Lambda_2: 0.011317, Time: 0.21\n",
            "It: 1380, Loss: 4.884e-03, Lambda_1: 0.621, Lambda_2: 0.011482, Time: 0.22\n",
            "It: 1390, Loss: 5.419e-03, Lambda_1: 0.626, Lambda_2: 0.011648, Time: 0.21\n",
            "It: 1400, Loss: 4.724e-03, Lambda_1: 0.630, Lambda_2: 0.011808, Time: 0.22\n",
            "It: 1410, Loss: 4.879e-03, Lambda_1: 0.633, Lambda_2: 0.011976, Time: 0.23\n",
            "It: 1420, Loss: 4.581e-03, Lambda_1: 0.636, Lambda_2: 0.012148, Time: 0.21\n",
            "It: 1430, Loss: 4.495e-03, Lambda_1: 0.640, Lambda_2: 0.012318, Time: 0.21\n",
            "It: 1440, Loss: 4.395e-03, Lambda_1: 0.645, Lambda_2: 0.012489, Time: 0.21\n",
            "It: 1450, Loss: 4.314e-03, Lambda_1: 0.649, Lambda_2: 0.012662, Time: 0.21\n",
            "It: 1460, Loss: 4.229e-03, Lambda_1: 0.654, Lambda_2: 0.012836, Time: 0.23\n",
            "It: 1470, Loss: 4.146e-03, Lambda_1: 0.658, Lambda_2: 0.013012, Time: 0.21\n",
            "It: 1480, Loss: 4.064e-03, Lambda_1: 0.663, Lambda_2: 0.013189, Time: 0.23\n",
            "It: 1490, Loss: 3.982e-03, Lambda_1: 0.667, Lambda_2: 0.013368, Time: 0.21\n",
            "It: 1500, Loss: 3.901e-03, Lambda_1: 0.671, Lambda_2: 0.013548, Time: 0.24\n",
            "It: 1510, Loss: 3.821e-03, Lambda_1: 0.676, Lambda_2: 0.013729, Time: 0.21\n",
            "It: 1520, Loss: 3.740e-03, Lambda_1: 0.680, Lambda_2: 0.013911, Time: 0.21\n",
            "It: 1530, Loss: 3.661e-03, Lambda_1: 0.685, Lambda_2: 0.014094, Time: 0.22\n",
            "It: 1540, Loss: 3.585e-03, Lambda_1: 0.689, Lambda_2: 0.014279, Time: 0.23\n",
            "It: 1550, Loss: 5.336e-03, Lambda_1: 0.693, Lambda_2: 0.014463, Time: 0.23\n",
            "It: 1560, Loss: 4.069e-03, Lambda_1: 0.697, Lambda_2: 0.014641, Time: 0.20\n",
            "It: 1570, Loss: 3.376e-03, Lambda_1: 0.700, Lambda_2: 0.014828, Time: 0.20\n",
            "It: 1580, Loss: 3.367e-03, Lambda_1: 0.703, Lambda_2: 0.015017, Time: 0.21\n",
            "It: 1590, Loss: 3.221e-03, Lambda_1: 0.707, Lambda_2: 0.015205, Time: 0.22\n",
            "It: 1600, Loss: 3.157e-03, Lambda_1: 0.711, Lambda_2: 0.015391, Time: 0.21\n",
            "It: 1610, Loss: 3.077e-03, Lambda_1: 0.716, Lambda_2: 0.015577, Time: 0.22\n",
            "It: 1620, Loss: 3.003e-03, Lambda_1: 0.720, Lambda_2: 0.015766, Time: 0.21\n",
            "It: 1630, Loss: 2.932e-03, Lambda_1: 0.724, Lambda_2: 0.015955, Time: 0.22\n",
            "It: 1640, Loss: 2.862e-03, Lambda_1: 0.729, Lambda_2: 0.016144, Time: 0.22\n",
            "It: 1650, Loss: 2.792e-03, Lambda_1: 0.733, Lambda_2: 0.016334, Time: 0.21\n",
            "It: 1660, Loss: 2.723e-03, Lambda_1: 0.737, Lambda_2: 0.016525, Time: 0.21\n",
            "It: 1670, Loss: 2.655e-03, Lambda_1: 0.741, Lambda_2: 0.016716, Time: 0.22\n",
            "It: 1680, Loss: 2.588e-03, Lambda_1: 0.745, Lambda_2: 0.016908, Time: 0.23\n",
            "It: 1690, Loss: 2.521e-03, Lambda_1: 0.749, Lambda_2: 0.017100, Time: 0.20\n",
            "It: 1700, Loss: 2.458e-03, Lambda_1: 0.754, Lambda_2: 0.017292, Time: 0.21\n",
            "It: 1710, Loss: 3.725e-03, Lambda_1: 0.758, Lambda_2: 0.017484, Time: 0.21\n",
            "It: 1720, Loss: 2.484e-03, Lambda_1: 0.761, Lambda_2: 0.017668, Time: 0.22\n",
            "It: 1730, Loss: 2.526e-03, Lambda_1: 0.763, Lambda_2: 0.017861, Time: 0.24\n",
            "It: 1740, Loss: 2.229e-03, Lambda_1: 0.766, Lambda_2: 0.018056, Time: 0.21\n",
            "It: 1750, Loss: 2.197e-03, Lambda_1: 0.770, Lambda_2: 0.018249, Time: 0.21\n",
            "It: 1760, Loss: 2.107e-03, Lambda_1: 0.774, Lambda_2: 0.018438, Time: 0.22\n",
            "It: 1770, Loss: 2.047e-03, Lambda_1: 0.778, Lambda_2: 0.018627, Time: 0.20\n",
            "It: 1780, Loss: 1.990e-03, Lambda_1: 0.782, Lambda_2: 0.018817, Time: 0.22\n",
            "It: 1790, Loss: 1.934e-03, Lambda_1: 0.786, Lambda_2: 0.019008, Time: 0.20\n",
            "It: 1800, Loss: 1.879e-03, Lambda_1: 0.790, Lambda_2: 0.019198, Time: 0.21\n",
            "It: 1810, Loss: 1.825e-03, Lambda_1: 0.793, Lambda_2: 0.019387, Time: 0.23\n",
            "It: 1820, Loss: 1.772e-03, Lambda_1: 0.797, Lambda_2: 0.019577, Time: 0.23\n",
            "It: 1830, Loss: 1.720e-03, Lambda_1: 0.801, Lambda_2: 0.019766, Time: 0.19\n",
            "It: 1840, Loss: 1.669e-03, Lambda_1: 0.805, Lambda_2: 0.019955, Time: 0.21\n",
            "It: 1850, Loss: 1.618e-03, Lambda_1: 0.808, Lambda_2: 0.020143, Time: 0.22\n",
            "It: 1860, Loss: 1.571e-03, Lambda_1: 0.812, Lambda_2: 0.020331, Time: 0.21\n",
            "It: 1870, Loss: 2.647e-03, Lambda_1: 0.815, Lambda_2: 0.020517, Time: 0.22\n",
            "It: 1880, Loss: 1.548e-03, Lambda_1: 0.818, Lambda_2: 0.020692, Time: 0.22\n",
            "It: 1890, Loss: 1.656e-03, Lambda_1: 0.820, Lambda_2: 0.020878, Time: 0.22\n",
            "It: 1900, Loss: 1.484e-03, Lambda_1: 0.822, Lambda_2: 0.021065, Time: 0.23\n",
            "It: 1910, Loss: 1.372e-03, Lambda_1: 0.825, Lambda_2: 0.021244, Time: 0.22\n",
            "It: 1920, Loss: 1.323e-03, Lambda_1: 0.828, Lambda_2: 0.021420, Time: 0.19\n",
            "It: 1930, Loss: 1.281e-03, Lambda_1: 0.832, Lambda_2: 0.021594, Time: 0.21\n",
            "It: 1940, Loss: 1.242e-03, Lambda_1: 0.835, Lambda_2: 0.021769, Time: 0.21\n",
            "It: 1950, Loss: 1.202e-03, Lambda_1: 0.838, Lambda_2: 0.021944, Time: 0.22\n",
            "It: 1960, Loss: 1.165e-03, Lambda_1: 0.841, Lambda_2: 0.022117, Time: 0.21\n",
            "It: 1970, Loss: 1.129e-03, Lambda_1: 0.844, Lambda_2: 0.022289, Time: 0.22\n",
            "It: 1980, Loss: 1.093e-03, Lambda_1: 0.847, Lambda_2: 0.022461, Time: 0.20\n",
            "It: 1990, Loss: 1.058e-03, Lambda_1: 0.850, Lambda_2: 0.022632, Time: 0.22\n",
            "Loss: 1.027751e-03, l1: 0.85309, l2: 0.02279\n",
            "Loss: 2.083676e+01, l1: 1.30029, l2: 0.04637\n",
            "Loss: 1.027557e-03, l1: 0.85320, l2: 0.02279\n",
            "Loss: 1.027037e-03, l1: 0.85324, l2: 0.02279\n",
            "Loss: 1.024315e-03, l1: 0.85368, l2: 0.02281\n",
            "Loss: 1.019525e-03, l1: 0.85479, l2: 0.02285\n",
            "Loss: 1.009002e-03, l1: 0.85777, l2: 0.02296\n",
            "Loss: 9.921395e-04, l1: 0.86311, l2: 0.02316\n",
            "Loss: 9.613467e-04, l1: 0.87249, l2: 0.02353\n",
            "Loss: 9.199427e-04, l1: 0.89918, l2: 0.02462\n",
            "Loss: 8.508879e-04, l1: 0.90820, l2: 0.02501\n",
            "Loss: 7.946016e-04, l1: 0.91643, l2: 0.02540\n",
            "Loss: 7.707094e-04, l1: 0.92279, l2: 0.02569\n",
            "Loss: 7.543778e-04, l1: 0.92941, l2: 0.02601\n",
            "Loss: 7.373815e-04, l1: 0.93414, l2: 0.02623\n",
            "Loss: 7.175277e-04, l1: 0.94075, l2: 0.02654\n",
            "Loss: 7.049920e-04, l1: 0.94405, l2: 0.02669\n",
            "Loss: 6.797214e-04, l1: 0.95025, l2: 0.02700\n",
            "Loss: 6.496222e-04, l1: 0.95990, l2: 0.02750\n",
            "Loss: 6.307400e-04, l1: 0.96527, l2: 0.02780\n",
            "Loss: 6.119622e-04, l1: 0.96958, l2: 0.02806\n",
            "Loss: 5.798945e-04, l1: 0.97578, l2: 0.02850\n",
            "Loss: 5.594795e-04, l1: 0.97593, l2: 0.02862\n",
            "Loss: 5.474875e-04, l1: 0.97680, l2: 0.02874\n",
            "Loss: 5.360264e-04, l1: 0.97568, l2: 0.02876\n",
            "Loss: 5.179698e-04, l1: 0.97874, l2: 0.02904\n",
            "Loss: 4.949854e-04, l1: 0.98056, l2: 0.02933\n",
            "Loss: 4.837418e-04, l1: 0.98596, l2: 0.02972\n",
            "Loss: 4.771900e-04, l1: 0.98574, l2: 0.02975\n",
            "Loss: 4.688047e-04, l1: 0.98705, l2: 0.02987\n",
            "Loss: 4.635669e-04, l1: 0.98598, l2: 0.02984\n",
            "Loss: 4.600263e-04, l1: 0.98564, l2: 0.02985\n",
            "Loss: 4.558762e-04, l1: 0.98572, l2: 0.02989\n",
            "Loss: 4.466131e-04, l1: 0.98716, l2: 0.03003\n",
            "Loss: 4.366561e-04, l1: 0.99071, l2: 0.03031\n",
            "Loss: 4.305448e-04, l1: 0.99146, l2: 0.03040\n",
            "Loss: 4.249231e-04, l1: 0.99259, l2: 0.03050\n",
            "Loss: 4.151992e-04, l1: 0.99246, l2: 0.03059\n",
            "Loss: 3.925358e-04, l1: 0.99201, l2: 0.03085\n",
            "Loss: 3.747146e-04, l1: 0.98700, l2: 0.03107\n",
            "Loss: 3.599175e-04, l1: 0.98571, l2: 0.03107\n",
            "Loss: 3.521303e-04, l1: 0.98461, l2: 0.03109\n",
            "Loss: 3.457624e-04, l1: 0.98377, l2: 0.03115\n",
            "Loss: 3.322423e-04, l1: 0.98250, l2: 0.03134\n",
            "Loss: 3.190344e-04, l1: 0.97701, l2: 0.03145\n",
            "Loss: 3.085034e-04, l1: 0.97746, l2: 0.03166\n",
            "Loss: 3.002820e-04, l1: 0.97598, l2: 0.03169\n",
            "Loss: 2.965947e-04, l1: 0.97594, l2: 0.03178\n",
            "Loss: 2.933538e-04, l1: 0.97450, l2: 0.03180\n",
            "Loss: 2.910125e-04, l1: 0.97421, l2: 0.03185\n",
            "Loss: 2.865312e-04, l1: 0.97287, l2: 0.03189\n",
            "Loss: 2.789742e-04, l1: 0.97191, l2: 0.03204\n",
            "Loss: 2.682941e-04, l1: 0.96580, l2: 0.03214\n",
            "Loss: 2.625390e-04, l1: 0.96626, l2: 0.03232\n",
            "Loss: 2.603803e-04, l1: 0.96458, l2: 0.03222\n",
            "Loss: 2.579114e-04, l1: 0.96319, l2: 0.03217\n",
            "Loss: 2.536164e-04, l1: 0.96161, l2: 0.03216\n",
            "Loss: 2.503483e-04, l1: 0.95686, l2: 0.03202\n",
            "Loss: 2.473902e-04, l1: 0.95755, l2: 0.03214\n",
            "Loss: 2.443804e-04, l1: 0.95878, l2: 0.03235\n",
            "Loss: 2.431233e-04, l1: 0.95917, l2: 0.03242\n",
            "Loss: 2.413355e-04, l1: 0.95993, l2: 0.03251\n",
            "Loss: 2.408636e-04, l1: 0.96005, l2: 0.03253\n",
            "Loss: 2.397881e-04, l1: 0.96064, l2: 0.03257\n",
            "Loss: 2.384830e-04, l1: 0.96059, l2: 0.03257\n",
            "Loss: 2.368231e-04, l1: 0.96032, l2: 0.03258\n",
            "Loss: 2.353173e-04, l1: 0.95936, l2: 0.03254\n",
            "Loss: 2.350843e-04, l1: 0.96123, l2: 0.03268\n",
            "Loss: 2.338637e-04, l1: 0.96012, l2: 0.03260\n",
            "Loss: 2.332520e-04, l1: 0.95979, l2: 0.03256\n",
            "Loss: 2.321767e-04, l1: 0.95988, l2: 0.03254\n",
            "Loss: 2.310828e-04, l1: 0.96021, l2: 0.03254\n",
            "Loss: 2.287749e-04, l1: 0.96060, l2: 0.03256\n",
            "Loss: 2.369304e-04, l1: 0.96116, l2: 0.03267\n",
            "Loss: 2.272728e-04, l1: 0.96076, l2: 0.03259\n",
            "Loss: 2.252315e-04, l1: 0.96045, l2: 0.03263\n",
            "Loss: 2.236679e-04, l1: 0.95993, l2: 0.03267\n",
            "Loss: 2.225399e-04, l1: 0.95997, l2: 0.03264\n",
            "Loss: 2.214906e-04, l1: 0.95883, l2: 0.03257\n",
            "Loss: 2.194637e-04, l1: 0.95795, l2: 0.03245\n",
            "Loss: 2.169809e-04, l1: 0.95767, l2: 0.03234\n",
            "Loss: 2.122435e-04, l1: 0.95874, l2: 0.03224\n",
            "Loss: 2.078124e-04, l1: 0.95938, l2: 0.03218\n",
            "Loss: 2.073950e-04, l1: 0.96818, l2: 0.03264\n",
            "Loss: 2.045276e-04, l1: 0.96446, l2: 0.03244\n",
            "Loss: 2.038838e-04, l1: 0.96386, l2: 0.03247\n",
            "Loss: 2.030484e-04, l1: 0.96372, l2: 0.03253\n",
            "Loss: 2.013918e-04, l1: 0.96368, l2: 0.03259\n",
            "Loss: 1.989453e-04, l1: 0.96400, l2: 0.03265\n",
            "Loss: 1.954196e-04, l1: 0.96468, l2: 0.03265\n",
            "Loss: 1.914315e-04, l1: 0.96439, l2: 0.03259\n",
            "Loss: 1.892623e-04, l1: 0.96604, l2: 0.03256\n",
            "Loss: 1.882603e-04, l1: 0.96579, l2: 0.03250\n",
            "Loss: 1.878584e-04, l1: 0.96558, l2: 0.03248\n",
            "Loss: 1.871000e-04, l1: 0.96550, l2: 0.03246\n",
            "Loss: 1.854276e-04, l1: 0.96529, l2: 0.03241\n",
            "Loss: 1.829785e-04, l1: 0.96577, l2: 0.03237\n",
            "Loss: 2.044150e-04, l1: 0.95844, l2: 0.03200\n",
            "Loss: 1.821009e-04, l1: 0.96456, l2: 0.03231\n",
            "Loss: 1.792542e-04, l1: 0.96562, l2: 0.03230\n",
            "Loss: 1.767991e-04, l1: 0.96638, l2: 0.03227\n",
            "Loss: 1.748959e-04, l1: 0.96757, l2: 0.03222\n",
            "Loss: 1.735808e-04, l1: 0.96793, l2: 0.03220\n",
            "Loss: 1.721953e-04, l1: 0.96843, l2: 0.03219\n",
            "Loss: 1.711156e-04, l1: 0.96920, l2: 0.03219\n",
            "Loss: 1.702817e-04, l1: 0.96954, l2: 0.03217\n",
            "Loss: 1.693146e-04, l1: 0.97237, l2: 0.03219\n",
            "Loss: 1.673393e-04, l1: 0.97150, l2: 0.03209\n",
            "Loss: 1.663130e-04, l1: 0.97112, l2: 0.03207\n",
            "Loss: 1.651669e-04, l1: 0.97125, l2: 0.03204\n",
            "Loss: 1.656956e-04, l1: 0.96900, l2: 0.03194\n",
            "Loss: 1.650008e-04, l1: 0.97051, l2: 0.03200\n",
            "Loss: 1.646286e-04, l1: 0.97108, l2: 0.03201\n",
            "Loss: 1.643156e-04, l1: 0.97171, l2: 0.03201\n",
            "Loss: 1.639346e-04, l1: 0.97237, l2: 0.03202\n",
            "Loss: 1.630803e-04, l1: 0.97311, l2: 0.03202\n",
            "Loss: 1.610930e-04, l1: 0.97493, l2: 0.03200\n",
            "Loss: 1.593463e-04, l1: 0.97618, l2: 0.03201\n",
            "Loss: 1.657761e-04, l1: 0.97824, l2: 0.03161\n",
            "Loss: 1.582413e-04, l1: 0.97675, l2: 0.03190\n",
            "Loss: 1.569254e-04, l1: 0.97633, l2: 0.03191\n",
            "Loss: 1.560099e-04, l1: 0.97527, l2: 0.03188\n",
            "Loss: 1.556255e-04, l1: 0.97462, l2: 0.03185\n",
            "Loss: 1.549518e-04, l1: 0.97479, l2: 0.03185\n",
            "Loss: 1.540574e-04, l1: 0.97490, l2: 0.03183\n",
            "Loss: 1.534516e-04, l1: 0.97481, l2: 0.03182\n",
            "Loss: 1.525430e-04, l1: 0.97481, l2: 0.03182\n",
            "Loss: 1.553045e-04, l1: 0.97207, l2: 0.03158\n",
            "Loss: 1.522367e-04, l1: 0.97415, l2: 0.03176\n",
            "Loss: 1.514234e-04, l1: 0.97466, l2: 0.03180\n",
            "Loss: 1.507087e-04, l1: 0.97513, l2: 0.03183\n",
            "Loss: 1.493674e-04, l1: 0.97644, l2: 0.03189\n",
            "Loss: 1.494223e-04, l1: 0.97718, l2: 0.03195\n",
            "Loss: 1.485209e-04, l1: 0.97680, l2: 0.03192\n",
            "Loss: 1.471617e-04, l1: 0.97746, l2: 0.03191\n",
            "Loss: 1.455354e-04, l1: 0.97774, l2: 0.03187\n",
            "Loss: 1.443825e-04, l1: 0.97816, l2: 0.03186\n",
            "Loss: 1.425753e-04, l1: 0.97859, l2: 0.03182\n",
            "Loss: 1.412255e-04, l1: 0.98071, l2: 0.03189\n",
            "Loss: 1.398800e-04, l1: 0.98002, l2: 0.03183\n",
            "Loss: 1.389105e-04, l1: 0.98032, l2: 0.03179\n",
            "Loss: 1.384226e-04, l1: 0.98057, l2: 0.03178\n",
            "Loss: 1.380230e-04, l1: 0.98173, l2: 0.03176\n",
            "Loss: 1.377250e-04, l1: 0.98159, l2: 0.03174\n",
            "Loss: 1.373122e-04, l1: 0.98156, l2: 0.03173\n",
            "Loss: 1.365489e-04, l1: 0.98124, l2: 0.03171\n",
            "Loss: 1.367288e-04, l1: 0.98075, l2: 0.03167\n",
            "Loss: 1.359716e-04, l1: 0.98101, l2: 0.03169\n",
            "Loss: 1.352471e-04, l1: 0.98036, l2: 0.03168\n",
            "Loss: 1.346661e-04, l1: 0.98027, l2: 0.03170\n",
            "Loss: 1.344225e-04, l1: 0.97956, l2: 0.03166\n",
            "Loss: 1.340842e-04, l1: 0.98041, l2: 0.03168\n",
            "Loss: 1.334120e-04, l1: 0.98250, l2: 0.03171\n",
            "Loss: 1.329525e-04, l1: 0.98373, l2: 0.03173\n",
            "Loss: 1.326578e-04, l1: 0.98422, l2: 0.03173\n",
            "Loss: 1.318688e-04, l1: 0.98452, l2: 0.03174\n",
            "Loss: 1.314710e-04, l1: 0.98445, l2: 0.03173\n",
            "Loss: 1.308845e-04, l1: 0.98424, l2: 0.03172\n",
            "Loss: 1.299352e-04, l1: 0.98477, l2: 0.03168\n",
            "Loss: 1.303414e-04, l1: 0.98493, l2: 0.03167\n",
            "Loss: 1.293179e-04, l1: 0.98484, l2: 0.03168\n",
            "Loss: 1.285192e-04, l1: 0.98535, l2: 0.03162\n",
            "Loss: 1.280411e-04, l1: 0.98535, l2: 0.03158\n",
            "Loss: 1.273665e-04, l1: 0.98569, l2: 0.03157\n",
            "Loss: 1.263019e-04, l1: 0.98470, l2: 0.03153\n",
            "Loss: 1.262246e-04, l1: 0.98680, l2: 0.03161\n",
            "Loss: 1.256588e-04, l1: 0.98578, l2: 0.03157\n",
            "Loss: 1.249740e-04, l1: 0.98608, l2: 0.03160\n",
            "Loss: 1.248718e-04, l1: 0.98448, l2: 0.03161\n",
            "Loss: 1.242033e-04, l1: 0.98629, l2: 0.03168\n",
            "Loss: 1.239940e-04, l1: 0.98639, l2: 0.03167\n",
            "Loss: 1.232981e-04, l1: 0.98674, l2: 0.03167\n",
            "Loss: 1.224984e-04, l1: 0.98762, l2: 0.03172\n",
            "Loss: 1.210393e-04, l1: 0.98893, l2: 0.03179\n",
            "Loss: 1.193419e-04, l1: 0.98924, l2: 0.03176\n",
            "Loss: 1.188125e-04, l1: 0.99153, l2: 0.03171\n",
            "Loss: 1.158956e-04, l1: 0.99024, l2: 0.03181\n",
            "Loss: 1.143976e-04, l1: 0.98845, l2: 0.03162\n",
            "Loss: 1.133608e-04, l1: 0.98949, l2: 0.03158\n",
            "Loss: 1.127788e-04, l1: 0.98928, l2: 0.03155\n",
            "Loss: 1.121478e-04, l1: 0.98906, l2: 0.03151\n",
            "Loss: 1.113204e-04, l1: 0.98834, l2: 0.03149\n",
            "Loss: 1.110356e-04, l1: 0.98706, l2: 0.03150\n",
            "Loss: 1.106556e-04, l1: 0.98730, l2: 0.03152\n",
            "Loss: 1.102656e-04, l1: 0.98768, l2: 0.03157\n",
            "Loss: 1.098528e-04, l1: 0.98780, l2: 0.03160\n",
            "Loss: 1.088430e-04, l1: 0.98891, l2: 0.03169\n",
            "Loss: 1.085171e-04, l1: 0.98708, l2: 0.03162\n",
            "Loss: 1.069145e-04, l1: 0.98876, l2: 0.03165\n",
            "Loss: 1.060846e-04, l1: 0.98947, l2: 0.03161\n",
            "Loss: 1.056129e-04, l1: 0.98962, l2: 0.03156\n",
            "Loss: 1.049850e-04, l1: 0.98920, l2: 0.03153\n",
            "Loss: 1.040900e-04, l1: 0.98829, l2: 0.03150\n",
            "Loss: 1.045498e-04, l1: 0.98885, l2: 0.03158\n",
            "Loss: 1.036456e-04, l1: 0.98852, l2: 0.03153\n",
            "Loss: 1.025970e-04, l1: 0.98764, l2: 0.03156\n",
            "Loss: 1.014487e-04, l1: 0.98714, l2: 0.03165\n",
            "Loss: 1.009008e-04, l1: 0.98683, l2: 0.03173\n",
            "Loss: 1.004857e-04, l1: 0.98689, l2: 0.03175\n",
            "Loss: 1.000697e-04, l1: 0.98660, l2: 0.03174\n",
            "Loss: 9.981659e-05, l1: 0.98622, l2: 0.03173\n",
            "Loss: 9.924941e-05, l1: 0.98511, l2: 0.03171\n",
            "Loss: 1.025234e-04, l1: 0.98127, l2: 0.03170\n",
            "Loss: 9.894512e-05, l1: 0.98425, l2: 0.03171\n",
            "Loss: 9.827359e-05, l1: 0.98368, l2: 0.03168\n",
            "Loss: 9.768505e-05, l1: 0.98363, l2: 0.03169\n",
            "Loss: 9.721630e-05, l1: 0.98400, l2: 0.03170\n",
            "Loss: 9.649275e-05, l1: 0.98481, l2: 0.03171\n",
            "Loss: 9.527368e-05, l1: 0.98453, l2: 0.03165\n",
            "Loss: 9.405964e-05, l1: 0.98792, l2: 0.03173\n",
            "Loss: 9.320508e-05, l1: 0.98686, l2: 0.03166\n",
            "Loss: 9.253116e-05, l1: 0.98576, l2: 0.03154\n",
            "Loss: 9.211509e-05, l1: 0.98640, l2: 0.03159\n",
            "Loss: 9.179649e-05, l1: 0.98659, l2: 0.03162\n",
            "Loss: 9.158005e-05, l1: 0.98682, l2: 0.03166\n",
            "Loss: 9.138578e-05, l1: 0.98655, l2: 0.03167\n",
            "Loss: 9.115413e-05, l1: 0.98648, l2: 0.03169\n",
            "Loss: 9.080761e-05, l1: 0.98676, l2: 0.03171\n",
            "Loss: 9.027177e-05, l1: 0.98740, l2: 0.03174\n",
            "Loss: 9.174092e-05, l1: 0.98919, l2: 0.03181\n",
            "Loss: 9.001256e-05, l1: 0.98790, l2: 0.03176\n",
            "Loss: 8.968578e-05, l1: 0.98791, l2: 0.03177\n",
            "Loss: 8.930375e-05, l1: 0.98799, l2: 0.03175\n",
            "Loss: 9.005885e-05, l1: 0.98736, l2: 0.03176\n",
            "Loss: 8.924694e-05, l1: 0.98786, l2: 0.03175\n",
            "Loss: 8.911513e-05, l1: 0.98719, l2: 0.03173\n",
            "Loss: 8.898594e-05, l1: 0.98650, l2: 0.03171\n",
            "Loss: 8.887467e-05, l1: 0.98610, l2: 0.03171\n",
            "Loss: 8.870848e-05, l1: 0.98592, l2: 0.03172\n",
            "Loss: 8.837514e-05, l1: 0.98589, l2: 0.03174\n",
            "Loss: 8.791900e-05, l1: 0.98692, l2: 0.03183\n",
            "Loss: 8.744673e-05, l1: 0.98723, l2: 0.03183\n",
            "Loss: 8.683410e-05, l1: 0.98774, l2: 0.03184\n",
            "Loss: 8.625537e-05, l1: 0.98824, l2: 0.03187\n",
            "Loss: 8.580611e-05, l1: 0.98839, l2: 0.03190\n",
            "Loss: 8.536724e-05, l1: 0.98867, l2: 0.03193\n",
            "Loss: 8.502701e-05, l1: 0.98874, l2: 0.03195\n",
            "Loss: 8.475118e-05, l1: 0.98894, l2: 0.03194\n",
            "Loss: 8.445064e-05, l1: 0.98934, l2: 0.03194\n",
            "Loss: 8.419536e-05, l1: 0.99023, l2: 0.03190\n",
            "Loss: 8.394325e-05, l1: 0.99051, l2: 0.03188\n",
            "Loss: 8.364335e-05, l1: 0.99089, l2: 0.03186\n",
            "Loss: 8.339392e-05, l1: 0.99116, l2: 0.03185\n",
            "Loss: 8.326182e-05, l1: 0.99101, l2: 0.03186\n",
            "Loss: 8.313691e-05, l1: 0.99073, l2: 0.03186\n",
            "Loss: 8.304844e-05, l1: 0.99080, l2: 0.03188\n",
            "Loss: 8.295284e-05, l1: 0.99061, l2: 0.03188\n",
            "Loss: 8.277973e-05, l1: 0.99055, l2: 0.03187\n",
            "Loss: 8.262811e-05, l1: 0.99056, l2: 0.03187\n",
            "Loss: 8.249843e-05, l1: 0.99060, l2: 0.03186\n",
            "Loss: 8.237414e-05, l1: 0.99092, l2: 0.03187\n",
            "Loss: 8.218060e-05, l1: 0.99101, l2: 0.03188\n",
            "Loss: 8.188788e-05, l1: 0.99154, l2: 0.03188\n",
            "Loss: 8.150125e-05, l1: 0.99197, l2: 0.03189\n",
            "Loss: 8.107080e-05, l1: 0.99185, l2: 0.03187\n",
            "Loss: 8.070190e-05, l1: 0.99163, l2: 0.03186\n",
            "Loss: 8.030416e-05, l1: 0.99118, l2: 0.03183\n",
            "Loss: 8.096638e-05, l1: 0.99204, l2: 0.03184\n",
            "Loss: 8.014748e-05, l1: 0.99144, l2: 0.03183\n",
            "Loss: 7.981736e-05, l1: 0.99122, l2: 0.03183\n",
            "Loss: 7.950047e-05, l1: 0.99124, l2: 0.03182\n",
            "Loss: 7.916060e-05, l1: 0.99150, l2: 0.03184\n",
            "Loss: 7.871592e-05, l1: 0.99178, l2: 0.03185\n",
            "Loss: 7.797375e-05, l1: 0.99190, l2: 0.03186\n",
            "Loss: 7.885009e-05, l1: 0.99005, l2: 0.03170\n",
            "Loss: 7.769716e-05, l1: 0.99129, l2: 0.03180\n",
            "Loss: 7.728223e-05, l1: 0.99006, l2: 0.03172\n",
            "Loss: 7.683590e-05, l1: 0.99071, l2: 0.03177\n",
            "Loss: 7.665651e-05, l1: 0.99088, l2: 0.03176\n",
            "Loss: 7.648106e-05, l1: 0.99052, l2: 0.03176\n",
            "Loss: 7.632400e-05, l1: 0.99023, l2: 0.03175\n",
            "Loss: 7.617688e-05, l1: 0.99021, l2: 0.03175\n",
            "Loss: 7.593807e-05, l1: 0.98964, l2: 0.03179\n",
            "Loss: 7.566168e-05, l1: 0.98977, l2: 0.03184\n",
            "Loss: 7.538426e-05, l1: 0.99003, l2: 0.03189\n",
            "Loss: 7.524663e-05, l1: 0.98990, l2: 0.03191\n",
            "Loss: 7.501301e-05, l1: 0.99000, l2: 0.03192\n",
            "Loss: 7.502442e-05, l1: 0.98997, l2: 0.03193\n",
            "Loss: 7.486902e-05, l1: 0.98998, l2: 0.03192\n",
            "Loss: 7.500073e-05, l1: 0.98979, l2: 0.03193\n",
            "Loss: 7.477798e-05, l1: 0.98990, l2: 0.03193\n",
            "Loss: 7.466132e-05, l1: 0.99005, l2: 0.03191\n",
            "Loss: 7.449494e-05, l1: 0.99011, l2: 0.03189\n",
            "Loss: 7.437325e-05, l1: 0.99055, l2: 0.03190\n",
            "Loss: 7.416830e-05, l1: 0.99107, l2: 0.03191\n",
            "Loss: 7.398748e-05, l1: 0.99136, l2: 0.03194\n",
            "Loss: 7.361141e-05, l1: 0.99147, l2: 0.03196\n",
            "Loss: 7.299730e-05, l1: 0.99084, l2: 0.03196\n",
            "Loss: 7.256622e-05, l1: 0.99194, l2: 0.03199\n",
            "Loss: 7.204883e-05, l1: 0.99042, l2: 0.03191\n",
            "Loss: 7.166290e-05, l1: 0.98994, l2: 0.03185\n",
            "Loss: 7.129980e-05, l1: 0.99019, l2: 0.03180\n",
            "Loss: 7.093218e-05, l1: 0.99026, l2: 0.03175\n",
            "Loss: 7.036174e-05, l1: 0.99103, l2: 0.03175\n",
            "Loss: 6.953599e-05, l1: 0.99195, l2: 0.03173\n",
            "Loss: 6.914775e-05, l1: 0.99259, l2: 0.03178\n",
            "Loss: 6.893193e-05, l1: 0.99255, l2: 0.03180\n",
            "Loss: 6.869671e-05, l1: 0.99230, l2: 0.03180\n",
            "Loss: 6.851614e-05, l1: 0.99217, l2: 0.03178\n",
            "Loss: 6.833202e-05, l1: 0.99211, l2: 0.03175\n",
            "Loss: 6.846087e-05, l1: 0.99259, l2: 0.03167\n",
            "Loss: 6.825884e-05, l1: 0.99229, l2: 0.03172\n",
            "Loss: 6.810894e-05, l1: 0.99226, l2: 0.03171\n",
            "Loss: 6.783314e-05, l1: 0.99228, l2: 0.03171\n",
            "Loss: 6.773433e-05, l1: 0.99196, l2: 0.03173\n",
            "Loss: 6.761069e-05, l1: 0.99200, l2: 0.03170\n",
            "Loss: 6.747716e-05, l1: 0.99195, l2: 0.03169\n",
            "Loss: 6.728667e-05, l1: 0.99193, l2: 0.03169\n",
            "Loss: 6.710824e-05, l1: 0.99188, l2: 0.03169\n",
            "Loss: 6.686970e-05, l1: 0.99212, l2: 0.03170\n",
            "Loss: 6.669600e-05, l1: 0.99148, l2: 0.03169\n",
            "Loss: 6.644626e-05, l1: 0.99187, l2: 0.03170\n",
            "Loss: 6.609564e-05, l1: 0.99223, l2: 0.03171\n",
            "Loss: 6.589745e-05, l1: 0.99220, l2: 0.03170\n",
            "Loss: 6.574152e-05, l1: 0.99194, l2: 0.03169\n",
            "Loss: 6.577680e-05, l1: 0.99222, l2: 0.03171\n",
            "Loss: 6.567105e-05, l1: 0.99206, l2: 0.03170\n",
            "Loss: 6.555289e-05, l1: 0.99199, l2: 0.03170\n",
            "Loss: 6.530934e-05, l1: 0.99210, l2: 0.03173\n",
            "Loss: 6.514463e-05, l1: 0.99250, l2: 0.03176\n",
            "Loss: 6.488633e-05, l1: 0.99287, l2: 0.03177\n",
            "Loss: 6.466252e-05, l1: 0.99343, l2: 0.03179\n",
            "Loss: 6.445109e-05, l1: 0.99390, l2: 0.03180\n",
            "Loss: 6.416751e-05, l1: 0.99433, l2: 0.03180\n",
            "Loss: 6.397583e-05, l1: 0.99435, l2: 0.03177\n",
            "Loss: 6.365269e-05, l1: 0.99445, l2: 0.03177\n",
            "Loss: 6.343741e-05, l1: 0.99425, l2: 0.03175\n",
            "Loss: 6.329464e-05, l1: 0.99409, l2: 0.03173\n",
            "Loss: 6.322643e-05, l1: 0.99399, l2: 0.03173\n",
            "Loss: 6.316556e-05, l1: 0.99402, l2: 0.03173\n",
            "Loss: 6.302818e-05, l1: 0.99396, l2: 0.03173\n",
            "Loss: 6.295144e-05, l1: 0.99374, l2: 0.03173\n",
            "Loss: 6.288300e-05, l1: 0.99349, l2: 0.03172\n",
            "Loss: 6.283677e-05, l1: 0.99340, l2: 0.03171\n",
            "Loss: 6.279305e-05, l1: 0.99306, l2: 0.03170\n",
            "Loss: 6.272233e-05, l1: 0.99340, l2: 0.03171\n",
            "Loss: 6.263581e-05, l1: 0.99392, l2: 0.03171\n",
            "Loss: 6.253937e-05, l1: 0.99432, l2: 0.03171\n",
            "Loss: 6.239890e-05, l1: 0.99491, l2: 0.03171\n",
            "Loss: 6.226052e-05, l1: 0.99521, l2: 0.03171\n",
            "Loss: 6.200709e-05, l1: 0.99566, l2: 0.03172\n",
            "Loss: 6.181021e-05, l1: 0.99561, l2: 0.03172\n",
            "Loss: 6.160634e-05, l1: 0.99556, l2: 0.03172\n",
            "Loss: 6.133156e-05, l1: 0.99598, l2: 0.03172\n",
            "Loss: 6.097270e-05, l1: 0.99667, l2: 0.03172\n",
            "Loss: 6.071932e-05, l1: 0.99718, l2: 0.03174\n",
            "Loss: 6.045366e-05, l1: 0.99692, l2: 0.03174\n",
            "Loss: 6.014284e-05, l1: 0.99664, l2: 0.03175\n",
            "Loss: 5.990901e-05, l1: 0.99587, l2: 0.03176\n",
            "Loss: 5.965642e-05, l1: 0.99539, l2: 0.03175\n",
            "Loss: 5.945946e-05, l1: 0.99502, l2: 0.03176\n",
            "Loss: 5.927566e-05, l1: 0.99412, l2: 0.03173\n",
            "Loss: 5.912858e-05, l1: 0.99430, l2: 0.03174\n",
            "Loss: 5.901388e-05, l1: 0.99453, l2: 0.03174\n",
            "Loss: 5.889622e-05, l1: 0.99468, l2: 0.03173\n",
            "Loss: 5.876201e-05, l1: 0.99476, l2: 0.03173\n",
            "Loss: 5.852040e-05, l1: 0.99487, l2: 0.03176\n",
            "Loss: 5.822661e-05, l1: 0.99487, l2: 0.03180\n",
            "Loss: 5.806948e-05, l1: 0.99551, l2: 0.03188\n",
            "Loss: 5.785671e-05, l1: 0.99516, l2: 0.03187\n",
            "Loss: 5.775264e-05, l1: 0.99493, l2: 0.03186\n",
            "Loss: 5.760284e-05, l1: 0.99486, l2: 0.03185\n",
            "Loss: 5.738997e-05, l1: 0.99474, l2: 0.03183\n",
            "Loss: 5.712519e-05, l1: 0.99481, l2: 0.03181\n",
            "Loss: 5.693627e-05, l1: 0.99478, l2: 0.03179\n",
            "Loss: 5.682777e-05, l1: 0.99489, l2: 0.03180\n",
            "Loss: 5.675633e-05, l1: 0.99492, l2: 0.03181\n",
            "Loss: 5.666394e-05, l1: 0.99417, l2: 0.03183\n",
            "Loss: 5.633802e-05, l1: 0.99409, l2: 0.03184\n",
            "Loss: 5.620127e-05, l1: 0.99403, l2: 0.03184\n",
            "Loss: 5.607251e-05, l1: 0.99381, l2: 0.03184\n",
            "Loss: 5.593012e-05, l1: 0.99397, l2: 0.03185\n",
            "Loss: 5.580954e-05, l1: 0.99390, l2: 0.03186\n",
            "Loss: 5.569254e-05, l1: 0.99377, l2: 0.03186\n",
            "Loss: 5.559566e-05, l1: 0.99378, l2: 0.03186\n",
            "Loss: 5.547918e-05, l1: 0.99372, l2: 0.03184\n",
            "Loss: 5.528018e-05, l1: 0.99361, l2: 0.03181\n",
            "Loss: 5.512893e-05, l1: 0.99359, l2: 0.03179\n",
            "Loss: 5.496159e-05, l1: 0.99328, l2: 0.03173\n",
            "Loss: 5.477730e-05, l1: 0.99335, l2: 0.03173\n",
            "Loss: 5.465135e-05, l1: 0.99369, l2: 0.03173\n",
            "Loss: 5.438887e-05, l1: 0.99340, l2: 0.03172\n",
            "Loss: 5.430188e-05, l1: 0.99322, l2: 0.03171\n",
            "Loss: 5.411858e-05, l1: 0.99305, l2: 0.03170\n",
            "Loss: 5.391860e-05, l1: 0.99304, l2: 0.03171\n",
            "Loss: 5.440559e-05, l1: 0.99370, l2: 0.03172\n",
            "Loss: 5.383600e-05, l1: 0.99322, l2: 0.03171\n",
            "Loss: 5.359721e-05, l1: 0.99355, l2: 0.03174\n",
            "Loss: 5.335251e-05, l1: 0.99412, l2: 0.03177\n",
            "Loss: 5.342775e-05, l1: 0.99546, l2: 0.03179\n",
            "Loss: 5.323996e-05, l1: 0.99471, l2: 0.03178\n",
            "Loss: 5.310646e-05, l1: 0.99512, l2: 0.03178\n",
            "Loss: 5.298772e-05, l1: 0.99526, l2: 0.03177\n",
            "Loss: 5.287204e-05, l1: 0.99524, l2: 0.03175\n",
            "Loss: 5.278640e-05, l1: 0.99510, l2: 0.03173\n",
            "Loss: 5.267917e-05, l1: 0.99496, l2: 0.03173\n",
            "Loss: 5.255959e-05, l1: 0.99485, l2: 0.03173\n",
            "Loss: 5.234755e-05, l1: 0.99495, l2: 0.03175\n",
            "Loss: 5.209182e-05, l1: 0.99521, l2: 0.03179\n",
            "Loss: 5.172295e-05, l1: 0.99627, l2: 0.03185\n",
            "Loss: 5.158415e-05, l1: 0.99664, l2: 0.03187\n",
            "Loss: 5.148341e-05, l1: 0.99676, l2: 0.03187\n",
            "Loss: 5.137733e-05, l1: 0.99678, l2: 0.03186\n",
            "Loss: 5.126813e-05, l1: 0.99680, l2: 0.03186\n",
            "Loss: 5.111644e-05, l1: 0.99657, l2: 0.03186\n",
            "Loss: 5.098267e-05, l1: 0.99661, l2: 0.03187\n",
            "Loss: 5.088141e-05, l1: 0.99651, l2: 0.03188\n",
            "Loss: 5.075423e-05, l1: 0.99639, l2: 0.03189\n",
            "Loss: 5.062972e-05, l1: 0.99668, l2: 0.03191\n",
            "Loss: 5.042934e-05, l1: 0.99643, l2: 0.03190\n",
            "Loss: 5.023665e-05, l1: 0.99632, l2: 0.03190\n",
            "Loss: 4.996831e-05, l1: 0.99613, l2: 0.03188\n",
            "Loss: 5.089357e-05, l1: 0.99553, l2: 0.03192\n",
            "Loss: 4.986759e-05, l1: 0.99598, l2: 0.03189\n",
            "Loss: 4.955719e-05, l1: 0.99585, l2: 0.03190\n",
            "Loss: 4.944019e-05, l1: 0.99576, l2: 0.03190\n",
            "Loss: 4.931985e-05, l1: 0.99566, l2: 0.03191\n",
            "Loss: 4.925029e-05, l1: 0.99551, l2: 0.03192\n",
            "Loss: 4.918930e-05, l1: 0.99534, l2: 0.03192\n",
            "Loss: 4.913477e-05, l1: 0.99522, l2: 0.03192\n",
            "Loss: 4.904907e-05, l1: 0.99527, l2: 0.03191\n",
            "Loss: 4.893374e-05, l1: 0.99527, l2: 0.03189\n",
            "Loss: 4.884457e-05, l1: 0.99540, l2: 0.03188\n",
            "Loss: 4.887854e-05, l1: 0.99505, l2: 0.03182\n",
            "Loss: 4.875619e-05, l1: 0.99524, l2: 0.03186\n",
            "Loss: 4.858432e-05, l1: 0.99551, l2: 0.03186\n",
            "Loss: 4.852519e-05, l1: 0.99583, l2: 0.03188\n",
            "Loss: 4.843833e-05, l1: 0.99561, l2: 0.03188\n",
            "Loss: 4.836654e-05, l1: 0.99553, l2: 0.03188\n",
            "Loss: 4.818245e-05, l1: 0.99524, l2: 0.03187\n",
            "Loss: 4.814348e-05, l1: 0.99512, l2: 0.03185\n",
            "Loss: 4.790410e-05, l1: 0.99507, l2: 0.03186\n",
            "Loss: 4.782813e-05, l1: 0.99503, l2: 0.03185\n",
            "Loss: 4.777362e-05, l1: 0.99495, l2: 0.03183\n",
            "Loss: 4.773624e-05, l1: 0.99476, l2: 0.03182\n",
            "Loss: 4.768173e-05, l1: 0.99465, l2: 0.03181\n",
            "Loss: 4.758798e-05, l1: 0.99447, l2: 0.03180\n",
            "Loss: 4.752586e-05, l1: 0.99435, l2: 0.03179\n",
            "Loss: 4.748497e-05, l1: 0.99435, l2: 0.03179\n",
            "Loss: 4.744906e-05, l1: 0.99435, l2: 0.03180\n",
            "Loss: 4.756662e-05, l1: 0.99373, l2: 0.03178\n",
            "Loss: 4.740379e-05, l1: 0.99414, l2: 0.03179\n",
            "Loss: 4.729856e-05, l1: 0.99410, l2: 0.03179\n",
            "Loss: 4.719025e-05, l1: 0.99419, l2: 0.03178\n",
            "Loss: 4.706043e-05, l1: 0.99441, l2: 0.03177\n",
            "Loss: 4.696705e-05, l1: 0.99440, l2: 0.03176\n",
            "Loss: 4.702192e-05, l1: 0.99527, l2: 0.03179\n",
            "Loss: 4.690259e-05, l1: 0.99477, l2: 0.03177\n",
            "Loss: 4.684143e-05, l1: 0.99470, l2: 0.03177\n",
            "Loss: 4.680137e-05, l1: 0.99462, l2: 0.03177\n",
            "Loss: 4.677389e-05, l1: 0.99462, l2: 0.03177\n",
            "Loss: 4.669588e-05, l1: 0.99462, l2: 0.03177\n",
            "Loss: 4.660474e-05, l1: 0.99466, l2: 0.03176\n",
            "Loss: 4.649630e-05, l1: 0.99522, l2: 0.03174\n",
            "Loss: 4.640164e-05, l1: 0.99576, l2: 0.03176\n",
            "Loss: 4.626956e-05, l1: 0.99567, l2: 0.03176\n",
            "Loss: 4.621365e-05, l1: 0.99557, l2: 0.03175\n",
            "Loss: 4.617182e-05, l1: 0.99578, l2: 0.03175\n",
            "Loss: 4.609074e-05, l1: 0.99608, l2: 0.03176\n",
            "Loss: 4.603693e-05, l1: 0.99659, l2: 0.03177\n",
            "Loss: 4.596857e-05, l1: 0.99657, l2: 0.03178\n",
            "Loss: 4.593267e-05, l1: 0.99637, l2: 0.03179\n",
            "Loss: 4.589530e-05, l1: 0.99625, l2: 0.03179\n",
            "Loss: 4.585490e-05, l1: 0.99635, l2: 0.03180\n",
            "Loss: 4.580298e-05, l1: 0.99651, l2: 0.03181\n",
            "Loss: 4.572825e-05, l1: 0.99670, l2: 0.03181\n",
            "Loss: 4.571964e-05, l1: 0.99741, l2: 0.03182\n",
            "Loss: 4.563783e-05, l1: 0.99717, l2: 0.03181\n",
            "Loss: 4.561554e-05, l1: 0.99719, l2: 0.03181\n",
            "Loss: 4.559187e-05, l1: 0.99719, l2: 0.03181\n",
            "Loss: 4.557023e-05, l1: 0.99727, l2: 0.03181\n",
            "Loss: 4.553046e-05, l1: 0.99746, l2: 0.03182\n",
            "Loss: 4.547117e-05, l1: 0.99743, l2: 0.03182\n",
            "Loss: 4.542286e-05, l1: 0.99756, l2: 0.03183\n",
            "Loss: 4.539272e-05, l1: 0.99743, l2: 0.03183\n",
            "Loss: 4.536422e-05, l1: 0.99730, l2: 0.03183\n",
            "Loss: 4.531941e-05, l1: 0.99685, l2: 0.03182\n",
            "Loss: 4.527775e-05, l1: 0.99649, l2: 0.03181\n",
            "Loss: 4.522433e-05, l1: 0.99645, l2: 0.03181\n",
            "Loss: 4.518665e-05, l1: 0.99648, l2: 0.03182\n",
            "Loss: 4.515813e-05, l1: 0.99644, l2: 0.03182\n",
            "Loss: 4.508216e-05, l1: 0.99628, l2: 0.03182\n",
            "Loss: 4.516497e-05, l1: 0.99542, l2: 0.03181\n",
            "Loss: 4.504044e-05, l1: 0.99596, l2: 0.03182\n",
            "Loss: 4.495157e-05, l1: 0.99574, l2: 0.03182\n",
            "Loss: 4.488479e-05, l1: 0.99551, l2: 0.03181\n",
            "Loss: 4.482599e-05, l1: 0.99536, l2: 0.03180\n",
            "Loss: 4.476077e-05, l1: 0.99505, l2: 0.03178\n",
            "Loss: 4.469016e-05, l1: 0.99498, l2: 0.03177\n",
            "Loss: 4.457532e-05, l1: 0.99496, l2: 0.03177\n",
            "Loss: 4.467647e-05, l1: 0.99508, l2: 0.03175\n",
            "Loss: 4.452106e-05, l1: 0.99500, l2: 0.03176\n",
            "Loss: 4.445315e-05, l1: 0.99492, l2: 0.03176\n",
            "Loss: 4.445529e-05, l1: 0.99528, l2: 0.03174\n",
            "Loss: 4.441389e-05, l1: 0.99510, l2: 0.03175\n",
            "Loss: 4.436108e-05, l1: 0.99514, l2: 0.03175\n",
            "Loss: 4.429388e-05, l1: 0.99515, l2: 0.03176\n",
            "Loss: 4.426239e-05, l1: 0.99514, l2: 0.03176\n",
            "Loss: 4.420734e-05, l1: 0.99504, l2: 0.03176\n",
            "Loss: 4.414613e-05, l1: 0.99509, l2: 0.03176\n",
            "Loss: 4.407683e-05, l1: 0.99514, l2: 0.03176\n",
            "Loss: 4.400169e-05, l1: 0.99529, l2: 0.03177\n",
            "Loss: 4.394987e-05, l1: 0.99545, l2: 0.03177\n",
            "Loss: 4.389017e-05, l1: 0.99564, l2: 0.03177\n",
            "Loss: 4.389781e-05, l1: 0.99618, l2: 0.03181\n",
            "Loss: 4.385435e-05, l1: 0.99590, l2: 0.03179\n",
            "Loss: 4.380364e-05, l1: 0.99605, l2: 0.03178\n",
            "Loss: 4.375952e-05, l1: 0.99624, l2: 0.03178\n",
            "Loss: 4.372515e-05, l1: 0.99641, l2: 0.03179\n",
            "Loss: 4.364199e-05, l1: 0.99660, l2: 0.03179\n",
            "Loss: 4.375116e-05, l1: 0.99737, l2: 0.03181\n",
            "Loss: 4.360380e-05, l1: 0.99686, l2: 0.03180\n",
            "Loss: 4.353131e-05, l1: 0.99686, l2: 0.03180\n",
            "Loss: 4.349566e-05, l1: 0.99686, l2: 0.03180\n",
            "Loss: 4.347204e-05, l1: 0.99685, l2: 0.03180\n",
            "Loss: 4.344920e-05, l1: 0.99684, l2: 0.03179\n",
            "Loss: 4.340428e-05, l1: 0.99698, l2: 0.03178\n",
            "Loss: 4.339901e-05, l1: 0.99701, l2: 0.03177\n",
            "Loss: 4.333607e-05, l1: 0.99720, l2: 0.03178\n",
            "Loss: 4.329381e-05, l1: 0.99733, l2: 0.03178\n",
            "Loss: 4.323767e-05, l1: 0.99761, l2: 0.03179\n",
            "Loss: 4.318524e-05, l1: 0.99770, l2: 0.03180\n",
            "Loss: 4.314581e-05, l1: 0.99815, l2: 0.03180\n",
            "Loss: 4.310969e-05, l1: 0.99788, l2: 0.03180\n",
            "Loss: 4.309427e-05, l1: 0.99772, l2: 0.03179\n",
            "Loss: 4.307819e-05, l1: 0.99764, l2: 0.03179\n",
            "Loss: 4.303736e-05, l1: 0.99758, l2: 0.03179\n",
            "Loss: 4.298266e-05, l1: 0.99768, l2: 0.03180\n",
            "Loss: 4.293430e-05, l1: 0.99760, l2: 0.03180\n",
            "Loss: 4.289536e-05, l1: 0.99770, l2: 0.03181\n",
            "Loss: 4.284270e-05, l1: 0.99770, l2: 0.03181\n",
            "Loss: 4.281985e-05, l1: 0.99740, l2: 0.03180\n",
            "Loss: 4.277831e-05, l1: 0.99731, l2: 0.03180\n",
            "Loss: 4.276013e-05, l1: 0.99717, l2: 0.03179\n",
            "Loss: 4.274865e-05, l1: 0.99708, l2: 0.03179\n",
            "Loss: 4.271849e-05, l1: 0.99698, l2: 0.03178\n",
            "Loss: 4.284312e-05, l1: 0.99724, l2: 0.03177\n",
            "Loss: 4.269925e-05, l1: 0.99705, l2: 0.03178\n",
            "Loss: 4.265952e-05, l1: 0.99691, l2: 0.03178\n",
            "Loss: 4.262810e-05, l1: 0.99672, l2: 0.03178\n",
            "Loss: 4.260363e-05, l1: 0.99671, l2: 0.03178\n",
            "Loss: 4.258285e-05, l1: 0.99663, l2: 0.03178\n",
            "Loss: 4.256047e-05, l1: 0.99653, l2: 0.03178\n",
            "Loss: 4.252780e-05, l1: 0.99640, l2: 0.03177\n",
            "Loss: 4.246279e-05, l1: 0.99614, l2: 0.03177\n",
            "Loss: 4.237761e-05, l1: 0.99605, l2: 0.03177\n",
            "Loss: 4.225712e-05, l1: 0.99559, l2: 0.03174\n",
            "Loss: 4.216890e-05, l1: 0.99600, l2: 0.03176\n",
            "Loss: 4.209792e-05, l1: 0.99581, l2: 0.03175\n",
            "Loss: 4.202391e-05, l1: 0.99584, l2: 0.03174\n",
            "Loss: 4.197259e-05, l1: 0.99593, l2: 0.03174\n",
            "Loss: 4.192571e-05, l1: 0.99595, l2: 0.03173\n",
            "Loss: 4.188143e-05, l1: 0.99619, l2: 0.03173\n",
            "Loss: 4.180988e-05, l1: 0.99612, l2: 0.03172\n",
            "Loss: 4.176599e-05, l1: 0.99619, l2: 0.03172\n",
            "Loss: 4.169664e-05, l1: 0.99629, l2: 0.03173\n",
            "Loss: 4.186496e-05, l1: 0.99639, l2: 0.03175\n",
            "Loss: 4.165919e-05, l1: 0.99632, l2: 0.03173\n",
            "Loss: 4.159210e-05, l1: 0.99636, l2: 0.03173\n",
            "Loss: 4.149655e-05, l1: 0.99640, l2: 0.03172\n",
            "Loss: 4.142550e-05, l1: 0.99662, l2: 0.03169\n",
            "Loss: 4.205139e-05, l1: 0.99628, l2: 0.03166\n",
            "Loss: 4.137625e-05, l1: 0.99655, l2: 0.03169\n",
            "Loss: 4.130638e-05, l1: 0.99664, l2: 0.03168\n",
            "Loss: 4.118038e-05, l1: 0.99683, l2: 0.03166\n",
            "Loss: 4.114557e-05, l1: 0.99699, l2: 0.03166\n",
            "Loss: 4.110338e-05, l1: 0.99715, l2: 0.03166\n",
            "Loss: 4.107221e-05, l1: 0.99736, l2: 0.03166\n",
            "Loss: 4.161477e-05, l1: 0.99626, l2: 0.03165\n",
            "Loss: 4.105763e-05, l1: 0.99721, l2: 0.03166\n",
            "Loss: 4.103586e-05, l1: 0.99717, l2: 0.03166\n",
            "Loss: 4.101525e-05, l1: 0.99706, l2: 0.03166\n",
            "Loss: 4.097257e-05, l1: 0.99696, l2: 0.03165\n",
            "Loss: 4.110846e-05, l1: 0.99661, l2: 0.03164\n",
            "Loss: 4.094333e-05, l1: 0.99686, l2: 0.03165\n",
            "Loss: 4.088581e-05, l1: 0.99675, l2: 0.03164\n",
            "Loss: 4.085460e-05, l1: 0.99672, l2: 0.03165\n",
            "Loss: 4.081201e-05, l1: 0.99677, l2: 0.03166\n",
            "Loss: 4.077127e-05, l1: 0.99679, l2: 0.03168\n",
            "Loss: 4.071158e-05, l1: 0.99697, l2: 0.03169\n",
            "Loss: 4.062134e-05, l1: 0.99719, l2: 0.03170\n",
            "Loss: 4.094018e-05, l1: 0.99878, l2: 0.03172\n",
            "Loss: 4.058590e-05, l1: 0.99757, l2: 0.03170\n",
            "Loss: 4.050529e-05, l1: 0.99772, l2: 0.03170\n",
            "Loss: 4.046692e-05, l1: 0.99782, l2: 0.03170\n",
            "Loss: 4.044471e-05, l1: 0.99787, l2: 0.03170\n",
            "Loss: 4.042799e-05, l1: 0.99800, l2: 0.03171\n",
            "Loss: 4.039793e-05, l1: 0.99812, l2: 0.03171\n",
            "Loss: 4.036659e-05, l1: 0.99821, l2: 0.03172\n",
            "Loss: 4.032919e-05, l1: 0.99828, l2: 0.03173\n",
            "Loss: 4.029406e-05, l1: 0.99853, l2: 0.03174\n",
            "Loss: 4.025096e-05, l1: 0.99841, l2: 0.03174\n",
            "Loss: 4.020115e-05, l1: 0.99827, l2: 0.03173\n",
            "Loss: 4.014831e-05, l1: 0.99816, l2: 0.03173\n",
            "Loss: 4.018928e-05, l1: 0.99796, l2: 0.03175\n",
            "Loss: 4.011495e-05, l1: 0.99808, l2: 0.03174\n",
            "Loss: 4.004289e-05, l1: 0.99814, l2: 0.03174\n",
            "Loss: 3.998756e-05, l1: 0.99825, l2: 0.03175\n",
            "Loss: 3.995125e-05, l1: 0.99844, l2: 0.03175\n",
            "Loss: 3.992040e-05, l1: 0.99845, l2: 0.03176\n",
            "Loss: 3.990027e-05, l1: 0.99849, l2: 0.03177\n",
            "Loss: 3.986418e-05, l1: 0.99851, l2: 0.03177\n",
            "Loss: 3.982724e-05, l1: 0.99851, l2: 0.03178\n",
            "Loss: 3.975663e-05, l1: 0.99854, l2: 0.03179\n",
            "Loss: 3.972368e-05, l1: 0.99856, l2: 0.03181\n",
            "Loss: 3.967145e-05, l1: 0.99863, l2: 0.03181\n",
            "Loss: 3.964962e-05, l1: 0.99858, l2: 0.03181\n",
            "Loss: 3.963365e-05, l1: 0.99857, l2: 0.03181\n",
            "Loss: 3.961432e-05, l1: 0.99856, l2: 0.03181\n",
            "Loss: 3.958560e-05, l1: 0.99848, l2: 0.03182\n",
            "Loss: 3.957783e-05, l1: 0.99837, l2: 0.03184\n",
            "Loss: 3.953965e-05, l1: 0.99834, l2: 0.03184\n",
            "Loss: 3.951877e-05, l1: 0.99827, l2: 0.03184\n",
            "Loss: 3.948823e-05, l1: 0.99811, l2: 0.03184\n",
            "Loss: 3.945771e-05, l1: 0.99792, l2: 0.03184\n",
            "Loss: 3.941781e-05, l1: 0.99765, l2: 0.03183\n",
            "Loss: 3.940219e-05, l1: 0.99724, l2: 0.03185\n",
            "Loss: 3.945046e-05, l1: 0.99705, l2: 0.03187\n",
            "Loss: 3.937427e-05, l1: 0.99717, l2: 0.03186\n",
            "Loss: 3.934202e-05, l1: 0.99731, l2: 0.03185\n",
            "Loss: 3.931754e-05, l1: 0.99726, l2: 0.03186\n",
            "Loss: 3.929072e-05, l1: 0.99712, l2: 0.03187\n",
            "Loss: 3.926621e-05, l1: 0.99725, l2: 0.03187\n",
            "Loss: 3.924832e-05, l1: 0.99719, l2: 0.03187\n",
            "Loss: 3.923013e-05, l1: 0.99708, l2: 0.03187\n",
            "Loss: 3.922090e-05, l1: 0.99718, l2: 0.03186\n",
            "Loss: 3.920359e-05, l1: 0.99713, l2: 0.03186\n",
            "Loss: 3.918731e-05, l1: 0.99717, l2: 0.03186\n",
            "Loss: 3.917731e-05, l1: 0.99724, l2: 0.03186\n",
            "Loss: 3.915883e-05, l1: 0.99735, l2: 0.03186\n",
            "Loss: 3.915837e-05, l1: 0.99751, l2: 0.03186\n",
            "Loss: 3.914383e-05, l1: 0.99743, l2: 0.03186\n",
            "Loss: 3.912106e-05, l1: 0.99749, l2: 0.03186\n",
            "Loss: 3.910565e-05, l1: 0.99749, l2: 0.03186\n",
            "Loss: 3.909771e-05, l1: 0.99749, l2: 0.03187\n",
            "Loss: 3.908853e-05, l1: 0.99758, l2: 0.03187\n",
            "Loss: 3.907640e-05, l1: 0.99760, l2: 0.03187\n",
            "Loss: 3.905437e-05, l1: 0.99772, l2: 0.03187\n",
            "Loss: 3.904310e-05, l1: 0.99780, l2: 0.03186\n",
            "Loss: 3.902429e-05, l1: 0.99785, l2: 0.03186\n",
            "Loss: 3.900695e-05, l1: 0.99775, l2: 0.03185\n",
            "Loss: 3.898923e-05, l1: 0.99769, l2: 0.03184\n",
            "Loss: 3.896300e-05, l1: 0.99752, l2: 0.03183\n",
            "Loss: 3.892257e-05, l1: 0.99728, l2: 0.03182\n",
            "Loss: 3.889059e-05, l1: 0.99690, l2: 0.03179\n",
            "Loss: 3.883455e-05, l1: 0.99648, l2: 0.03178\n",
            "Loss: 3.880059e-05, l1: 0.99667, l2: 0.03179\n",
            "Loss: 3.876487e-05, l1: 0.99681, l2: 0.03179\n",
            "Loss: 3.874223e-05, l1: 0.99684, l2: 0.03179\n",
            "Loss: 3.869871e-05, l1: 0.99683, l2: 0.03177\n",
            "Loss: 3.966251e-05, l1: 0.99628, l2: 0.03177\n",
            "Loss: 3.868757e-05, l1: 0.99678, l2: 0.03177\n",
            "Loss: 3.865999e-05, l1: 0.99671, l2: 0.03176\n",
            "Loss: 3.863544e-05, l1: 0.99656, l2: 0.03175\n",
            "Loss: 3.862242e-05, l1: 0.99648, l2: 0.03174\n",
            "Loss: 3.860026e-05, l1: 0.99646, l2: 0.03174\n",
            "Loss: 3.856471e-05, l1: 0.99646, l2: 0.03174\n",
            "Loss: 3.854118e-05, l1: 0.99650, l2: 0.03174\n",
            "Loss: 3.851331e-05, l1: 0.99653, l2: 0.03173\n",
            "Loss: 3.848378e-05, l1: 0.99657, l2: 0.03173\n",
            "Loss: 3.845109e-05, l1: 0.99658, l2: 0.03173\n",
            "Loss: 3.847710e-05, l1: 0.99669, l2: 0.03174\n",
            "Loss: 3.843613e-05, l1: 0.99662, l2: 0.03173\n",
            "Loss: 3.841279e-05, l1: 0.99662, l2: 0.03173\n",
            "Loss: 3.839828e-05, l1: 0.99664, l2: 0.03173\n",
            "Loss: 3.837765e-05, l1: 0.99669, l2: 0.03173\n",
            "Loss: 3.835453e-05, l1: 0.99679, l2: 0.03173\n",
            "Loss: 3.835998e-05, l1: 0.99689, l2: 0.03174\n",
            "Loss: 3.834028e-05, l1: 0.99684, l2: 0.03173\n",
            "Loss: 3.831225e-05, l1: 0.99704, l2: 0.03173\n",
            "Loss: 3.829972e-05, l1: 0.99700, l2: 0.03173\n",
            "Loss: 3.827409e-05, l1: 0.99693, l2: 0.03172\n",
            "Loss: 3.825340e-05, l1: 0.99692, l2: 0.03172\n",
            "Loss: 3.820379e-05, l1: 0.99705, l2: 0.03172\n",
            "Loss: 3.820476e-05, l1: 0.99726, l2: 0.03173\n",
            "Loss: 3.817728e-05, l1: 0.99715, l2: 0.03173\n",
            "Loss: 3.815002e-05, l1: 0.99735, l2: 0.03172\n",
            "Loss: 3.813330e-05, l1: 0.99750, l2: 0.03173\n",
            "Loss: 3.812189e-05, l1: 0.99758, l2: 0.03173\n",
            "Loss: 3.809611e-05, l1: 0.99775, l2: 0.03174\n",
            "Loss: 3.806403e-05, l1: 0.99779, l2: 0.03174\n",
            "Loss: 3.802644e-05, l1: 0.99795, l2: 0.03175\n",
            "Loss: 3.799031e-05, l1: 0.99828, l2: 0.03176\n",
            "Loss: 3.794755e-05, l1: 0.99821, l2: 0.03175\n",
            "Loss: 3.792478e-05, l1: 0.99816, l2: 0.03174\n",
            "Loss: 3.789086e-05, l1: 0.99818, l2: 0.03174\n",
            "Loss: 3.786180e-05, l1: 0.99831, l2: 0.03174\n",
            "Loss: 3.784427e-05, l1: 0.99844, l2: 0.03174\n",
            "Loss: 3.783718e-05, l1: 0.99856, l2: 0.03175\n",
            "Loss: 3.781460e-05, l1: 0.99860, l2: 0.03176\n",
            "Loss: 3.780136e-05, l1: 0.99856, l2: 0.03176\n",
            "Loss: 3.778446e-05, l1: 0.99849, l2: 0.03177\n",
            "Loss: 3.776318e-05, l1: 0.99842, l2: 0.03178\n",
            "Loss: 3.779685e-05, l1: 0.99834, l2: 0.03178\n",
            "Loss: 3.775061e-05, l1: 0.99839, l2: 0.03178\n",
            "Loss: 3.772103e-05, l1: 0.99832, l2: 0.03178\n",
            "Loss: 3.769973e-05, l1: 0.99831, l2: 0.03178\n",
            "Loss: 3.768104e-05, l1: 0.99833, l2: 0.03178\n",
            "Loss: 3.765577e-05, l1: 0.99835, l2: 0.03178\n",
            "Loss: 3.778844e-05, l1: 0.99860, l2: 0.03180\n",
            "Loss: 3.764135e-05, l1: 0.99840, l2: 0.03179\n",
            "Loss: 3.759497e-05, l1: 0.99835, l2: 0.03179\n",
            "Loss: 3.756111e-05, l1: 0.99838, l2: 0.03180\n",
            "Loss: 3.752760e-05, l1: 0.99821, l2: 0.03180\n",
            "Loss: 3.750972e-05, l1: 0.99818, l2: 0.03180\n",
            "Loss: 3.748556e-05, l1: 0.99810, l2: 0.03180\n",
            "Loss: 3.747107e-05, l1: 0.99808, l2: 0.03180\n",
            "Loss: 3.744786e-05, l1: 0.99806, l2: 0.03180\n",
            "Loss: 3.745189e-05, l1: 0.99805, l2: 0.03181\n",
            "Loss: 3.743384e-05, l1: 0.99806, l2: 0.03180\n",
            "Loss: 3.741436e-05, l1: 0.99807, l2: 0.03180\n",
            "Loss: 3.740063e-05, l1: 0.99809, l2: 0.03180\n",
            "Loss: 3.738364e-05, l1: 0.99813, l2: 0.03180\n",
            "Loss: 3.737208e-05, l1: 0.99815, l2: 0.03181\n",
            "Loss: 3.735981e-05, l1: 0.99821, l2: 0.03182\n",
            "Loss: 3.734969e-05, l1: 0.99815, l2: 0.03182\n",
            "Loss: 3.734108e-05, l1: 0.99811, l2: 0.03181\n",
            "Loss: 3.733130e-05, l1: 0.99812, l2: 0.03181\n",
            "Loss: 3.732480e-05, l1: 0.99813, l2: 0.03181\n",
            "Loss: 3.731827e-05, l1: 0.99816, l2: 0.03182\n",
            "Loss: 3.730572e-05, l1: 0.99821, l2: 0.03182\n",
            "Loss: 3.729121e-05, l1: 0.99824, l2: 0.03182\n",
            "Loss: 3.727435e-05, l1: 0.99824, l2: 0.03182\n",
            "Loss: 3.725596e-05, l1: 0.99827, l2: 0.03181\n",
            "Loss: 3.724402e-05, l1: 0.99828, l2: 0.03181\n",
            "Loss: 3.722722e-05, l1: 0.99820, l2: 0.03180\n",
            "Loss: 3.721534e-05, l1: 0.99816, l2: 0.03180\n",
            "Loss: 3.720368e-05, l1: 0.99811, l2: 0.03180\n",
            "Loss: 3.719726e-05, l1: 0.99825, l2: 0.03180\n",
            "Loss: 3.718790e-05, l1: 0.99826, l2: 0.03180\n",
            "Loss: 3.718327e-05, l1: 0.99829, l2: 0.03180\n",
            "Loss: 3.717377e-05, l1: 0.99840, l2: 0.03180\n",
            "Loss: 3.716000e-05, l1: 0.99854, l2: 0.03179\n",
            "Loss: 3.713783e-05, l1: 0.99868, l2: 0.03179\n",
            "Loss: 3.711059e-05, l1: 0.99869, l2: 0.03179\n",
            "Loss: 3.708608e-05, l1: 0.99860, l2: 0.03179\n",
            "Loss: 3.707517e-05, l1: 0.99852, l2: 0.03179\n",
            "Loss: 3.708325e-05, l1: 0.99831, l2: 0.03178\n",
            "Loss: 3.707102e-05, l1: 0.99844, l2: 0.03179\n",
            "Loss: 3.706215e-05, l1: 0.99841, l2: 0.03179\n",
            "Loss: 3.704589e-05, l1: 0.99841, l2: 0.03179\n",
            "Loss: 3.702955e-05, l1: 0.99847, l2: 0.03179\n",
            "Loss: 3.700902e-05, l1: 0.99857, l2: 0.03178\n",
            "Loss: 3.698298e-05, l1: 0.99867, l2: 0.03178\n",
            "Loss: 3.694860e-05, l1: 0.99872, l2: 0.03177\n",
            "Loss: 3.697400e-05, l1: 0.99869, l2: 0.03177\n",
            "Loss: 3.693797e-05, l1: 0.99871, l2: 0.03177\n",
            "Loss: 3.691877e-05, l1: 0.99867, l2: 0.03176\n",
            "Loss: 3.690411e-05, l1: 0.99857, l2: 0.03176\n",
            "Loss: 3.688537e-05, l1: 0.99845, l2: 0.03175\n",
            "Loss: 3.686534e-05, l1: 0.99837, l2: 0.03175\n",
            "Loss: 3.683790e-05, l1: 0.99828, l2: 0.03175\n",
            "Loss: 3.681958e-05, l1: 0.99827, l2: 0.03175\n",
            "Loss: 3.680607e-05, l1: 0.99826, l2: 0.03175\n",
            "Loss: 3.679783e-05, l1: 0.99830, l2: 0.03174\n",
            "Loss: 3.678884e-05, l1: 0.99829, l2: 0.03174\n",
            "Loss: 3.678136e-05, l1: 0.99834, l2: 0.03175\n",
            "Loss: 3.676735e-05, l1: 0.99835, l2: 0.03175\n",
            "Loss: 3.675513e-05, l1: 0.99835, l2: 0.03175\n",
            "Loss: 3.674102e-05, l1: 0.99839, l2: 0.03175\n",
            "Loss: 3.672746e-05, l1: 0.99839, l2: 0.03175\n",
            "Loss: 3.671696e-05, l1: 0.99845, l2: 0.03175\n",
            "Loss: 3.670857e-05, l1: 0.99846, l2: 0.03175\n",
            "Loss: 3.669333e-05, l1: 0.99845, l2: 0.03175\n",
            "Loss: 3.667880e-05, l1: 0.99840, l2: 0.03175\n",
            "Loss: 3.668115e-05, l1: 0.99839, l2: 0.03174\n",
            "Loss: 3.666763e-05, l1: 0.99839, l2: 0.03174\n",
            "Loss: 3.665114e-05, l1: 0.99830, l2: 0.03174\n",
            "Loss: 3.663695e-05, l1: 0.99824, l2: 0.03174\n",
            "Loss: 3.662856e-05, l1: 0.99824, l2: 0.03173\n",
            "Loss: 3.662044e-05, l1: 0.99823, l2: 0.03173\n",
            "Loss: 3.661254e-05, l1: 0.99825, l2: 0.03173\n",
            "Loss: 3.659943e-05, l1: 0.99829, l2: 0.03172\n",
            "Loss: 3.660494e-05, l1: 0.99832, l2: 0.03171\n",
            "Loss: 3.659187e-05, l1: 0.99830, l2: 0.03172\n",
            "Loss: 3.658181e-05, l1: 0.99830, l2: 0.03172\n",
            "Loss: 3.656432e-05, l1: 0.99829, l2: 0.03171\n",
            "Loss: 3.655323e-05, l1: 0.99830, l2: 0.03171\n",
            "Loss: 3.654122e-05, l1: 0.99834, l2: 0.03171\n",
            "Loss: 3.653468e-05, l1: 0.99836, l2: 0.03171\n",
            "Loss: 3.652561e-05, l1: 0.99837, l2: 0.03171\n",
            "Loss: 3.651197e-05, l1: 0.99839, l2: 0.03171\n",
            "Loss: 3.649171e-05, l1: 0.99840, l2: 0.03171\n",
            "Loss: 3.646157e-05, l1: 0.99837, l2: 0.03171\n",
            "Loss: 3.643413e-05, l1: 0.99839, l2: 0.03171\n",
            "Loss: 3.641859e-05, l1: 0.99823, l2: 0.03172\n",
            "Loss: 3.639992e-05, l1: 0.99832, l2: 0.03172\n",
            "Loss: 3.639155e-05, l1: 0.99837, l2: 0.03172\n",
            "Loss: 3.638152e-05, l1: 0.99841, l2: 0.03172\n",
            "Loss: 3.636580e-05, l1: 0.99847, l2: 0.03173\n",
            "Loss: 3.635112e-05, l1: 0.99859, l2: 0.03174\n",
            "Loss: 3.633649e-05, l1: 0.99863, l2: 0.03174\n",
            "Loss: 3.631989e-05, l1: 0.99864, l2: 0.03174\n",
            "Loss: 3.630616e-05, l1: 0.99866, l2: 0.03173\n",
            "Loss: 3.629330e-05, l1: 0.99856, l2: 0.03172\n",
            "Loss: 3.627758e-05, l1: 0.99878, l2: 0.03171\n",
            "Loss: 3.626174e-05, l1: 0.99873, l2: 0.03171\n",
            "Loss: 3.624574e-05, l1: 0.99870, l2: 0.03172\n",
            "Loss: 3.623103e-05, l1: 0.99872, l2: 0.03172\n",
            "Loss: 3.620575e-05, l1: 0.99880, l2: 0.03172\n",
            "Loss: 3.637342e-05, l1: 0.99909, l2: 0.03171\n",
            "Loss: 3.619829e-05, l1: 0.99885, l2: 0.03172\n",
            "Loss: 3.617849e-05, l1: 0.99894, l2: 0.03172\n",
            "Loss: 3.616379e-05, l1: 0.99902, l2: 0.03172\n",
            "Loss: 3.616598e-05, l1: 0.99897, l2: 0.03171\n",
            "Loss: 3.615456e-05, l1: 0.99899, l2: 0.03172\n",
            "Loss: 3.614151e-05, l1: 0.99899, l2: 0.03172\n",
            "Loss: 3.612124e-05, l1: 0.99896, l2: 0.03172\n",
            "Loss: 3.610577e-05, l1: 0.99894, l2: 0.03172\n",
            "Loss: 3.609126e-05, l1: 0.99895, l2: 0.03172\n",
            "Loss: 3.607884e-05, l1: 0.99903, l2: 0.03173\n",
            "Loss: 3.606682e-05, l1: 0.99908, l2: 0.03173\n",
            "Loss: 3.604779e-05, l1: 0.99920, l2: 0.03173\n",
            "Loss: 3.603791e-05, l1: 0.99928, l2: 0.03173\n",
            "Loss: 3.601140e-05, l1: 0.99944, l2: 0.03174\n",
            "Loss: 3.601133e-05, l1: 0.99955, l2: 0.03174\n",
            "Loss: 3.600360e-05, l1: 0.99950, l2: 0.03174\n",
            "Loss: 3.599223e-05, l1: 0.99950, l2: 0.03174\n",
            "Loss: 3.598367e-05, l1: 0.99947, l2: 0.03173\n",
            "Loss: 3.597140e-05, l1: 0.99946, l2: 0.03173\n",
            "Loss: 3.595245e-05, l1: 0.99949, l2: 0.03173\n",
            "Loss: 3.593359e-05, l1: 0.99959, l2: 0.03174\n",
            "Loss: 3.591750e-05, l1: 0.99966, l2: 0.03174\n",
            "Loss: 3.590808e-05, l1: 0.99979, l2: 0.03175\n",
            "Loss: 3.589996e-05, l1: 0.99982, l2: 0.03175\n",
            "Loss: 3.588961e-05, l1: 0.99982, l2: 0.03175\n",
            "Loss: 3.587643e-05, l1: 0.99983, l2: 0.03175\n",
            "Loss: 3.586380e-05, l1: 0.99983, l2: 0.03175\n",
            "Loss: 3.586376e-05, l1: 0.99986, l2: 0.03176\n",
            "Loss: 3.586016e-05, l1: 0.99984, l2: 0.03176\n",
            "Loss: 3.585466e-05, l1: 0.99983, l2: 0.03176\n",
            "Loss: 3.585111e-05, l1: 0.99983, l2: 0.03176\n",
            "Loss: 3.584534e-05, l1: 0.99983, l2: 0.03176\n",
            "Loss: 3.583889e-05, l1: 0.99980, l2: 0.03177\n",
            "Loss: 3.583232e-05, l1: 0.99983, l2: 0.03177\n",
            "Loss: 3.582528e-05, l1: 0.99982, l2: 0.03177\n",
            "Loss: 3.581699e-05, l1: 0.99983, l2: 0.03177\n",
            "Loss: 3.580482e-05, l1: 0.99982, l2: 0.03177\n",
            "Loss: 3.579028e-05, l1: 0.99980, l2: 0.03178\n",
            "Loss: 3.577643e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.576923e-05, l1: 0.99975, l2: 0.03178\n",
            "Loss: 3.575631e-05, l1: 0.99971, l2: 0.03178\n",
            "Loss: 3.574720e-05, l1: 0.99969, l2: 0.03178\n",
            "Loss: 3.573932e-05, l1: 0.99968, l2: 0.03178\n",
            "Loss: 3.572569e-05, l1: 0.99963, l2: 0.03178\n",
            "Loss: 3.571321e-05, l1: 0.99948, l2: 0.03178\n",
            "Loss: 3.571110e-05, l1: 0.99943, l2: 0.03178\n",
            "Loss: 3.569757e-05, l1: 0.99942, l2: 0.03178\n",
            "Loss: 3.568974e-05, l1: 0.99938, l2: 0.03178\n",
            "Loss: 3.567801e-05, l1: 0.99929, l2: 0.03179\n",
            "Loss: 3.567010e-05, l1: 0.99920, l2: 0.03179\n",
            "Loss: 3.574929e-05, l1: 0.99938, l2: 0.03181\n",
            "Loss: 3.566737e-05, l1: 0.99923, l2: 0.03179\n",
            "Loss: 3.565992e-05, l1: 0.99914, l2: 0.03179\n",
            "Loss: 3.565373e-05, l1: 0.99910, l2: 0.03179\n",
            "Loss: 3.564589e-05, l1: 0.99906, l2: 0.03178\n",
            "Loss: 3.563560e-05, l1: 0.99902, l2: 0.03178\n",
            "Loss: 3.563358e-05, l1: 0.99893, l2: 0.03177\n",
            "Loss: 3.561754e-05, l1: 0.99893, l2: 0.03177\n",
            "Loss: 3.561317e-05, l1: 0.99894, l2: 0.03177\n",
            "Loss: 3.560637e-05, l1: 0.99894, l2: 0.03178\n",
            "Loss: 3.559865e-05, l1: 0.99893, l2: 0.03178\n",
            "Loss: 3.575732e-05, l1: 0.99856, l2: 0.03175\n",
            "Loss: 3.559458e-05, l1: 0.99888, l2: 0.03177\n",
            "Loss: 3.557929e-05, l1: 0.99884, l2: 0.03178\n",
            "Loss: 3.555674e-05, l1: 0.99876, l2: 0.03178\n",
            "Loss: 3.553463e-05, l1: 0.99861, l2: 0.03177\n",
            "Loss: 3.552833e-05, l1: 0.99860, l2: 0.03177\n",
            "Loss: 3.550965e-05, l1: 0.99852, l2: 0.03176\n",
            "Loss: 3.549698e-05, l1: 0.99848, l2: 0.03176\n",
            "Loss: 3.548245e-05, l1: 0.99844, l2: 0.03176\n",
            "Loss: 3.546909e-05, l1: 0.99838, l2: 0.03176\n",
            "Loss: 3.544921e-05, l1: 0.99832, l2: 0.03176\n",
            "Loss: 3.545117e-05, l1: 0.99816, l2: 0.03176\n",
            "Loss: 3.543923e-05, l1: 0.99824, l2: 0.03176\n",
            "Loss: 3.542811e-05, l1: 0.99821, l2: 0.03176\n",
            "Loss: 3.542162e-05, l1: 0.99820, l2: 0.03176\n",
            "Loss: 3.541084e-05, l1: 0.99817, l2: 0.03176\n",
            "Loss: 3.539893e-05, l1: 0.99812, l2: 0.03176\n",
            "Loss: 3.538996e-05, l1: 0.99809, l2: 0.03177\n",
            "Loss: 3.538299e-05, l1: 0.99806, l2: 0.03177\n",
            "Loss: 3.537181e-05, l1: 0.99799, l2: 0.03177\n",
            "Loss: 3.535341e-05, l1: 0.99785, l2: 0.03177\n",
            "Loss: 3.533798e-05, l1: 0.99782, l2: 0.03178\n",
            "Loss: 3.531532e-05, l1: 0.99772, l2: 0.03178\n",
            "Loss: 3.529968e-05, l1: 0.99772, l2: 0.03178\n",
            "Loss: 3.528888e-05, l1: 0.99775, l2: 0.03178\n",
            "Loss: 3.526696e-05, l1: 0.99778, l2: 0.03178\n",
            "Loss: 3.531552e-05, l1: 0.99779, l2: 0.03176\n",
            "Loss: 3.525692e-05, l1: 0.99778, l2: 0.03177\n",
            "Loss: 3.524568e-05, l1: 0.99778, l2: 0.03178\n",
            "Loss: 3.523190e-05, l1: 0.99774, l2: 0.03178\n",
            "Loss: 3.521536e-05, l1: 0.99768, l2: 0.03178\n",
            "Loss: 3.519681e-05, l1: 0.99762, l2: 0.03179\n",
            "Loss: 3.516881e-05, l1: 0.99753, l2: 0.03180\n",
            "Loss: 3.514959e-05, l1: 0.99749, l2: 0.03180\n",
            "Loss: 3.514148e-05, l1: 0.99754, l2: 0.03180\n",
            "Loss: 3.513086e-05, l1: 0.99764, l2: 0.03179\n",
            "Loss: 3.512013e-05, l1: 0.99768, l2: 0.03179\n",
            "Loss: 3.510510e-05, l1: 0.99767, l2: 0.03178\n",
            "Loss: 3.508013e-05, l1: 0.99764, l2: 0.03178\n",
            "Loss: 3.506761e-05, l1: 0.99764, l2: 0.03178\n",
            "Loss: 3.505417e-05, l1: 0.99757, l2: 0.03178\n",
            "Loss: 3.504324e-05, l1: 0.99765, l2: 0.03178\n",
            "Loss: 3.503168e-05, l1: 0.99778, l2: 0.03178\n",
            "Loss: 3.502029e-05, l1: 0.99790, l2: 0.03179\n",
            "Loss: 3.500465e-05, l1: 0.99811, l2: 0.03180\n",
            "Loss: 3.505270e-05, l1: 0.99827, l2: 0.03180\n",
            "Loss: 3.499613e-05, l1: 0.99815, l2: 0.03180\n",
            "Loss: 3.498467e-05, l1: 0.99816, l2: 0.03180\n",
            "Loss: 3.495558e-05, l1: 0.99817, l2: 0.03181\n",
            "Loss: 3.493317e-05, l1: 0.99823, l2: 0.03182\n",
            "Loss: 3.489227e-05, l1: 0.99853, l2: 0.03183\n",
            "Loss: 3.486876e-05, l1: 0.99870, l2: 0.03183\n",
            "Loss: 3.485173e-05, l1: 0.99884, l2: 0.03182\n",
            "Loss: 3.483367e-05, l1: 0.99901, l2: 0.03182\n",
            "Loss: 3.481209e-05, l1: 0.99915, l2: 0.03182\n",
            "Loss: 3.479284e-05, l1: 0.99931, l2: 0.03182\n",
            "Loss: 3.478600e-05, l1: 0.99928, l2: 0.03182\n",
            "Loss: 3.478189e-05, l1: 0.99925, l2: 0.03182\n",
            "Loss: 3.477803e-05, l1: 0.99920, l2: 0.03182\n",
            "Loss: 3.477342e-05, l1: 0.99917, l2: 0.03182\n",
            "Loss: 3.476636e-05, l1: 0.99914, l2: 0.03182\n",
            "Loss: 3.481072e-05, l1: 0.99925, l2: 0.03183\n",
            "Loss: 3.476378e-05, l1: 0.99916, l2: 0.03182\n",
            "Loss: 3.475665e-05, l1: 0.99916, l2: 0.03182\n",
            "Loss: 3.474932e-05, l1: 0.99917, l2: 0.03182\n",
            "Loss: 3.473962e-05, l1: 0.99919, l2: 0.03182\n",
            "Loss: 3.472870e-05, l1: 0.99920, l2: 0.03182\n",
            "Loss: 3.471280e-05, l1: 0.99923, l2: 0.03182\n",
            "Loss: 3.469938e-05, l1: 0.99928, l2: 0.03182\n",
            "Loss: 3.468777e-05, l1: 0.99933, l2: 0.03182\n",
            "Loss: 3.467513e-05, l1: 0.99944, l2: 0.03182\n",
            "Loss: 3.467115e-05, l1: 0.99948, l2: 0.03183\n",
            "Loss: 3.465721e-05, l1: 0.99949, l2: 0.03183\n",
            "Loss: 3.465108e-05, l1: 0.99946, l2: 0.03183\n",
            "Loss: 3.464473e-05, l1: 0.99942, l2: 0.03182\n",
            "Loss: 3.463617e-05, l1: 0.99935, l2: 0.03182\n",
            "Loss: 3.463660e-05, l1: 0.99921, l2: 0.03182\n",
            "Loss: 3.463113e-05, l1: 0.99928, l2: 0.03182\n",
            "Loss: 3.461994e-05, l1: 0.99923, l2: 0.03182\n",
            "Loss: 3.461021e-05, l1: 0.99921, l2: 0.03182\n",
            "Loss: 3.460017e-05, l1: 0.99922, l2: 0.03182\n",
            "Loss: 3.458632e-05, l1: 0.99926, l2: 0.03181\n",
            "Loss: 3.459673e-05, l1: 0.99934, l2: 0.03181\n",
            "Loss: 3.457690e-05, l1: 0.99929, l2: 0.03181\n",
            "Loss: 3.455926e-05, l1: 0.99935, l2: 0.03180\n",
            "Loss: 3.454228e-05, l1: 0.99942, l2: 0.03180\n",
            "Loss: 3.453219e-05, l1: 0.99944, l2: 0.03179\n",
            "Loss: 3.450980e-05, l1: 0.99946, l2: 0.03179\n",
            "Loss: 3.449984e-05, l1: 0.99946, l2: 0.03179\n",
            "Loss: 3.448997e-05, l1: 0.99945, l2: 0.03179\n",
            "Loss: 3.448020e-05, l1: 0.99953, l2: 0.03179\n",
            "Loss: 3.447191e-05, l1: 0.99952, l2: 0.03179\n",
            "Loss: 3.445958e-05, l1: 0.99954, l2: 0.03179\n",
            "Loss: 3.444988e-05, l1: 0.99958, l2: 0.03179\n",
            "Loss: 3.443601e-05, l1: 0.99967, l2: 0.03178\n",
            "Loss: 3.443201e-05, l1: 0.99966, l2: 0.03179\n",
            "Loss: 3.440707e-05, l1: 0.99976, l2: 0.03179\n",
            "Loss: 3.444252e-05, l1: 0.99985, l2: 0.03179\n",
            "Loss: 3.439805e-05, l1: 0.99979, l2: 0.03179\n",
            "Loss: 3.438624e-05, l1: 0.99977, l2: 0.03179\n",
            "Loss: 3.437323e-05, l1: 0.99973, l2: 0.03179\n",
            "Loss: 3.436770e-05, l1: 0.99970, l2: 0.03179\n",
            "Loss: 3.436115e-05, l1: 0.99971, l2: 0.03178\n",
            "Loss: 3.435315e-05, l1: 0.99971, l2: 0.03178\n",
            "Loss: 3.434588e-05, l1: 0.99968, l2: 0.03178\n",
            "Loss: 3.433485e-05, l1: 0.99958, l2: 0.03178\n",
            "Loss: 3.452666e-05, l1: 0.99936, l2: 0.03179\n",
            "Loss: 3.433309e-05, l1: 0.99957, l2: 0.03178\n",
            "Loss: 3.432839e-05, l1: 0.99952, l2: 0.03178\n",
            "Loss: 3.431588e-05, l1: 0.99951, l2: 0.03178\n",
            "Loss: 3.430435e-05, l1: 0.99953, l2: 0.03178\n",
            "Loss: 3.429675e-05, l1: 0.99958, l2: 0.03178\n",
            "Loss: 3.429267e-05, l1: 0.99960, l2: 0.03178\n",
            "Loss: 3.428633e-05, l1: 0.99963, l2: 0.03178\n",
            "Loss: 3.428351e-05, l1: 0.99956, l2: 0.03178\n",
            "Loss: 3.427307e-05, l1: 0.99963, l2: 0.03178\n",
            "Loss: 3.426921e-05, l1: 0.99966, l2: 0.03178\n",
            "Loss: 3.426241e-05, l1: 0.99970, l2: 0.03177\n",
            "Loss: 3.424824e-05, l1: 0.99975, l2: 0.03177\n",
            "Loss: 3.425944e-05, l1: 0.99983, l2: 0.03177\n",
            "Loss: 3.423812e-05, l1: 0.99978, l2: 0.03177\n",
            "Loss: 3.422092e-05, l1: 0.99978, l2: 0.03177\n",
            "Loss: 3.420310e-05, l1: 0.99975, l2: 0.03177\n",
            "Loss: 3.418746e-05, l1: 0.99971, l2: 0.03178\n",
            "Loss: 3.416291e-05, l1: 0.99972, l2: 0.03178\n",
            "Loss: 3.414286e-05, l1: 0.99967, l2: 0.03179\n",
            "Loss: 3.413087e-05, l1: 0.99974, l2: 0.03179\n",
            "Loss: 3.412295e-05, l1: 0.99976, l2: 0.03180\n",
            "Loss: 3.411149e-05, l1: 0.99974, l2: 0.03180\n",
            "Loss: 3.411644e-05, l1: 0.99968, l2: 0.03180\n",
            "Loss: 3.410555e-05, l1: 0.99971, l2: 0.03180\n",
            "Loss: 3.409562e-05, l1: 0.99967, l2: 0.03180\n",
            "Loss: 3.407876e-05, l1: 0.99956, l2: 0.03180\n",
            "Loss: 3.406412e-05, l1: 0.99948, l2: 0.03180\n",
            "Loss: 3.412455e-05, l1: 0.99928, l2: 0.03181\n",
            "Loss: 3.405955e-05, l1: 0.99944, l2: 0.03180\n",
            "Loss: 3.404240e-05, l1: 0.99936, l2: 0.03180\n",
            "Loss: 3.402338e-05, l1: 0.99934, l2: 0.03180\n",
            "Loss: 3.413858e-05, l1: 0.99920, l2: 0.03181\n",
            "Loss: 3.401541e-05, l1: 0.99931, l2: 0.03180\n",
            "Loss: 3.400096e-05, l1: 0.99934, l2: 0.03180\n",
            "Loss: 3.399248e-05, l1: 0.99941, l2: 0.03179\n",
            "Loss: 3.397806e-05, l1: 0.99946, l2: 0.03179\n",
            "Loss: 3.396114e-05, l1: 0.99947, l2: 0.03179\n",
            "Loss: 3.394284e-05, l1: 0.99951, l2: 0.03180\n",
            "Loss: 3.393076e-05, l1: 0.99951, l2: 0.03180\n",
            "Loss: 3.392473e-05, l1: 0.99953, l2: 0.03181\n",
            "Loss: 3.391770e-05, l1: 0.99953, l2: 0.03181\n",
            "Loss: 3.391885e-05, l1: 0.99950, l2: 0.03181\n",
            "Loss: 3.391466e-05, l1: 0.99952, l2: 0.03181\n",
            "Loss: 3.390831e-05, l1: 0.99955, l2: 0.03181\n",
            "Loss: 3.390383e-05, l1: 0.99955, l2: 0.03181\n",
            "Loss: 3.390126e-05, l1: 0.99954, l2: 0.03180\n",
            "Loss: 3.389862e-05, l1: 0.99950, l2: 0.03180\n",
            "Loss: 3.389524e-05, l1: 0.99946, l2: 0.03181\n",
            "Loss: 3.389094e-05, l1: 0.99940, l2: 0.03181\n",
            "Loss: 3.388536e-05, l1: 0.99932, l2: 0.03181\n",
            "Loss: 3.387579e-05, l1: 0.99921, l2: 0.03182\n",
            "Loss: 3.386681e-05, l1: 0.99908, l2: 0.03182\n",
            "Loss: 3.385463e-05, l1: 0.99911, l2: 0.03181\n",
            "Loss: 3.384540e-05, l1: 0.99918, l2: 0.03181\n",
            "Loss: 3.383445e-05, l1: 0.99924, l2: 0.03181\n",
            "Loss: 3.382475e-05, l1: 0.99936, l2: 0.03181\n",
            "Loss: 3.384093e-05, l1: 0.99930, l2: 0.03180\n",
            "Loss: 3.382099e-05, l1: 0.99934, l2: 0.03180\n",
            "Loss: 3.381613e-05, l1: 0.99929, l2: 0.03180\n",
            "Loss: 3.381092e-05, l1: 0.99916, l2: 0.03181\n",
            "Loss: 3.380714e-05, l1: 0.99911, l2: 0.03181\n",
            "Loss: 3.380145e-05, l1: 0.99914, l2: 0.03181\n",
            "Loss: 3.379403e-05, l1: 0.99919, l2: 0.03180\n",
            "Loss: 3.378726e-05, l1: 0.99922, l2: 0.03180\n",
            "Loss: 3.378062e-05, l1: 0.99924, l2: 0.03180\n",
            "Loss: 3.378642e-05, l1: 0.99917, l2: 0.03180\n",
            "Loss: 3.377805e-05, l1: 0.99921, l2: 0.03180\n",
            "Loss: 3.377319e-05, l1: 0.99921, l2: 0.03180\n",
            "Loss: 3.376929e-05, l1: 0.99919, l2: 0.03180\n",
            "Loss: 3.376301e-05, l1: 0.99915, l2: 0.03180\n",
            "Loss: 3.375436e-05, l1: 0.99912, l2: 0.03180\n",
            "Loss: 3.374199e-05, l1: 0.99896, l2: 0.03181\n",
            "Loss: 3.372262e-05, l1: 0.99896, l2: 0.03181\n",
            "Loss: 3.371196e-05, l1: 0.99896, l2: 0.03181\n",
            "Loss: 3.370526e-05, l1: 0.99895, l2: 0.03180\n",
            "Loss: 3.369995e-05, l1: 0.99893, l2: 0.03180\n",
            "Loss: 3.368716e-05, l1: 0.99888, l2: 0.03180\n",
            "Loss: 3.367733e-05, l1: 0.99885, l2: 0.03180\n",
            "Loss: 3.366377e-05, l1: 0.99883, l2: 0.03180\n",
            "Loss: 3.365689e-05, l1: 0.99881, l2: 0.03180\n",
            "Loss: 3.365030e-05, l1: 0.99882, l2: 0.03180\n",
            "Loss: 3.364440e-05, l1: 0.99884, l2: 0.03180\n",
            "Loss: 3.363980e-05, l1: 0.99886, l2: 0.03180\n",
            "Loss: 3.363547e-05, l1: 0.99889, l2: 0.03180\n",
            "Loss: 3.378909e-05, l1: 0.99885, l2: 0.03181\n",
            "Loss: 3.363316e-05, l1: 0.99888, l2: 0.03180\n",
            "Loss: 3.362757e-05, l1: 0.99889, l2: 0.03180\n",
            "Loss: 3.361996e-05, l1: 0.99887, l2: 0.03180\n",
            "Loss: 3.361416e-05, l1: 0.99885, l2: 0.03180\n",
            "Loss: 3.360678e-05, l1: 0.99881, l2: 0.03180\n",
            "Loss: 3.359836e-05, l1: 0.99876, l2: 0.03180\n",
            "Loss: 3.359007e-05, l1: 0.99873, l2: 0.03180\n",
            "Loss: 3.358214e-05, l1: 0.99872, l2: 0.03180\n",
            "Loss: 3.357411e-05, l1: 0.99874, l2: 0.03180\n",
            "Loss: 3.356945e-05, l1: 0.99878, l2: 0.03180\n",
            "Loss: 3.356633e-05, l1: 0.99878, l2: 0.03180\n",
            "Loss: 3.356440e-05, l1: 0.99877, l2: 0.03180\n",
            "Loss: 3.356188e-05, l1: 0.99875, l2: 0.03180\n",
            "Loss: 3.356075e-05, l1: 0.99868, l2: 0.03181\n",
            "Loss: 3.355363e-05, l1: 0.99869, l2: 0.03181\n",
            "Loss: 3.355032e-05, l1: 0.99872, l2: 0.03180\n",
            "Loss: 3.354766e-05, l1: 0.99870, l2: 0.03180\n",
            "Loss: 3.354509e-05, l1: 0.99870, l2: 0.03180\n",
            "Loss: 3.354069e-05, l1: 0.99869, l2: 0.03180\n",
            "Loss: 3.353629e-05, l1: 0.99869, l2: 0.03180\n",
            "Loss: 3.353026e-05, l1: 0.99870, l2: 0.03180\n",
            "Loss: 3.352892e-05, l1: 0.99877, l2: 0.03179\n",
            "Loss: 3.352169e-05, l1: 0.99874, l2: 0.03179\n",
            "Loss: 3.351905e-05, l1: 0.99873, l2: 0.03179\n",
            "Loss: 3.351609e-05, l1: 0.99873, l2: 0.03179\n",
            "Loss: 3.351264e-05, l1: 0.99874, l2: 0.03179\n",
            "Loss: 3.350713e-05, l1: 0.99873, l2: 0.03179\n",
            "Loss: 3.350097e-05, l1: 0.99877, l2: 0.03180\n",
            "Loss: 3.349439e-05, l1: 0.99876, l2: 0.03180\n",
            "Loss: 3.348748e-05, l1: 0.99875, l2: 0.03180\n",
            "Loss: 3.348648e-05, l1: 0.99874, l2: 0.03180\n",
            "Loss: 3.348054e-05, l1: 0.99876, l2: 0.03180\n",
            "Loss: 3.347478e-05, l1: 0.99879, l2: 0.03180\n",
            "Loss: 3.346895e-05, l1: 0.99884, l2: 0.03180\n",
            "Loss: 3.346438e-05, l1: 0.99888, l2: 0.03180\n",
            "Loss: 3.346060e-05, l1: 0.99891, l2: 0.03180\n",
            "Loss: 3.345633e-05, l1: 0.99895, l2: 0.03180\n",
            "Loss: 3.345228e-05, l1: 0.99896, l2: 0.03180\n",
            "Loss: 3.344897e-05, l1: 0.99898, l2: 0.03180\n",
            "Loss: 3.344117e-05, l1: 0.99897, l2: 0.03180\n",
            "Loss: 3.343387e-05, l1: 0.99903, l2: 0.03180\n",
            "Loss: 3.342429e-05, l1: 0.99899, l2: 0.03181\n",
            "Loss: 3.341875e-05, l1: 0.99898, l2: 0.03181\n",
            "Loss: 3.341437e-05, l1: 0.99900, l2: 0.03180\n",
            "Loss: 3.340745e-05, l1: 0.99902, l2: 0.03180\n",
            "Loss: 3.339461e-05, l1: 0.99904, l2: 0.03180\n",
            "Loss: 3.338240e-05, l1: 0.99908, l2: 0.03180\n",
            "Loss: 3.337068e-05, l1: 0.99907, l2: 0.03181\n",
            "Loss: 3.336305e-05, l1: 0.99904, l2: 0.03181\n",
            "Loss: 3.335708e-05, l1: 0.99901, l2: 0.03181\n",
            "Loss: 3.335298e-05, l1: 0.99901, l2: 0.03181\n",
            "Loss: 3.334902e-05, l1: 0.99904, l2: 0.03181\n",
            "Loss: 3.334537e-05, l1: 0.99907, l2: 0.03180\n",
            "Loss: 3.334070e-05, l1: 0.99915, l2: 0.03180\n",
            "Loss: 3.333794e-05, l1: 0.99921, l2: 0.03180\n",
            "Loss: 3.333003e-05, l1: 0.99918, l2: 0.03180\n",
            "Loss: 3.332589e-05, l1: 0.99916, l2: 0.03180\n",
            "Loss: 3.332140e-05, l1: 0.99912, l2: 0.03181\n",
            "Loss: 3.331525e-05, l1: 0.99909, l2: 0.03181\n",
            "Loss: 3.330532e-05, l1: 0.99907, l2: 0.03181\n",
            "Loss: 3.329754e-05, l1: 0.99905, l2: 0.03181\n",
            "Loss: 3.329032e-05, l1: 0.99906, l2: 0.03181\n",
            "Loss: 3.327787e-05, l1: 0.99913, l2: 0.03181\n",
            "Loss: 3.328361e-05, l1: 0.99935, l2: 0.03182\n",
            "Loss: 3.327015e-05, l1: 0.99923, l2: 0.03181\n",
            "Loss: 3.326418e-05, l1: 0.99925, l2: 0.03181\n",
            "Loss: 3.325758e-05, l1: 0.99928, l2: 0.03181\n",
            "Loss: 3.325292e-05, l1: 0.99932, l2: 0.03181\n",
            "Loss: 3.324978e-05, l1: 0.99934, l2: 0.03181\n",
            "Loss: 3.324727e-05, l1: 0.99935, l2: 0.03181\n",
            "Loss: 3.324528e-05, l1: 0.99937, l2: 0.03181\n",
            "Loss: 3.324190e-05, l1: 0.99937, l2: 0.03181\n",
            "Loss: 3.323631e-05, l1: 0.99936, l2: 0.03182\n",
            "Loss: 3.323072e-05, l1: 0.99934, l2: 0.03182\n",
            "Loss: 3.323024e-05, l1: 0.99935, l2: 0.03182\n",
            "Loss: 3.322515e-05, l1: 0.99935, l2: 0.03182\n",
            "Loss: 3.321730e-05, l1: 0.99934, l2: 0.03182\n",
            "Loss: 3.321069e-05, l1: 0.99932, l2: 0.03182\n",
            "Loss: 3.320745e-05, l1: 0.99934, l2: 0.03182\n",
            "Loss: 3.320310e-05, l1: 0.99935, l2: 0.03182\n",
            "Loss: 3.319468e-05, l1: 0.99936, l2: 0.03181\n",
            "Loss: 3.319086e-05, l1: 0.99943, l2: 0.03181\n",
            "Loss: 3.318562e-05, l1: 0.99938, l2: 0.03181\n",
            "Loss: 3.318273e-05, l1: 0.99935, l2: 0.03181\n",
            "Loss: 3.317737e-05, l1: 0.99933, l2: 0.03181\n",
            "Loss: 3.316925e-05, l1: 0.99926, l2: 0.03181\n",
            "Loss: 3.329146e-05, l1: 0.99955, l2: 0.03180\n",
            "Loss: 3.316534e-05, l1: 0.99930, l2: 0.03181\n",
            "Loss: 3.316047e-05, l1: 0.99929, l2: 0.03180\n",
            "Loss: 3.315337e-05, l1: 0.99928, l2: 0.03180\n",
            "Loss: 3.314905e-05, l1: 0.99928, l2: 0.03180\n",
            "Loss: 3.314367e-05, l1: 0.99922, l2: 0.03181\n",
            "Loss: 3.313725e-05, l1: 0.99918, l2: 0.03181\n",
            "Loss: 3.313070e-05, l1: 0.99912, l2: 0.03181\n",
            "Loss: 3.312513e-05, l1: 0.99909, l2: 0.03181\n",
            "Loss: 3.311125e-05, l1: 0.99901, l2: 0.03181\n",
            "Loss: 3.310257e-05, l1: 0.99906, l2: 0.03181\n",
            "Loss: 3.309395e-05, l1: 0.99897, l2: 0.03181\n",
            "Loss: 3.308760e-05, l1: 0.99900, l2: 0.03181\n",
            "Loss: 3.308348e-05, l1: 0.99903, l2: 0.03181\n",
            "Loss: 3.307872e-05, l1: 0.99903, l2: 0.03181\n",
            "Loss: 3.306648e-05, l1: 0.99904, l2: 0.03180\n",
            "Loss: 3.305735e-05, l1: 0.99897, l2: 0.03181\n",
            "Loss: 3.305021e-05, l1: 0.99897, l2: 0.03181\n",
            "Loss: 3.304643e-05, l1: 0.99896, l2: 0.03181\n",
            "Loss: 3.304610e-05, l1: 0.99896, l2: 0.03181\n",
            "Loss: 3.304186e-05, l1: 0.99896, l2: 0.03181\n",
            "Loss: 3.303871e-05, l1: 0.99897, l2: 0.03181\n",
            "Loss: 3.303610e-05, l1: 0.99898, l2: 0.03181\n",
            "Loss: 3.303329e-05, l1: 0.99899, l2: 0.03181\n",
            "Loss: 3.302616e-05, l1: 0.99900, l2: 0.03181\n",
            "Loss: 3.301961e-05, l1: 0.99900, l2: 0.03181\n",
            "Loss: 3.301120e-05, l1: 0.99899, l2: 0.03182\n",
            "Loss: 3.299987e-05, l1: 0.99900, l2: 0.03182\n",
            "Loss: 3.344415e-05, l1: 0.99911, l2: 0.03184\n",
            "Loss: 3.299711e-05, l1: 0.99900, l2: 0.03182\n",
            "Loss: 3.298771e-05, l1: 0.99901, l2: 0.03182\n",
            "Loss: 3.297747e-05, l1: 0.99901, l2: 0.03182\n",
            "Loss: 3.297257e-05, l1: 0.99902, l2: 0.03182\n",
            "Loss: 3.296719e-05, l1: 0.99903, l2: 0.03182\n",
            "Loss: 3.296075e-05, l1: 0.99905, l2: 0.03182\n",
            "Loss: 3.295334e-05, l1: 0.99908, l2: 0.03182\n",
            "Loss: 3.294785e-05, l1: 0.99913, l2: 0.03183\n",
            "Loss: 3.294337e-05, l1: 0.99913, l2: 0.03183\n",
            "Loss: 3.293914e-05, l1: 0.99908, l2: 0.03182\n",
            "Loss: 3.294232e-05, l1: 0.99920, l2: 0.03183\n",
            "Loss: 3.293641e-05, l1: 0.99913, l2: 0.03183\n",
            "Loss: 3.293042e-05, l1: 0.99910, l2: 0.03183\n",
            "Loss: 3.291552e-05, l1: 0.99905, l2: 0.03183\n",
            "Loss: 3.290445e-05, l1: 0.99902, l2: 0.03183\n",
            "Loss: 3.288716e-05, l1: 0.99899, l2: 0.03183\n",
            "Loss: 3.288987e-05, l1: 0.99895, l2: 0.03183\n",
            "Loss: 3.288094e-05, l1: 0.99897, l2: 0.03183\n",
            "Loss: 3.287063e-05, l1: 0.99897, l2: 0.03183\n",
            "Loss: 3.286334e-05, l1: 0.99898, l2: 0.03183\n",
            "Loss: 3.285297e-05, l1: 0.99897, l2: 0.03183\n",
            "Loss: 3.284010e-05, l1: 0.99894, l2: 0.03183\n",
            "Loss: 3.283064e-05, l1: 0.99892, l2: 0.03183\n",
            "Loss: 3.283810e-05, l1: 0.99881, l2: 0.03183\n",
            "Loss: 3.282609e-05, l1: 0.99888, l2: 0.03183\n",
            "Loss: 3.281602e-05, l1: 0.99889, l2: 0.03183\n",
            "Loss: 3.280988e-05, l1: 0.99890, l2: 0.03183\n",
            "Loss: 3.279808e-05, l1: 0.99893, l2: 0.03183\n",
            "Loss: 3.278744e-05, l1: 0.99893, l2: 0.03184\n",
            "Loss: 3.278063e-05, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.278226e-05, l1: 0.99892, l2: 0.03185\n",
            "Loss: 3.277384e-05, l1: 0.99898, l2: 0.03184\n",
            "Loss: 3.276929e-05, l1: 0.99896, l2: 0.03184\n",
            "Loss: 3.276595e-05, l1: 0.99899, l2: 0.03184\n",
            "Loss: 3.276108e-05, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.275882e-05, l1: 0.99908, l2: 0.03184\n",
            "Loss: 3.275170e-05, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.274677e-05, l1: 0.99920, l2: 0.03184\n",
            "Loss: 3.274226e-05, l1: 0.99919, l2: 0.03184\n",
            "Loss: 3.274724e-05, l1: 0.99919, l2: 0.03183\n",
            "Loss: 3.273921e-05, l1: 0.99919, l2: 0.03184\n",
            "Loss: 3.273248e-05, l1: 0.99918, l2: 0.03184\n",
            "Loss: 3.272726e-05, l1: 0.99918, l2: 0.03183\n",
            "Loss: 3.271759e-05, l1: 0.99924, l2: 0.03183\n",
            "Loss: 3.271048e-05, l1: 0.99927, l2: 0.03183\n",
            "Loss: 3.271929e-05, l1: 0.99944, l2: 0.03182\n",
            "Loss: 3.270835e-05, l1: 0.99932, l2: 0.03183\n",
            "Loss: 3.270497e-05, l1: 0.99933, l2: 0.03183\n",
            "Loss: 3.270166e-05, l1: 0.99932, l2: 0.03183\n",
            "Loss: 3.269503e-05, l1: 0.99930, l2: 0.03183\n",
            "Loss: 3.269130e-05, l1: 0.99929, l2: 0.03183\n",
            "Loss: 3.268682e-05, l1: 0.99927, l2: 0.03183\n",
            "Loss: 3.268426e-05, l1: 0.99929, l2: 0.03183\n",
            "Loss: 3.267813e-05, l1: 0.99932, l2: 0.03183\n",
            "Loss: 3.267347e-05, l1: 0.99934, l2: 0.03182\n",
            "Loss: 3.266896e-05, l1: 0.99934, l2: 0.03182\n",
            "Loss: 3.266552e-05, l1: 0.99932, l2: 0.03182\n",
            "Loss: 3.266071e-05, l1: 0.99929, l2: 0.03182\n",
            "Loss: 3.274851e-05, l1: 0.99923, l2: 0.03184\n",
            "Loss: 3.266008e-05, l1: 0.99928, l2: 0.03182\n",
            "Loss: 3.265613e-05, l1: 0.99927, l2: 0.03182\n",
            "Loss: 3.265413e-05, l1: 0.99925, l2: 0.03182\n",
            "Loss: 3.265037e-05, l1: 0.99923, l2: 0.03182\n",
            "Loss: 3.264543e-05, l1: 0.99918, l2: 0.03182\n",
            "Loss: 3.264056e-05, l1: 0.99911, l2: 0.03182\n",
            "Loss: 3.263714e-05, l1: 0.99907, l2: 0.03182\n",
            "Loss: 3.263428e-05, l1: 0.99908, l2: 0.03181\n",
            "Loss: 3.263224e-05, l1: 0.99908, l2: 0.03181\n",
            "Loss: 3.262990e-05, l1: 0.99908, l2: 0.03181\n",
            "Loss: 3.262741e-05, l1: 0.99908, l2: 0.03181\n",
            "Loss: 3.262495e-05, l1: 0.99907, l2: 0.03181\n",
            "Loss: 3.262214e-05, l1: 0.99907, l2: 0.03181\n",
            "Loss: 3.261948e-05, l1: 0.99907, l2: 0.03180\n",
            "Loss: 3.261594e-05, l1: 0.99907, l2: 0.03180\n",
            "Loss: 3.261172e-05, l1: 0.99908, l2: 0.03180\n",
            "Loss: 3.260517e-05, l1: 0.99911, l2: 0.03180\n",
            "Loss: 3.259927e-05, l1: 0.99912, l2: 0.03180\n",
            "Loss: 3.259641e-05, l1: 0.99915, l2: 0.03180\n",
            "Loss: 3.259419e-05, l1: 0.99917, l2: 0.03180\n",
            "Loss: 3.259139e-05, l1: 0.99915, l2: 0.03180\n",
            "Loss: 3.258817e-05, l1: 0.99914, l2: 0.03179\n",
            "Loss: 3.258334e-05, l1: 0.99916, l2: 0.03179\n",
            "Loss: 3.258034e-05, l1: 0.99918, l2: 0.03179\n",
            "Loss: 3.257374e-05, l1: 0.99923, l2: 0.03178\n",
            "Loss: 3.257737e-05, l1: 0.99940, l2: 0.03177\n",
            "Loss: 3.256975e-05, l1: 0.99930, l2: 0.03178\n",
            "Loss: 3.256424e-05, l1: 0.99931, l2: 0.03178\n",
            "Loss: 3.255888e-05, l1: 0.99937, l2: 0.03178\n",
            "Loss: 3.255545e-05, l1: 0.99936, l2: 0.03178\n",
            "Loss: 3.255167e-05, l1: 0.99937, l2: 0.03178\n",
            "Loss: 3.255219e-05, l1: 0.99945, l2: 0.03178\n",
            "Loss: 3.254618e-05, l1: 0.99941, l2: 0.03178\n",
            "Loss: 3.253981e-05, l1: 0.99947, l2: 0.03178\n",
            "Loss: 3.253092e-05, l1: 0.99962, l2: 0.03177\n",
            "Loss: 3.252961e-05, l1: 0.99964, l2: 0.03176\n",
            "Loss: 3.252625e-05, l1: 0.99970, l2: 0.03177\n",
            "Loss: 3.252150e-05, l1: 0.99968, l2: 0.03177\n",
            "Loss: 3.251814e-05, l1: 0.99967, l2: 0.03177\n",
            "Loss: 3.251271e-05, l1: 0.99968, l2: 0.03177\n",
            "Loss: 3.250924e-05, l1: 0.99968, l2: 0.03177\n",
            "Loss: 3.250512e-05, l1: 0.99972, l2: 0.03176\n",
            "Loss: 3.251356e-05, l1: 0.99974, l2: 0.03176\n",
            "Loss: 3.250417e-05, l1: 0.99972, l2: 0.03176\n",
            "Loss: 3.250184e-05, l1: 0.99976, l2: 0.03176\n",
            "Loss: 3.249979e-05, l1: 0.99979, l2: 0.03176\n",
            "Loss: 3.249703e-05, l1: 0.99981, l2: 0.03176\n",
            "Loss: 3.250025e-05, l1: 0.99983, l2: 0.03176\n",
            "Loss: 3.249567e-05, l1: 0.99982, l2: 0.03176\n",
            "Loss: 3.249316e-05, l1: 0.99982, l2: 0.03176\n",
            "Loss: 3.248713e-05, l1: 0.99978, l2: 0.03176\n",
            "Loss: 3.248041e-05, l1: 0.99974, l2: 0.03176\n",
            "Loss: 3.252177e-05, l1: 0.99962, l2: 0.03177\n",
            "Loss: 3.247910e-05, l1: 0.99973, l2: 0.03176\n",
            "Loss: 3.247463e-05, l1: 0.99967, l2: 0.03176\n",
            "Loss: 3.247165e-05, l1: 0.99965, l2: 0.03176\n",
            "Loss: 3.246659e-05, l1: 0.99967, l2: 0.03176\n",
            "Loss: 3.245951e-05, l1: 0.99971, l2: 0.03176\n",
            "Loss: 3.245451e-05, l1: 0.99984, l2: 0.03176\n",
            "Loss: 3.244393e-05, l1: 0.99985, l2: 0.03176\n",
            "Loss: 3.244054e-05, l1: 0.99986, l2: 0.03176\n",
            "Loss: 3.243605e-05, l1: 0.99986, l2: 0.03176\n",
            "Loss: 3.243086e-05, l1: 0.99985, l2: 0.03176\n",
            "Loss: 3.242728e-05, l1: 0.99981, l2: 0.03176\n",
            "Loss: 3.242463e-05, l1: 0.99979, l2: 0.03175\n",
            "Loss: 3.242342e-05, l1: 0.99977, l2: 0.03176\n",
            "Loss: 3.242027e-05, l1: 0.99971, l2: 0.03176\n",
            "Loss: 3.241628e-05, l1: 0.99968, l2: 0.03176\n",
            "Loss: 3.241025e-05, l1: 0.99967, l2: 0.03176\n",
            "Loss: 3.241067e-05, l1: 0.99963, l2: 0.03176\n",
            "Loss: 3.240594e-05, l1: 0.99965, l2: 0.03176\n",
            "Loss: 3.239876e-05, l1: 0.99967, l2: 0.03176\n",
            "Loss: 3.239338e-05, l1: 0.99965, l2: 0.03176\n",
            "Loss: 3.238765e-05, l1: 0.99964, l2: 0.03176\n",
            "Loss: 3.237844e-05, l1: 0.99961, l2: 0.03177\n",
            "Loss: 3.237162e-05, l1: 0.99958, l2: 0.03177\n",
            "Loss: 3.236345e-05, l1: 0.99955, l2: 0.03177\n",
            "Loss: 3.235517e-05, l1: 0.99950, l2: 0.03177\n",
            "Loss: 3.234046e-05, l1: 0.99941, l2: 0.03177\n",
            "Loss: 3.232425e-05, l1: 0.99929, l2: 0.03176\n",
            "Loss: 3.231387e-05, l1: 0.99919, l2: 0.03176\n",
            "Loss: 3.230674e-05, l1: 0.99919, l2: 0.03176\n",
            "Loss: 3.230327e-05, l1: 0.99918, l2: 0.03176\n",
            "Loss: 3.229980e-05, l1: 0.99915, l2: 0.03175\n",
            "Loss: 3.229591e-05, l1: 0.99906, l2: 0.03175\n",
            "Loss: 3.229106e-05, l1: 0.99901, l2: 0.03175\n",
            "Loss: 3.228604e-05, l1: 0.99897, l2: 0.03175\n",
            "Loss: 3.227829e-05, l1: 0.99891, l2: 0.03176\n",
            "Loss: 3.227159e-05, l1: 0.99889, l2: 0.03176\n",
            "Loss: 3.226538e-05, l1: 0.99887, l2: 0.03176\n",
            "Loss: 3.225939e-05, l1: 0.99891, l2: 0.03176\n",
            "Loss: 3.224818e-05, l1: 0.99885, l2: 0.03176\n",
            "Loss: 3.224118e-05, l1: 0.99887, l2: 0.03176\n",
            "Loss: 3.223788e-05, l1: 0.99885, l2: 0.03176\n",
            "Loss: 3.223072e-05, l1: 0.99882, l2: 0.03176\n",
            "Loss: 3.222796e-05, l1: 0.99879, l2: 0.03176\n",
            "Loss: 3.222401e-05, l1: 0.99879, l2: 0.03176\n",
            "Loss: 3.222024e-05, l1: 0.99876, l2: 0.03176\n",
            "Loss: 3.221739e-05, l1: 0.99873, l2: 0.03176\n",
            "Loss: 3.221513e-05, l1: 0.99867, l2: 0.03176\n",
            "Loss: 3.221737e-05, l1: 0.99860, l2: 0.03175\n",
            "Loss: 3.221288e-05, l1: 0.99864, l2: 0.03176\n",
            "Loss: 3.221168e-05, l1: 0.99865, l2: 0.03176\n",
            "Loss: 3.221032e-05, l1: 0.99864, l2: 0.03176\n",
            "Loss: 3.220897e-05, l1: 0.99866, l2: 0.03175\n",
            "Loss: 3.220735e-05, l1: 0.99868, l2: 0.03175\n",
            "Loss: 3.220030e-05, l1: 0.99873, l2: 0.03175\n",
            "Loss: 3.219509e-05, l1: 0.99876, l2: 0.03175\n",
            "Loss: 3.218749e-05, l1: 0.99879, l2: 0.03175\n",
            "Loss: 3.218552e-05, l1: 0.99880, l2: 0.03175\n",
            "Loss: 3.217182e-05, l1: 0.99878, l2: 0.03175\n",
            "Loss: 3.216266e-05, l1: 0.99876, l2: 0.03176\n",
            "Loss: 3.214585e-05, l1: 0.99874, l2: 0.03176\n",
            "Loss: 3.217390e-05, l1: 0.99867, l2: 0.03177\n",
            "Loss: 3.214254e-05, l1: 0.99872, l2: 0.03177\n",
            "Loss: 3.213570e-05, l1: 0.99877, l2: 0.03177\n",
            "Loss: 3.213013e-05, l1: 0.99881, l2: 0.03178\n",
            "Loss: 3.212621e-05, l1: 0.99887, l2: 0.03178\n",
            "Loss: 3.212384e-05, l1: 0.99886, l2: 0.03178\n",
            "Loss: 3.212105e-05, l1: 0.99885, l2: 0.03178\n",
            "Loss: 3.211882e-05, l1: 0.99880, l2: 0.03178\n",
            "Loss: 3.211608e-05, l1: 0.99878, l2: 0.03178\n",
            "Loss: 3.211192e-05, l1: 0.99877, l2: 0.03179\n",
            "Loss: 3.211079e-05, l1: 0.99888, l2: 0.03180\n",
            "Loss: 3.210428e-05, l1: 0.99886, l2: 0.03179\n",
            "Loss: 3.210035e-05, l1: 0.99889, l2: 0.03179\n",
            "Loss: 3.209596e-05, l1: 0.99896, l2: 0.03179\n",
            "Loss: 3.209140e-05, l1: 0.99903, l2: 0.03180\n",
            "Loss: 3.209803e-05, l1: 0.99899, l2: 0.03180\n",
            "Loss: 3.208910e-05, l1: 0.99902, l2: 0.03180\n",
            "Loss: 3.208359e-05, l1: 0.99909, l2: 0.03180\n",
            "Loss: 3.207837e-05, l1: 0.99912, l2: 0.03181\n",
            "Loss: 3.207021e-05, l1: 0.99916, l2: 0.03181\n",
            "Loss: 3.206649e-05, l1: 0.99924, l2: 0.03182\n",
            "Loss: 3.206147e-05, l1: 0.99923, l2: 0.03182\n",
            "Loss: 3.206254e-05, l1: 0.99939, l2: 0.03182\n",
            "Loss: 3.205481e-05, l1: 0.99931, l2: 0.03182\n",
            "Loss: 3.204979e-05, l1: 0.99935, l2: 0.03182\n",
            "Loss: 3.204354e-05, l1: 0.99943, l2: 0.03182\n",
            "Loss: 3.203610e-05, l1: 0.99952, l2: 0.03183\n",
            "Loss: 3.202958e-05, l1: 0.99957, l2: 0.03183\n",
            "Loss: 3.202602e-05, l1: 0.99966, l2: 0.03184\n",
            "Loss: 3.202078e-05, l1: 0.99964, l2: 0.03184\n",
            "Loss: 3.201728e-05, l1: 0.99963, l2: 0.03184\n",
            "Loss: 3.201543e-05, l1: 0.99964, l2: 0.03184\n",
            "Loss: 3.201384e-05, l1: 0.99972, l2: 0.03184\n",
            "Loss: 3.201233e-05, l1: 0.99972, l2: 0.03184\n",
            "Loss: 3.201520e-05, l1: 0.99979, l2: 0.03184\n",
            "Loss: 3.201205e-05, l1: 0.99974, l2: 0.03184\n",
            "Loss: 3.201110e-05, l1: 0.99976, l2: 0.03184\n",
            "Loss: 3.200991e-05, l1: 0.99977, l2: 0.03184\n",
            "Loss: 3.200850e-05, l1: 0.99980, l2: 0.03184\n",
            "Loss: 3.200721e-05, l1: 0.99982, l2: 0.03184\n",
            "Loss: 3.200610e-05, l1: 0.99991, l2: 0.03184\n",
            "Loss: 3.200348e-05, l1: 0.99990, l2: 0.03184\n",
            "Loss: 3.200214e-05, l1: 0.99989, l2: 0.03184\n",
            "Loss: 3.200032e-05, l1: 0.99991, l2: 0.03185\n",
            "Loss: 3.199853e-05, l1: 0.99992, l2: 0.03185\n",
            "Loss: 3.199540e-05, l1: 0.99994, l2: 0.03185\n",
            "Loss: 3.198991e-05, l1: 0.99997, l2: 0.03185\n",
            "Loss: 3.199196e-05, l1: 1.00002, l2: 0.03185\n",
            "Loss: 3.198766e-05, l1: 0.99999, l2: 0.03185\n",
            "Loss: 3.198478e-05, l1: 1.00000, l2: 0.03185\n",
            "Loss: 3.198260e-05, l1: 1.00000, l2: 0.03185\n",
            "Loss: 3.198040e-05, l1: 1.00001, l2: 0.03185\n",
            "Loss: 3.197708e-05, l1: 1.00004, l2: 0.03185\n",
            "Loss: 3.197317e-05, l1: 1.00009, l2: 0.03185\n",
            "Loss: 3.197321e-05, l1: 1.00008, l2: 0.03185\n",
            "Loss: 3.197049e-05, l1: 1.00008, l2: 0.03185\n",
            "Loss: 3.196788e-05, l1: 1.00012, l2: 0.03185\n",
            "Loss: 3.196590e-05, l1: 1.00013, l2: 0.03185\n",
            "Loss: 3.196833e-05, l1: 1.00011, l2: 0.03185\n",
            "Loss: 3.196488e-05, l1: 1.00013, l2: 0.03185\n",
            "Loss: 3.196350e-05, l1: 1.00012, l2: 0.03185\n",
            "Loss: 3.196201e-05, l1: 1.00010, l2: 0.03185\n",
            "Loss: 3.196125e-05, l1: 1.00009, l2: 0.03185\n",
            "Loss: 3.195935e-05, l1: 1.00008, l2: 0.03185\n",
            "Loss: 3.195589e-05, l1: 1.00008, l2: 0.03184\n",
            "Loss: 3.195202e-05, l1: 1.00004, l2: 0.03184\n",
            "Loss: 3.195001e-05, l1: 1.00005, l2: 0.03184\n",
            "Loss: 3.194794e-05, l1: 1.00005, l2: 0.03184\n",
            "Loss: 3.194463e-05, l1: 1.00005, l2: 0.03184\n",
            "Loss: 3.194232e-05, l1: 1.00006, l2: 0.03184\n",
            "Loss: 3.193161e-05, l1: 1.00004, l2: 0.03184\n",
            "Loss: 3.192641e-05, l1: 1.00001, l2: 0.03184\n",
            "Loss: 3.192107e-05, l1: 0.99995, l2: 0.03183\n",
            "Loss: 3.191837e-05, l1: 0.99988, l2: 0.03183\n",
            "Loss: 3.191401e-05, l1: 0.99984, l2: 0.03182\n",
            "Loss: 3.190920e-05, l1: 0.99980, l2: 0.03182\n",
            "Loss: 3.190522e-05, l1: 0.99976, l2: 0.03182\n",
            "Loss: 3.190131e-05, l1: 0.99976, l2: 0.03181\n",
            "Loss: 3.189548e-05, l1: 0.99971, l2: 0.03181\n",
            "Loss: 3.188812e-05, l1: 0.99967, l2: 0.03181\n",
            "Loss: 3.188260e-05, l1: 0.99965, l2: 0.03180\n",
            "Loss: 3.187686e-05, l1: 0.99965, l2: 0.03180\n",
            "Loss: 3.187626e-05, l1: 0.99960, l2: 0.03179\n",
            "Loss: 3.187223e-05, l1: 0.99963, l2: 0.03179\n",
            "Loss: 3.186417e-05, l1: 0.99963, l2: 0.03179\n",
            "Loss: 3.185642e-05, l1: 0.99964, l2: 0.03179\n",
            "Loss: 3.185466e-05, l1: 0.99961, l2: 0.03179\n",
            "Loss: 3.185033e-05, l1: 0.99962, l2: 0.03179\n",
            "Loss: 3.184691e-05, l1: 0.99963, l2: 0.03180\n",
            "Loss: 3.184326e-05, l1: 0.99963, l2: 0.03180\n",
            "Loss: 3.183922e-05, l1: 0.99964, l2: 0.03180\n",
            "Loss: 3.184398e-05, l1: 0.99957, l2: 0.03179\n",
            "Loss: 3.183691e-05, l1: 0.99961, l2: 0.03179\n",
            "Loss: 3.183424e-05, l1: 0.99962, l2: 0.03179\n",
            "Loss: 3.183163e-05, l1: 0.99963, l2: 0.03179\n",
            "Loss: 3.182834e-05, l1: 0.99965, l2: 0.03179\n",
            "Loss: 3.182292e-05, l1: 0.99967, l2: 0.03178\n",
            "Loss: 3.182741e-05, l1: 0.99977, l2: 0.03178\n",
            "Loss: 3.181901e-05, l1: 0.99971, l2: 0.03178\n",
            "Loss: 3.181624e-05, l1: 0.99970, l2: 0.03178\n",
            "Loss: 3.181362e-05, l1: 0.99973, l2: 0.03178\n",
            "Loss: 3.181271e-05, l1: 0.99972, l2: 0.03178\n",
            "Loss: 3.181052e-05, l1: 0.99970, l2: 0.03178\n",
            "Loss: 3.180864e-05, l1: 0.99969, l2: 0.03178\n",
            "Loss: 3.180496e-05, l1: 0.99970, l2: 0.03177\n",
            "Loss: 3.179979e-05, l1: 0.99966, l2: 0.03177\n",
            "Loss: 3.179595e-05, l1: 0.99964, l2: 0.03177\n",
            "Loss: 3.179170e-05, l1: 0.99963, l2: 0.03177\n",
            "Loss: 3.178761e-05, l1: 0.99966, l2: 0.03177\n",
            "Loss: 3.178259e-05, l1: 0.99962, l2: 0.03177\n",
            "Loss: 3.177782e-05, l1: 0.99964, l2: 0.03177\n",
            "Loss: 3.177452e-05, l1: 0.99964, l2: 0.03177\n",
            "Loss: 3.177231e-05, l1: 0.99965, l2: 0.03177\n",
            "Loss: 3.177055e-05, l1: 0.99963, l2: 0.03177\n",
            "Loss: 3.176841e-05, l1: 0.99962, l2: 0.03177\n",
            "Loss: 3.176610e-05, l1: 0.99960, l2: 0.03176\n",
            "Loss: 3.176352e-05, l1: 0.99959, l2: 0.03176\n",
            "Loss: 3.175919e-05, l1: 0.99958, l2: 0.03176\n",
            "Loss: 3.175369e-05, l1: 0.99957, l2: 0.03176\n",
            "Loss: 3.174926e-05, l1: 0.99956, l2: 0.03175\n",
            "Loss: 3.174557e-05, l1: 0.99959, l2: 0.03175\n",
            "Loss: 3.174213e-05, l1: 0.99963, l2: 0.03175\n",
            "Loss: 3.173984e-05, l1: 0.99965, l2: 0.03175\n",
            "Loss: 3.173555e-05, l1: 0.99968, l2: 0.03176\n",
            "Loss: 3.173263e-05, l1: 0.99970, l2: 0.03176\n",
            "Loss: 3.173106e-05, l1: 0.99970, l2: 0.03176\n",
            "Loss: 3.173023e-05, l1: 0.99970, l2: 0.03176\n",
            "Loss: 3.172936e-05, l1: 0.99970, l2: 0.03176\n",
            "Loss: 3.172870e-05, l1: 0.99970, l2: 0.03176\n",
            "Loss: 3.172709e-05, l1: 0.99971, l2: 0.03176\n",
            "Loss: 3.172474e-05, l1: 0.99972, l2: 0.03177\n",
            "Loss: 3.172420e-05, l1: 0.99969, l2: 0.03177\n",
            "Loss: 3.171889e-05, l1: 0.99972, l2: 0.03177\n",
            "Loss: 3.171712e-05, l1: 0.99970, l2: 0.03177\n",
            "Loss: 3.171555e-05, l1: 0.99969, l2: 0.03177\n",
            "Loss: 3.171396e-05, l1: 0.99966, l2: 0.03177\n",
            "Loss: 3.171256e-05, l1: 0.99967, l2: 0.03177\n",
            "Loss: 3.171032e-05, l1: 0.99967, l2: 0.03177\n",
            "Loss: 3.170772e-05, l1: 0.99966, l2: 0.03178\n",
            "Loss: 3.170511e-05, l1: 0.99965, l2: 0.03178\n",
            "Loss: 3.170311e-05, l1: 0.99963, l2: 0.03178\n",
            "Loss: 3.170145e-05, l1: 0.99961, l2: 0.03178\n",
            "Loss: 3.169973e-05, l1: 0.99960, l2: 0.03178\n",
            "Loss: 3.169715e-05, l1: 0.99959, l2: 0.03178\n",
            "Loss: 3.169996e-05, l1: 0.99948, l2: 0.03177\n",
            "Loss: 3.169642e-05, l1: 0.99955, l2: 0.03178\n",
            "Loss: 3.169468e-05, l1: 0.99956, l2: 0.03178\n",
            "Loss: 3.169275e-05, l1: 0.99955, l2: 0.03178\n",
            "Loss: 3.169078e-05, l1: 0.99953, l2: 0.03178\n",
            "Loss: 3.168777e-05, l1: 0.99952, l2: 0.03178\n",
            "Loss: 3.168444e-05, l1: 0.99952, l2: 0.03178\n",
            "Loss: 3.168030e-05, l1: 0.99955, l2: 0.03177\n",
            "Loss: 3.167780e-05, l1: 0.99955, l2: 0.03177\n",
            "Loss: 3.167543e-05, l1: 0.99958, l2: 0.03177\n",
            "Loss: 3.167264e-05, l1: 0.99962, l2: 0.03178\n",
            "Loss: 3.167151e-05, l1: 0.99964, l2: 0.03178\n",
            "Loss: 3.166684e-05, l1: 0.99964, l2: 0.03178\n",
            "Loss: 3.166500e-05, l1: 0.99964, l2: 0.03178\n",
            "Loss: 3.166157e-05, l1: 0.99964, l2: 0.03178\n",
            "Loss: 3.166546e-05, l1: 0.99971, l2: 0.03179\n",
            "Loss: 3.165965e-05, l1: 0.99966, l2: 0.03178\n",
            "Loss: 3.165528e-05, l1: 0.99965, l2: 0.03179\n",
            "Loss: 3.165003e-05, l1: 0.99963, l2: 0.03179\n",
            "Loss: 3.164701e-05, l1: 0.99963, l2: 0.03180\n",
            "Loss: 3.165688e-05, l1: 0.99954, l2: 0.03180\n",
            "Loss: 3.164533e-05, l1: 0.99961, l2: 0.03180\n",
            "Loss: 3.164434e-05, l1: 0.99961, l2: 0.03180\n",
            "Loss: 3.164326e-05, l1: 0.99960, l2: 0.03179\n",
            "Loss: 3.164219e-05, l1: 0.99960, l2: 0.03179\n",
            "Loss: 3.164039e-05, l1: 0.99957, l2: 0.03179\n",
            "Loss: 3.163835e-05, l1: 0.99956, l2: 0.03180\n",
            "Loss: 3.163514e-05, l1: 0.99952, l2: 0.03180\n",
            "Loss: 3.163322e-05, l1: 0.99951, l2: 0.03180\n",
            "Loss: 3.163120e-05, l1: 0.99947, l2: 0.03180\n",
            "Loss: 3.162902e-05, l1: 0.99948, l2: 0.03180\n",
            "Loss: 3.162669e-05, l1: 0.99946, l2: 0.03180\n",
            "Loss: 3.162467e-05, l1: 0.99945, l2: 0.03180\n",
            "Loss: 3.162248e-05, l1: 0.99941, l2: 0.03180\n",
            "Loss: 3.162010e-05, l1: 0.99941, l2: 0.03180\n",
            "Loss: 3.161778e-05, l1: 0.99942, l2: 0.03180\n",
            "Loss: 3.161577e-05, l1: 0.99943, l2: 0.03180\n",
            "Loss: 3.161380e-05, l1: 0.99944, l2: 0.03180\n",
            "Loss: 3.161181e-05, l1: 0.99944, l2: 0.03180\n",
            "Loss: 3.160954e-05, l1: 0.99943, l2: 0.03181\n",
            "Loss: 3.160639e-05, l1: 0.99941, l2: 0.03181\n",
            "Loss: 3.160307e-05, l1: 0.99939, l2: 0.03181\n",
            "Loss: 3.160076e-05, l1: 0.99939, l2: 0.03182\n",
            "Loss: 3.159884e-05, l1: 0.99939, l2: 0.03182\n",
            "Loss: 3.159669e-05, l1: 0.99940, l2: 0.03182\n",
            "Loss: 3.159308e-05, l1: 0.99942, l2: 0.03181\n",
            "Loss: 3.159071e-05, l1: 0.99945, l2: 0.03181\n",
            "Loss: 3.158963e-05, l1: 0.99945, l2: 0.03181\n",
            "Loss: 3.158909e-05, l1: 0.99944, l2: 0.03181\n",
            "Loss: 3.158817e-05, l1: 0.99943, l2: 0.03181\n",
            "Loss: 3.158654e-05, l1: 0.99940, l2: 0.03181\n",
            "Loss: 3.158511e-05, l1: 0.99938, l2: 0.03181\n",
            "Loss: 3.158326e-05, l1: 0.99937, l2: 0.03181\n",
            "Loss: 3.157929e-05, l1: 0.99935, l2: 0.03181\n",
            "Loss: 3.157451e-05, l1: 0.99936, l2: 0.03180\n",
            "Loss: 3.156866e-05, l1: 0.99936, l2: 0.03180\n",
            "Loss: 3.156239e-05, l1: 0.99943, l2: 0.03181\n",
            "Loss: 3.156055e-05, l1: 0.99945, l2: 0.03180\n",
            "Loss: 3.155576e-05, l1: 0.99949, l2: 0.03180\n",
            "Loss: 3.155339e-05, l1: 0.99952, l2: 0.03181\n",
            "Loss: 3.155147e-05, l1: 0.99954, l2: 0.03181\n",
            "Loss: 3.154825e-05, l1: 0.99956, l2: 0.03181\n",
            "Loss: 3.154554e-05, l1: 0.99958, l2: 0.03181\n",
            "Loss: 3.154782e-05, l1: 0.99957, l2: 0.03180\n",
            "Loss: 3.154412e-05, l1: 0.99958, l2: 0.03181\n",
            "Loss: 3.154293e-05, l1: 0.99958, l2: 0.03180\n",
            "Loss: 3.154117e-05, l1: 0.99957, l2: 0.03180\n",
            "Loss: 3.153950e-05, l1: 0.99959, l2: 0.03180\n",
            "Loss: 3.153714e-05, l1: 0.99960, l2: 0.03180\n",
            "Loss: 3.153864e-05, l1: 0.99965, l2: 0.03179\n",
            "Loss: 3.153480e-05, l1: 0.99962, l2: 0.03179\n",
            "Loss: 3.153116e-05, l1: 0.99964, l2: 0.03179\n",
            "Loss: 3.152888e-05, l1: 0.99966, l2: 0.03179\n",
            "Loss: 3.152604e-05, l1: 0.99969, l2: 0.03179\n",
            "Loss: 3.152196e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.151822e-05, l1: 0.99976, l2: 0.03178\n",
            "Loss: 3.152867e-05, l1: 0.99984, l2: 0.03177\n",
            "Loss: 3.151717e-05, l1: 0.99978, l2: 0.03178\n",
            "Loss: 3.151540e-05, l1: 0.99977, l2: 0.03178\n",
            "Loss: 3.151196e-05, l1: 0.99975, l2: 0.03178\n",
            "Loss: 3.150931e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.150581e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.150274e-05, l1: 0.99974, l2: 0.03179\n",
            "Loss: 3.150031e-05, l1: 0.99975, l2: 0.03179\n",
            "Loss: 3.149839e-05, l1: 0.99975, l2: 0.03179\n",
            "Loss: 3.150808e-05, l1: 0.99985, l2: 0.03179\n",
            "Loss: 3.149757e-05, l1: 0.99977, l2: 0.03179\n",
            "Loss: 3.149604e-05, l1: 0.99977, l2: 0.03179\n",
            "Loss: 3.149509e-05, l1: 0.99978, l2: 0.03179\n",
            "Loss: 3.149363e-05, l1: 0.99979, l2: 0.03179\n",
            "Loss: 3.148999e-05, l1: 0.99982, l2: 0.03179\n",
            "Loss: 3.148869e-05, l1: 0.99989, l2: 0.03179\n",
            "Loss: 3.148638e-05, l1: 0.99988, l2: 0.03179\n",
            "Loss: 3.148500e-05, l1: 0.99987, l2: 0.03179\n",
            "Loss: 3.148357e-05, l1: 0.99986, l2: 0.03179\n",
            "Loss: 3.148192e-05, l1: 0.99986, l2: 0.03178\n",
            "Loss: 3.148035e-05, l1: 0.99986, l2: 0.03178\n",
            "Loss: 3.147944e-05, l1: 0.99986, l2: 0.03178\n",
            "Loss: 3.147869e-05, l1: 0.99986, l2: 0.03178\n",
            "Loss: 3.147740e-05, l1: 0.99985, l2: 0.03178\n",
            "Loss: 3.147539e-05, l1: 0.99983, l2: 0.03178\n",
            "Loss: 3.147265e-05, l1: 0.99981, l2: 0.03178\n",
            "Loss: 3.147019e-05, l1: 0.99978, l2: 0.03178\n",
            "Loss: 3.147761e-05, l1: 0.99967, l2: 0.03177\n",
            "Loss: 3.146941e-05, l1: 0.99976, l2: 0.03178\n",
            "Loss: 3.146797e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.146696e-05, l1: 0.99973, l2: 0.03178\n",
            "Loss: 3.146529e-05, l1: 0.99971, l2: 0.03177\n",
            "Loss: 3.146414e-05, l1: 0.99970, l2: 0.03177\n",
            "Loss: 3.146301e-05, l1: 0.99969, l2: 0.03177\n",
            "Loss: 3.146178e-05, l1: 0.99969, l2: 0.03177\n",
            "Loss: 3.146060e-05, l1: 0.99969, l2: 0.03177\n",
            "Loss: 3.145956e-05, l1: 0.99969, l2: 0.03177\n",
            "Loss: 3.145848e-05, l1: 0.99967, l2: 0.03177\n",
            "Loss: 3.145704e-05, l1: 0.99966, l2: 0.03177\n",
            "Loss: 3.145521e-05, l1: 0.99964, l2: 0.03177\n",
            "Loss: 3.145342e-05, l1: 0.99962, l2: 0.03177\n",
            "Loss: 3.145411e-05, l1: 0.99964, l2: 0.03177\n",
            "Loss: 3.145214e-05, l1: 0.99963, l2: 0.03177\n",
            "Loss: 3.144968e-05, l1: 0.99960, l2: 0.03176\n",
            "Loss: 3.144747e-05, l1: 0.99958, l2: 0.03177\n",
            "Loss: 3.144612e-05, l1: 0.99956, l2: 0.03177\n",
            "Loss: 3.144454e-05, l1: 0.99956, l2: 0.03177\n",
            "Loss: 3.144268e-05, l1: 0.99955, l2: 0.03177\n",
            "Loss: 3.144073e-05, l1: 0.99954, l2: 0.03177\n",
            "Loss: 3.143847e-05, l1: 0.99951, l2: 0.03177\n",
            "Loss: 3.144149e-05, l1: 0.99942, l2: 0.03176\n",
            "Loss: 3.143742e-05, l1: 0.99948, l2: 0.03177\n",
            "Loss: 3.143512e-05, l1: 0.99945, l2: 0.03176\n",
            "Loss: 3.143302e-05, l1: 0.99943, l2: 0.03176\n",
            "Loss: 3.143061e-05, l1: 0.99940, l2: 0.03177\n",
            "Loss: 3.142824e-05, l1: 0.99937, l2: 0.03177\n",
            "Loss: 3.142876e-05, l1: 0.99931, l2: 0.03177\n",
            "Loss: 3.142627e-05, l1: 0.99934, l2: 0.03177\n",
            "Loss: 3.142229e-05, l1: 0.99932, l2: 0.03177\n",
            "Loss: 3.141838e-05, l1: 0.99928, l2: 0.03177\n",
            "Loss: 3.141591e-05, l1: 0.99926, l2: 0.03177\n",
            "Loss: 3.141428e-05, l1: 0.99924, l2: 0.03177\n",
            "Loss: 3.141267e-05, l1: 0.99922, l2: 0.03177\n",
            "Loss: 3.140898e-05, l1: 0.99918, l2: 0.03177\n",
            "Loss: 3.140295e-05, l1: 0.99908, l2: 0.03177\n",
            "Loss: 3.140078e-05, l1: 0.99899, l2: 0.03177\n",
            "Loss: 3.139909e-05, l1: 0.99900, l2: 0.03177\n",
            "Loss: 3.139842e-05, l1: 0.99900, l2: 0.03177\n",
            "Loss: 3.139778e-05, l1: 0.99900, l2: 0.03177\n",
            "Loss: 3.139685e-05, l1: 0.99900, l2: 0.03177\n",
            "Loss: 3.139518e-05, l1: 0.99900, l2: 0.03177\n",
            "Loss: 3.139261e-05, l1: 0.99903, l2: 0.03177\n",
            "Loss: 3.139210e-05, l1: 0.99907, l2: 0.03177\n",
            "Loss: 3.138965e-05, l1: 0.99909, l2: 0.03177\n",
            "Loss: 3.138896e-05, l1: 0.99911, l2: 0.03177\n",
            "Loss: 3.138738e-05, l1: 0.99914, l2: 0.03177\n",
            "Loss: 3.138544e-05, l1: 0.99918, l2: 0.03177\n",
            "Loss: 3.138174e-05, l1: 0.99922, l2: 0.03177\n",
            "Loss: 3.137692e-05, l1: 0.99927, l2: 0.03177\n",
            "Loss: 3.137161e-05, l1: 0.99931, l2: 0.03177\n",
            "Loss: 3.137616e-05, l1: 0.99940, l2: 0.03178\n",
            "Loss: 3.136963e-05, l1: 0.99934, l2: 0.03178\n",
            "Loss: 3.136594e-05, l1: 0.99935, l2: 0.03178\n",
            "Loss: 3.136374e-05, l1: 0.99933, l2: 0.03178\n",
            "Loss: 3.136302e-05, l1: 0.99935, l2: 0.03179\n",
            "Loss: 3.136108e-05, l1: 0.99936, l2: 0.03179\n",
            "Loss: 3.136010e-05, l1: 0.99938, l2: 0.03179\n",
            "Loss: 3.135749e-05, l1: 0.99944, l2: 0.03178\n",
            "Loss: 3.135969e-05, l1: 0.99962, l2: 0.03179\n",
            "Loss: 3.135568e-05, l1: 0.99951, l2: 0.03178\n",
            "Loss: 3.135349e-05, l1: 0.99955, l2: 0.03178\n",
            "Loss: 3.135223e-05, l1: 0.99956, l2: 0.03178\n",
            "Loss: 3.135104e-05, l1: 0.99955, l2: 0.03178\n",
            "Loss: 3.134947e-05, l1: 0.99956, l2: 0.03178\n",
            "Loss: 3.134727e-05, l1: 0.99956, l2: 0.03178\n",
            "Loss: 3.134517e-05, l1: 0.99959, l2: 0.03178\n",
            "Loss: 3.134183e-05, l1: 0.99963, l2: 0.03178\n",
            "Loss: 3.133950e-05, l1: 0.99964, l2: 0.03178\n",
            "Loss: 3.133798e-05, l1: 0.99964, l2: 0.03178\n",
            "Loss: 3.133426e-05, l1: 0.99966, l2: 0.03178\n",
            "Loss: 3.133282e-05, l1: 0.99965, l2: 0.03178\n",
            "Loss: 3.133120e-05, l1: 0.99963, l2: 0.03179\n",
            "Loss: 3.132947e-05, l1: 0.99963, l2: 0.03179\n",
            "Loss: 3.132716e-05, l1: 0.99964, l2: 0.03179\n",
            "Loss: 3.132442e-05, l1: 0.99967, l2: 0.03180\n",
            "Loss: 3.132159e-05, l1: 0.99969, l2: 0.03180\n",
            "Loss: 3.131893e-05, l1: 0.99969, l2: 0.03180\n",
            "Loss: 3.131690e-05, l1: 0.99967, l2: 0.03179\n",
            "Loss: 3.131567e-05, l1: 0.99964, l2: 0.03179\n",
            "Loss: 3.131449e-05, l1: 0.99965, l2: 0.03179\n",
            "Loss: 3.131338e-05, l1: 0.99967, l2: 0.03179\n",
            "Loss: 3.131216e-05, l1: 0.99969, l2: 0.03179\n",
            "Loss: 3.131061e-05, l1: 0.99971, l2: 0.03179\n",
            "Loss: 3.130916e-05, l1: 0.99973, l2: 0.03179\n",
            "Loss: 3.130996e-05, l1: 0.99974, l2: 0.03179\n",
            "Loss: 3.130846e-05, l1: 0.99973, l2: 0.03179\n",
            "Loss: 3.130786e-05, l1: 0.99973, l2: 0.03179\n",
            "Loss: 3.130666e-05, l1: 0.99971, l2: 0.03179\n",
            "Loss: 3.130560e-05, l1: 0.99970, l2: 0.03180\n",
            "Loss: 3.130323e-05, l1: 0.99969, l2: 0.03180\n",
            "Loss: 3.131973e-05, l1: 0.99955, l2: 0.03179\n",
            "Loss: 3.130298e-05, l1: 0.99967, l2: 0.03180\n",
            "Loss: 3.130107e-05, l1: 0.99967, l2: 0.03180\n",
            "Loss: 3.129989e-05, l1: 0.99962, l2: 0.03180\n",
            "Loss: 3.129823e-05, l1: 0.99963, l2: 0.03180\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
            "  Objective function value: 0.000031\n",
            "  Number of iterations: 1569\n",
            "  Number of functions evaluations: 1686\n",
            "Error lambda_1: 0.036979%\n",
            "Error lambda_2: 0.101831%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################    \n",
        "    \n",
        "fig, ax = newfig(1.0, 1.4)\n",
        "ax.axis('off')\n",
        "    \n",
        "####### Row 0: u(t,x) ##################    \n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                  origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "    \n",
        "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "####### Row 1: u(t,x) slices ##################    \n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')    \n",
        "ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])    \n",
        "ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "####### Row 3: Identified PDE ##################    \n",
        "gs2 = gridspec.GridSpec(1, 3)\n",
        "gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "ax = plt.subplot(gs2[:, :])\n",
        "ax.axis('off')\n",
        "s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "s3 = r'Identified PDE (1\\% noise) & '\n",
        "s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "s5 = r'\\end{tabular}$'\n",
        "s = s1+s2+s3+s4+s5\n",
        "ax.text(0.1,0.1,s)\n",
        "        \n",
        "savefig('./Burgers_identification')   \n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "wQJocGkFn0j4",
        "outputId": "0fc7ecf1-62df-4e22-dcf0-e44bc3905a7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 388.543x336.186 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}