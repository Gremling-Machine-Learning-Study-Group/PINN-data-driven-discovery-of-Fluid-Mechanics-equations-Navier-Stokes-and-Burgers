{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Burgers_nu3_2000.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvsbii5zbr9p",
        "outputId": "51b53e1f-7c83-46c2-f79d-3513e8a4b761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-lmodern fonts-noto-mono libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpotrace0 libptexenc1 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13\n",
            "  lmodern poppler-data t1utils tex-common texlive-base texlive-binaries\n",
            "  texlive-latex-base\n",
            "Suggested packages:\n",
            "  fonts-noto poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum debhelper gv\n",
            "  | postscript-viewer perl-tk xpdf-reader | pdf-viewer texlive-latex-base-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-lmodern fonts-noto-mono libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpotrace0 libptexenc1 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13\n",
            "  lmodern poppler-data t1utils tex-common texlive-base texlive-binaries\n",
            "  texlive-latex-base texlive-latex-recommended\n",
            "0 upgraded, 24 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 68.4 MB of archives.\n",
            "After this operation, 223 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.15 [5,092 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [2,265 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Fetched 68.4 MB in 3s (22.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 24.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../01-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../02-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../03-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../04-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../05-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../06-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../09-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.15_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../10-libgs9_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../11-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../12-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../13-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../14-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../15-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../16-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../17-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../18-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../19-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../20-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../21-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../22-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../23-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-lato fonts-texgyre javascript-common libjs-jquery libruby2.5\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration tex-gyre texlive-fonts-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  apache2 | lighttpd | httpd ri ruby-dev bundler texlive-fonts-recommended-doc\n",
            "  python-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc dot2tex prerex\n",
            "  ruby-tcltk | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  fonts-lato fonts-texgyre javascript-common libjs-jquery libruby2.5\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration tex-gyre texlive-fonts-recommended texlive-latex-extra\n",
            "  texlive-pictures texlive-plain-generic tipa\n",
            "0 upgraded, 21 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 66.6 MB of archives.\n",
            "After this operation, 216 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.11 [48.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.11 [3,072 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Fetched 66.6 MB in 3s (21.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 21.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-lato.\n",
            "(Reading database ... 162654 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../01-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../02-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../03-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../04-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../05-ruby2.5_2.5.1-1ubuntu1.11_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.11) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../06-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../07-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../08-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../09-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../10-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../11-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../12-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../13-libruby2.5_2.5.1-1ubuntu1.11_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.11) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../14-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../15-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../16-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../17-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../18-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../19-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../20-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.11) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.11) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running mktexlsr. This may take some time... done.\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  ghostscript gsfonts\n",
            "Suggested packages:\n",
            "  ghostscript-x\n",
            "The following NEW packages will be installed:\n",
            "  dvipng ghostscript gsfonts\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,250 kB of archives.\n",
            "After this operation, 4,947 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [51.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dvipng amd64 1.15-1 [78.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Fetched 3,250 kB in 1s (3,176 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ghostscript.\n",
            "(Reading database ... 181005 files and directories currently installed.)\n",
            "Preparing to unpack .../ghostscript_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cm-super-minimal pfb2t1c2pfb\n",
            "The following NEW packages will be installed:\n",
            "  cm-super cm-super-minimal pfb2t1c2pfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 24.5 MB of archives.\n",
            "After this operation, 59.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super-minimal all 0.3.4-11 [5,810 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pfb2t1c2pfb amd64 0.3-11 [9,342 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super all 0.3.4-11 [18.7 MB]\n",
            "Fetched 24.5 MB in 2s (14.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cm-super-minimal.\n",
            "(Reading database ... 181198 files and directories currently installed.)\n",
            "Preparing to unpack .../cm-super-minimal_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super-minimal (0.3.4-11) ...\n",
            "Selecting previously unselected package pfb2t1c2pfb.\n",
            "Preparing to unpack .../pfb2t1c2pfb_0.3-11_amd64.deb ...\n",
            "Unpacking pfb2t1c2pfb (0.3-11) ...\n",
            "Selecting previously unselected package cm-super.\n",
            "Preparing to unpack .../cm-super_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super (0.3.4-11) ...\n",
            "Setting up pfb2t1c2pfb (0.3-11) ...\n",
            "Setting up cm-super-minimal (0.3.4-11) ...\n",
            "Setting up cm-super (0.3.4-11) ...\n",
            "Creating fonts. This may take some time... done.\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running mktexlsr. This may take some time... done.\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "#devido aos plots no final do script devemos intalar latex na máquina\n",
        "! sudo apt-get install texlive-latex-recommended \n",
        "! sudo apt install texlive-latex-extra\n",
        "! sudo apt install dvipng\n",
        "! sudo apt install cm-super\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.15.2\n",
        "import tensorflow as tf   #machine learning framework\n",
        "#tf.disable_v2_behavior()\n",
        "print('TensorFlow version: ', tf.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBCUeYMwbyT4",
        "outputId": "fa4de593-6197-4b20-9032-3249c36e0292"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.2`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "TensorFlow version:  <module 'tensorflow._api.v1.version' from '/tensorflow-1.15.2/python3.7/tensorflow_core/_api/v1/version/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@author: Maziar Raissi\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '../../Utilities/')\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from plotting import newfig, savefig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "import time\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)"
      ],
      "metadata": {
        "id": "ReZIGxTKnaNa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, X, u, layers, lb, ub):\n",
        "        \n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        \n",
        "        self.x = X[:,0:1]\n",
        "        self.t = X[:,1:2]\n",
        "        self.u = u\n",
        "        \n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NNs\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
        "        \n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
        "                \n",
        "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
        "        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n",
        "        \n",
        "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_pred))\n",
        "        \n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
        "                                                                method = 'L-BFGS-B', \n",
        "                                                                options = {'maxiter': 50000,\n",
        "                                                                           'maxfun': 50000,\n",
        "                                                                           'maxcor': 50,\n",
        "                                                                           'maxls': 50,\n",
        "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
        "    \n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "            \n",
        "    def net_u(self, x, t):  \n",
        "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
        "        return u\n",
        "    \n",
        "    def net_f(self, x, t):\n",
        "        lambda_1 = self.lambda_1        \n",
        "        lambda_2 = tf.exp(self.lambda_2)\n",
        "        u = self.net_u(x,t)\n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        f = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def callback(self, loss, lambda_1, lambda_2):\n",
        "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
        "        \n",
        "        \n",
        "    def train(self, nIter):\n",
        "        tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                lambda_1_value = self.sess.run(self.lambda_1)\n",
        "                lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
        "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
        "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict = tf_dict,\n",
        "                                fetches = [self.loss, self.lambda_1, self.lambda_2],\n",
        "                                loss_callback = self.callback)\n",
        "        \n",
        "        \n",
        "    def predict(self, X_star):\n",
        "        \n",
        "        tf_dict = {self.x_tf: X_star[:,0:1], self.t_tf: X_star[:,1:2]}\n",
        "        \n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
        "        \n",
        "        return u_star, f_star"
      ],
      "metadata": {
        "id": "bfhlzV7fnbJb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\": \n",
        "     \n",
        "    nu = 0.1/np.pi   #nu: valor de lambda_2\n",
        "\n",
        "    N_u = 2000  #N: número de elementos de treino\n",
        "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "    \n",
        "    data = scipy.io.loadmat('../content/un100.mat')\n",
        "    \n",
        "    t = data['t'].flatten()[:,None]\n",
        "    x = data['x'].flatten()[:,None]\n",
        "    Exact = np.real(data['usol']).T   #.T: transposição da matriz de dados usol\n",
        "    \n",
        "    X, T = np.meshgrid(x,t)\n",
        "    \n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "    u_star = Exact.flatten()[:,None]              \n",
        "\n",
        "    # Domain bounds\n",
        "    lb = X_star.min(0)\n",
        "    ub = X_star.max(0)    \n",
        "    \n",
        "    ######################################################################\n",
        "    ######################## Noiseless Data ###############################\n",
        "    ######################################################################\n",
        "    noise = 0.0            \n",
        "             \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "    \n",
        "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "    model.train(2000)\n",
        "    \n",
        "    u_pred, f_pred = model.predict(X_star)\n",
        "            \n",
        "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "    \n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "        \n",
        "    lambda_1_value = model.sess.run(model.lambda_1)\n",
        "    lambda_2_value = model.sess.run(model.lambda_2)\n",
        "    lambda_2_value = np.exp(lambda_2_value)\n",
        "    \n",
        "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
        "    error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
        "    \n",
        "    print('Error u: %e' % (error_u))    \n",
        "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
        "    print('Error l2: %.5f%%' % (error_lambda_2))  \n",
        "    \n",
        "                               "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWIWsRhBnzr2",
        "outputId": "3b270376-8813-47a7-db71-5ec7d6037821"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "It: 0, Loss: 2.898e-01, Lambda_1: -0.001, Lambda_2: 0.002479, Time: 1.66\n",
            "It: 10, Loss: 1.900e-01, Lambda_1: -0.001, Lambda_2: 0.002493, Time: 0.28\n",
            "It: 20, Loss: 1.663e-01, Lambda_1: -0.009, Lambda_2: 0.002516, Time: 0.27\n",
            "It: 30, Loss: 1.454e-01, Lambda_1: -0.021, Lambda_2: 0.002546, Time: 0.28\n",
            "It: 40, Loss: 1.296e-01, Lambda_1: -0.031, Lambda_2: 0.002576, Time: 0.27\n",
            "It: 50, Loss: 1.005e-01, Lambda_1: -0.036, Lambda_2: 0.002590, Time: 0.28\n",
            "It: 60, Loss: 6.922e-02, Lambda_1: -0.032, Lambda_2: 0.002560, Time: 0.27\n",
            "It: 70, Loss: 4.773e-02, Lambda_1: -0.018, Lambda_2: 0.002516, Time: 0.27\n",
            "It: 80, Loss: 3.795e-02, Lambda_1: 0.001, Lambda_2: 0.002467, Time: 0.27\n",
            "It: 90, Loss: 3.265e-02, Lambda_1: 0.020, Lambda_2: 0.002418, Time: 0.27\n",
            "It: 100, Loss: 2.778e-02, Lambda_1: 0.038, Lambda_2: 0.002376, Time: 0.29\n",
            "It: 110, Loss: 2.395e-02, Lambda_1: 0.051, Lambda_2: 0.002348, Time: 0.31\n",
            "It: 120, Loss: 2.148e-02, Lambda_1: 0.059, Lambda_2: 0.002334, Time: 0.28\n",
            "It: 130, Loss: 1.993e-02, Lambda_1: 0.065, Lambda_2: 0.002331, Time: 0.29\n",
            "It: 140, Loss: 1.901e-02, Lambda_1: 0.068, Lambda_2: 0.002335, Time: 0.27\n",
            "It: 150, Loss: 1.851e-02, Lambda_1: 0.069, Lambda_2: 0.002343, Time: 0.26\n",
            "It: 160, Loss: 1.823e-02, Lambda_1: 0.068, Lambda_2: 0.002355, Time: 0.25\n",
            "It: 170, Loss: 1.805e-02, Lambda_1: 0.067, Lambda_2: 0.002366, Time: 0.26\n",
            "It: 180, Loss: 1.792e-02, Lambda_1: 0.067, Lambda_2: 0.002378, Time: 0.28\n",
            "It: 190, Loss: 1.779e-02, Lambda_1: 0.067, Lambda_2: 0.002390, Time: 0.26\n",
            "It: 200, Loss: 1.768e-02, Lambda_1: 0.067, Lambda_2: 0.002402, Time: 0.26\n",
            "It: 210, Loss: 1.757e-02, Lambda_1: 0.068, Lambda_2: 0.002415, Time: 0.26\n",
            "It: 220, Loss: 1.746e-02, Lambda_1: 0.068, Lambda_2: 0.002429, Time: 0.28\n",
            "It: 230, Loss: 1.736e-02, Lambda_1: 0.069, Lambda_2: 0.002443, Time: 0.27\n",
            "It: 240, Loss: 1.725e-02, Lambda_1: 0.070, Lambda_2: 0.002458, Time: 0.27\n",
            "It: 250, Loss: 1.714e-02, Lambda_1: 0.071, Lambda_2: 0.002474, Time: 0.29\n",
            "It: 260, Loss: 1.703e-02, Lambda_1: 0.072, Lambda_2: 0.002491, Time: 0.29\n",
            "It: 270, Loss: 1.691e-02, Lambda_1: 0.074, Lambda_2: 0.002509, Time: 0.29\n",
            "It: 280, Loss: 1.684e-02, Lambda_1: 0.075, Lambda_2: 0.002527, Time: 0.27\n",
            "It: 290, Loss: 1.667e-02, Lambda_1: 0.077, Lambda_2: 0.002547, Time: 0.27\n",
            "It: 300, Loss: 1.653e-02, Lambda_1: 0.079, Lambda_2: 0.002567, Time: 0.26\n",
            "It: 310, Loss: 1.638e-02, Lambda_1: 0.082, Lambda_2: 0.002589, Time: 0.26\n",
            "It: 320, Loss: 1.624e-02, Lambda_1: 0.084, Lambda_2: 0.002611, Time: 0.30\n",
            "It: 330, Loss: 1.608e-02, Lambda_1: 0.088, Lambda_2: 0.002635, Time: 0.29\n",
            "It: 340, Loss: 1.593e-02, Lambda_1: 0.091, Lambda_2: 0.002659, Time: 0.29\n",
            "It: 350, Loss: 1.581e-02, Lambda_1: 0.095, Lambda_2: 0.002684, Time: 0.27\n",
            "It: 360, Loss: 1.562e-02, Lambda_1: 0.100, Lambda_2: 0.002711, Time: 0.27\n",
            "It: 370, Loss: 1.545e-02, Lambda_1: 0.105, Lambda_2: 0.002738, Time: 0.27\n",
            "It: 380, Loss: 1.529e-02, Lambda_1: 0.110, Lambda_2: 0.002766, Time: 0.26\n",
            "It: 390, Loss: 1.512e-02, Lambda_1: 0.115, Lambda_2: 0.002795, Time: 0.28\n",
            "It: 400, Loss: 1.493e-02, Lambda_1: 0.121, Lambda_2: 0.002824, Time: 0.30\n",
            "It: 410, Loss: 1.476e-02, Lambda_1: 0.126, Lambda_2: 0.002855, Time: 0.30\n",
            "It: 420, Loss: 1.473e-02, Lambda_1: 0.132, Lambda_2: 0.002887, Time: 0.28\n",
            "It: 430, Loss: 1.450e-02, Lambda_1: 0.138, Lambda_2: 0.002920, Time: 0.28\n",
            "It: 440, Loss: 1.430e-02, Lambda_1: 0.144, Lambda_2: 0.002954, Time: 0.27\n",
            "It: 450, Loss: 1.409e-02, Lambda_1: 0.150, Lambda_2: 0.002989, Time: 0.27\n",
            "It: 460, Loss: 1.390e-02, Lambda_1: 0.156, Lambda_2: 0.003025, Time: 0.28\n",
            "It: 470, Loss: 1.375e-02, Lambda_1: 0.162, Lambda_2: 0.003062, Time: 0.30\n",
            "It: 480, Loss: 1.395e-02, Lambda_1: 0.168, Lambda_2: 0.003100, Time: 0.28\n",
            "It: 490, Loss: 1.356e-02, Lambda_1: 0.174, Lambda_2: 0.003139, Time: 0.29\n",
            "It: 500, Loss: 1.336e-02, Lambda_1: 0.180, Lambda_2: 0.003180, Time: 0.28\n",
            "It: 510, Loss: 1.317e-02, Lambda_1: 0.185, Lambda_2: 0.003222, Time: 0.27\n",
            "It: 520, Loss: 1.302e-02, Lambda_1: 0.191, Lambda_2: 0.003264, Time: 0.26\n",
            "It: 530, Loss: 1.288e-02, Lambda_1: 0.197, Lambda_2: 0.003308, Time: 0.27\n",
            "It: 540, Loss: 1.311e-02, Lambda_1: 0.203, Lambda_2: 0.003353, Time: 0.28\n",
            "It: 550, Loss: 1.263e-02, Lambda_1: 0.208, Lambda_2: 0.003399, Time: 0.26\n",
            "It: 560, Loss: 1.255e-02, Lambda_1: 0.214, Lambda_2: 0.003446, Time: 0.25\n",
            "It: 570, Loss: 1.243e-02, Lambda_1: 0.219, Lambda_2: 0.003494, Time: 0.28\n",
            "It: 580, Loss: 1.231e-02, Lambda_1: 0.224, Lambda_2: 0.003543, Time: 0.29\n",
            "It: 590, Loss: 1.220e-02, Lambda_1: 0.230, Lambda_2: 0.003593, Time: 0.28\n",
            "It: 600, Loss: 1.211e-02, Lambda_1: 0.235, Lambda_2: 0.003645, Time: 0.28\n",
            "It: 610, Loss: 1.202e-02, Lambda_1: 0.240, Lambda_2: 0.003697, Time: 0.30\n",
            "It: 620, Loss: 1.192e-02, Lambda_1: 0.245, Lambda_2: 0.003751, Time: 0.29\n",
            "It: 630, Loss: 1.184e-02, Lambda_1: 0.250, Lambda_2: 0.003806, Time: 0.29\n",
            "It: 640, Loss: 1.215e-02, Lambda_1: 0.254, Lambda_2: 0.003861, Time: 0.26\n",
            "It: 650, Loss: 1.179e-02, Lambda_1: 0.259, Lambda_2: 0.003918, Time: 0.26\n",
            "It: 660, Loss: 1.161e-02, Lambda_1: 0.263, Lambda_2: 0.003976, Time: 0.25\n",
            "It: 670, Loss: 1.150e-02, Lambda_1: 0.268, Lambda_2: 0.004035, Time: 0.25\n",
            "It: 680, Loss: 1.142e-02, Lambda_1: 0.272, Lambda_2: 0.004095, Time: 0.28\n",
            "It: 690, Loss: 1.134e-02, Lambda_1: 0.276, Lambda_2: 0.004155, Time: 0.26\n",
            "It: 700, Loss: 1.155e-02, Lambda_1: 0.281, Lambda_2: 0.004217, Time: 0.25\n",
            "It: 710, Loss: 1.129e-02, Lambda_1: 0.285, Lambda_2: 0.004280, Time: 0.25\n",
            "It: 720, Loss: 1.109e-02, Lambda_1: 0.290, Lambda_2: 0.004344, Time: 0.27\n",
            "It: 730, Loss: 1.101e-02, Lambda_1: 0.294, Lambda_2: 0.004409, Time: 0.28\n",
            "It: 740, Loss: 1.091e-02, Lambda_1: 0.299, Lambda_2: 0.004475, Time: 0.26\n",
            "It: 750, Loss: 1.083e-02, Lambda_1: 0.303, Lambda_2: 0.004541, Time: 0.30\n",
            "It: 760, Loss: 1.121e-02, Lambda_1: 0.308, Lambda_2: 0.004609, Time: 0.29\n",
            "It: 770, Loss: 1.070e-02, Lambda_1: 0.312, Lambda_2: 0.004676, Time: 0.28\n",
            "It: 780, Loss: 1.058e-02, Lambda_1: 0.316, Lambda_2: 0.004744, Time: 0.28\n",
            "It: 790, Loss: 1.048e-02, Lambda_1: 0.321, Lambda_2: 0.004814, Time: 0.30\n",
            "It: 800, Loss: 1.040e-02, Lambda_1: 0.326, Lambda_2: 0.004884, Time: 0.28\n",
            "It: 810, Loss: 1.031e-02, Lambda_1: 0.331, Lambda_2: 0.004955, Time: 0.28\n",
            "It: 820, Loss: 1.108e-02, Lambda_1: 0.335, Lambda_2: 0.005027, Time: 0.26\n",
            "It: 830, Loss: 1.029e-02, Lambda_1: 0.340, Lambda_2: 0.005098, Time: 0.27\n",
            "It: 840, Loss: 1.016e-02, Lambda_1: 0.345, Lambda_2: 0.005172, Time: 0.26\n",
            "It: 850, Loss: 9.989e-03, Lambda_1: 0.349, Lambda_2: 0.005247, Time: 0.26\n",
            "It: 860, Loss: 1.002e-02, Lambda_1: 0.354, Lambda_2: 0.005321, Time: 0.28\n",
            "It: 870, Loss: 1.004e-02, Lambda_1: 0.359, Lambda_2: 0.005394, Time: 0.29\n",
            "It: 880, Loss: 9.729e-03, Lambda_1: 0.364, Lambda_2: 0.005469, Time: 0.29\n",
            "It: 890, Loss: 9.643e-03, Lambda_1: 0.368, Lambda_2: 0.005544, Time: 0.29\n",
            "It: 900, Loss: 9.548e-03, Lambda_1: 0.373, Lambda_2: 0.005621, Time: 0.30\n",
            "It: 910, Loss: 9.459e-03, Lambda_1: 0.378, Lambda_2: 0.005698, Time: 0.28\n",
            "It: 920, Loss: 9.456e-03, Lambda_1: 0.383, Lambda_2: 0.005777, Time: 0.28\n",
            "It: 930, Loss: 9.310e-03, Lambda_1: 0.388, Lambda_2: 0.005856, Time: 0.30\n",
            "It: 940, Loss: 9.291e-03, Lambda_1: 0.392, Lambda_2: 0.005935, Time: 0.29\n",
            "It: 950, Loss: 9.204e-03, Lambda_1: 0.396, Lambda_2: 0.006015, Time: 0.29\n",
            "It: 960, Loss: 9.065e-03, Lambda_1: 0.401, Lambda_2: 0.006097, Time: 0.27\n",
            "It: 970, Loss: 8.990e-03, Lambda_1: 0.406, Lambda_2: 0.006180, Time: 0.27\n",
            "It: 980, Loss: 9.012e-03, Lambda_1: 0.411, Lambda_2: 0.006264, Time: 0.26\n",
            "It: 990, Loss: 9.125e-03, Lambda_1: 0.415, Lambda_2: 0.006348, Time: 0.25\n",
            "It: 1000, Loss: 8.838e-03, Lambda_1: 0.419, Lambda_2: 0.006432, Time: 0.29\n",
            "It: 1010, Loss: 8.761e-03, Lambda_1: 0.424, Lambda_2: 0.006519, Time: 0.30\n",
            "It: 1020, Loss: 8.623e-03, Lambda_1: 0.428, Lambda_2: 0.006606, Time: 0.28\n",
            "It: 1030, Loss: 8.716e-03, Lambda_1: 0.433, Lambda_2: 0.006693, Time: 0.29\n",
            "It: 1040, Loss: 9.273e-03, Lambda_1: 0.437, Lambda_2: 0.006778, Time: 0.31\n",
            "It: 1050, Loss: 8.564e-03, Lambda_1: 0.440, Lambda_2: 0.006864, Time: 0.28\n",
            "It: 1060, Loss: 8.299e-03, Lambda_1: 0.444, Lambda_2: 0.006954, Time: 0.28\n",
            "It: 1070, Loss: 8.221e-03, Lambda_1: 0.448, Lambda_2: 0.007045, Time: 0.28\n",
            "It: 1080, Loss: 8.150e-03, Lambda_1: 0.453, Lambda_2: 0.007138, Time: 0.30\n",
            "It: 1090, Loss: 8.060e-03, Lambda_1: 0.457, Lambda_2: 0.007233, Time: 0.29\n",
            "It: 1100, Loss: 7.989e-03, Lambda_1: 0.462, Lambda_2: 0.007329, Time: 0.29\n",
            "It: 1110, Loss: 8.346e-03, Lambda_1: 0.466, Lambda_2: 0.007428, Time: 0.30\n",
            "It: 1120, Loss: 8.617e-03, Lambda_1: 0.470, Lambda_2: 0.007522, Time: 0.28\n",
            "It: 1130, Loss: 7.872e-03, Lambda_1: 0.473, Lambda_2: 0.007620, Time: 0.27\n",
            "It: 1140, Loss: 7.697e-03, Lambda_1: 0.476, Lambda_2: 0.007720, Time: 0.26\n",
            "It: 1150, Loss: 7.622e-03, Lambda_1: 0.480, Lambda_2: 0.007822, Time: 0.26\n",
            "It: 1160, Loss: 7.560e-03, Lambda_1: 0.485, Lambda_2: 0.007924, Time: 0.25\n",
            "It: 1170, Loss: 7.467e-03, Lambda_1: 0.489, Lambda_2: 0.008030, Time: 0.28\n",
            "It: 1180, Loss: 7.391e-03, Lambda_1: 0.494, Lambda_2: 0.008138, Time: 0.30\n",
            "It: 1190, Loss: 9.522e-03, Lambda_1: 0.498, Lambda_2: 0.008247, Time: 0.28\n",
            "It: 1200, Loss: 7.712e-03, Lambda_1: 0.501, Lambda_2: 0.008348, Time: 0.28\n",
            "It: 1210, Loss: 7.440e-03, Lambda_1: 0.504, Lambda_2: 0.008458, Time: 0.27\n",
            "It: 1220, Loss: 7.103e-03, Lambda_1: 0.507, Lambda_2: 0.008569, Time: 0.30\n",
            "It: 1230, Loss: 7.031e-03, Lambda_1: 0.511, Lambda_2: 0.008679, Time: 0.29\n",
            "It: 1240, Loss: 6.946e-03, Lambda_1: 0.515, Lambda_2: 0.008791, Time: 0.27\n",
            "It: 1250, Loss: 6.869e-03, Lambda_1: 0.520, Lambda_2: 0.008906, Time: 0.28\n",
            "It: 1260, Loss: 7.046e-03, Lambda_1: 0.524, Lambda_2: 0.009024, Time: 0.27\n",
            "It: 1270, Loss: 7.657e-03, Lambda_1: 0.527, Lambda_2: 0.009130, Time: 0.27\n",
            "It: 1280, Loss: 6.806e-03, Lambda_1: 0.529, Lambda_2: 0.009239, Time: 0.29\n",
            "It: 1290, Loss: 6.784e-03, Lambda_1: 0.530, Lambda_2: 0.009355, Time: 0.29\n",
            "It: 1300, Loss: 6.661e-03, Lambda_1: 0.533, Lambda_2: 0.009468, Time: 0.29\n",
            "It: 1310, Loss: 6.497e-03, Lambda_1: 0.537, Lambda_2: 0.009580, Time: 0.29\n",
            "It: 1320, Loss: 6.407e-03, Lambda_1: 0.541, Lambda_2: 0.009695, Time: 0.32\n",
            "It: 1330, Loss: 6.330e-03, Lambda_1: 0.546, Lambda_2: 0.009814, Time: 0.31\n",
            "It: 1340, Loss: 6.255e-03, Lambda_1: 0.549, Lambda_2: 0.009937, Time: 0.29\n",
            "It: 1350, Loss: 6.178e-03, Lambda_1: 0.553, Lambda_2: 0.010062, Time: 0.29\n",
            "It: 1360, Loss: 6.101e-03, Lambda_1: 0.557, Lambda_2: 0.010189, Time: 0.31\n",
            "It: 1370, Loss: 6.025e-03, Lambda_1: 0.561, Lambda_2: 0.010319, Time: 0.29\n",
            "It: 1380, Loss: 5.948e-03, Lambda_1: 0.565, Lambda_2: 0.010450, Time: 0.28\n",
            "It: 1390, Loss: 6.082e-03, Lambda_1: 0.569, Lambda_2: 0.010583, Time: 0.28\n",
            "It: 1400, Loss: 7.270e-03, Lambda_1: 0.571, Lambda_2: 0.010697, Time: 0.26\n",
            "It: 1410, Loss: 6.248e-03, Lambda_1: 0.572, Lambda_2: 0.010813, Time: 0.27\n",
            "It: 1420, Loss: 5.895e-03, Lambda_1: 0.572, Lambda_2: 0.010941, Time: 0.29\n",
            "It: 1430, Loss: 5.849e-03, Lambda_1: 0.573, Lambda_2: 0.011065, Time: 0.30\n",
            "It: 1440, Loss: 5.619e-03, Lambda_1: 0.577, Lambda_2: 0.011189, Time: 0.29\n",
            "It: 1450, Loss: 5.550e-03, Lambda_1: 0.581, Lambda_2: 0.011317, Time: 0.29\n",
            "It: 1460, Loss: 5.473e-03, Lambda_1: 0.585, Lambda_2: 0.011450, Time: 0.30\n",
            "It: 1470, Loss: 5.394e-03, Lambda_1: 0.589, Lambda_2: 0.011586, Time: 0.29\n",
            "It: 1480, Loss: 5.318e-03, Lambda_1: 0.593, Lambda_2: 0.011727, Time: 0.29\n",
            "It: 1490, Loss: 5.243e-03, Lambda_1: 0.597, Lambda_2: 0.011870, Time: 0.29\n",
            "It: 1500, Loss: 5.169e-03, Lambda_1: 0.601, Lambda_2: 0.012014, Time: 0.30\n",
            "It: 1510, Loss: 5.094e-03, Lambda_1: 0.605, Lambda_2: 0.012161, Time: 0.28\n",
            "It: 1520, Loss: 5.020e-03, Lambda_1: 0.609, Lambda_2: 0.012310, Time: 0.29\n",
            "It: 1530, Loss: 4.946e-03, Lambda_1: 0.613, Lambda_2: 0.012460, Time: 0.31\n",
            "It: 1540, Loss: 4.871e-03, Lambda_1: 0.617, Lambda_2: 0.012613, Time: 0.28\n",
            "It: 1550, Loss: 4.797e-03, Lambda_1: 0.620, Lambda_2: 0.012768, Time: 0.29\n",
            "It: 1560, Loss: 4.722e-03, Lambda_1: 0.624, Lambda_2: 0.012926, Time: 0.28\n",
            "It: 1570, Loss: 4.648e-03, Lambda_1: 0.628, Lambda_2: 0.013085, Time: 0.29\n",
            "It: 1580, Loss: 5.502e-03, Lambda_1: 0.632, Lambda_2: 0.013246, Time: 0.28\n",
            "It: 1590, Loss: 8.274e-03, Lambda_1: 0.633, Lambda_2: 0.013363, Time: 0.28\n",
            "It: 1600, Loss: 4.617e-03, Lambda_1: 0.631, Lambda_2: 0.013520, Time: 0.31\n",
            "It: 1610, Loss: 4.589e-03, Lambda_1: 0.630, Lambda_2: 0.013670, Time: 0.29\n",
            "It: 1620, Loss: 4.535e-03, Lambda_1: 0.631, Lambda_2: 0.013810, Time: 0.28\n",
            "It: 1630, Loss: 4.418e-03, Lambda_1: 0.635, Lambda_2: 0.013951, Time: 0.31\n",
            "It: 1640, Loss: 4.285e-03, Lambda_1: 0.639, Lambda_2: 0.014103, Time: 0.29\n",
            "It: 1650, Loss: 4.220e-03, Lambda_1: 0.643, Lambda_2: 0.014255, Time: 0.29\n",
            "It: 1660, Loss: 4.142e-03, Lambda_1: 0.647, Lambda_2: 0.014414, Time: 0.27\n",
            "It: 1670, Loss: 4.073e-03, Lambda_1: 0.651, Lambda_2: 0.014577, Time: 0.27\n",
            "It: 1680, Loss: 4.005e-03, Lambda_1: 0.655, Lambda_2: 0.014741, Time: 0.27\n",
            "It: 1690, Loss: 3.937e-03, Lambda_1: 0.659, Lambda_2: 0.014907, Time: 0.26\n",
            "It: 1700, Loss: 3.869e-03, Lambda_1: 0.663, Lambda_2: 0.015076, Time: 0.26\n",
            "It: 1710, Loss: 3.803e-03, Lambda_1: 0.667, Lambda_2: 0.015246, Time: 0.28\n",
            "It: 1720, Loss: 3.737e-03, Lambda_1: 0.671, Lambda_2: 0.015417, Time: 0.27\n",
            "It: 1730, Loss: 3.671e-03, Lambda_1: 0.675, Lambda_2: 0.015591, Time: 0.27\n",
            "It: 1740, Loss: 3.606e-03, Lambda_1: 0.679, Lambda_2: 0.015765, Time: 0.28\n",
            "It: 1750, Loss: 3.542e-03, Lambda_1: 0.682, Lambda_2: 0.015942, Time: 0.27\n",
            "It: 1760, Loss: 3.479e-03, Lambda_1: 0.686, Lambda_2: 0.016119, Time: 0.28\n",
            "It: 1770, Loss: 3.416e-03, Lambda_1: 0.690, Lambda_2: 0.016298, Time: 0.29\n",
            "It: 1780, Loss: 3.354e-03, Lambda_1: 0.694, Lambda_2: 0.016478, Time: 0.30\n",
            "It: 1790, Loss: 4.278e-03, Lambda_1: 0.697, Lambda_2: 0.016657, Time: 0.29\n",
            "It: 1800, Loss: 4.501e-03, Lambda_1: 0.699, Lambda_2: 0.016796, Time: 0.26\n",
            "It: 1810, Loss: 3.804e-03, Lambda_1: 0.699, Lambda_2: 0.016968, Time: 0.27\n",
            "It: 1820, Loss: 3.262e-03, Lambda_1: 0.698, Lambda_2: 0.017128, Time: 0.26\n",
            "It: 1830, Loss: 3.190e-03, Lambda_1: 0.700, Lambda_2: 0.017271, Time: 0.26\n",
            "It: 1840, Loss: 3.106e-03, Lambda_1: 0.703, Lambda_2: 0.017415, Time: 0.26\n",
            "It: 1850, Loss: 3.057e-03, Lambda_1: 0.706, Lambda_2: 0.017562, Time: 0.29\n",
            "It: 1860, Loss: 2.995e-03, Lambda_1: 0.710, Lambda_2: 0.017714, Time: 0.27\n",
            "It: 1870, Loss: 2.942e-03, Lambda_1: 0.713, Lambda_2: 0.017869, Time: 0.28\n",
            "It: 1880, Loss: 2.892e-03, Lambda_1: 0.716, Lambda_2: 0.018027, Time: 0.28\n",
            "It: 1890, Loss: 2.844e-03, Lambda_1: 0.720, Lambda_2: 0.018185, Time: 0.30\n",
            "It: 1900, Loss: 2.796e-03, Lambda_1: 0.723, Lambda_2: 0.018345, Time: 0.29\n",
            "It: 1910, Loss: 2.749e-03, Lambda_1: 0.726, Lambda_2: 0.018505, Time: 0.27\n",
            "It: 1920, Loss: 2.702e-03, Lambda_1: 0.729, Lambda_2: 0.018665, Time: 0.28\n",
            "It: 1930, Loss: 2.657e-03, Lambda_1: 0.732, Lambda_2: 0.018826, Time: 0.27\n",
            "It: 1940, Loss: 2.612e-03, Lambda_1: 0.735, Lambda_2: 0.018988, Time: 0.28\n",
            "It: 1950, Loss: 2.568e-03, Lambda_1: 0.739, Lambda_2: 0.019150, Time: 0.28\n",
            "It: 1960, Loss: 2.524e-03, Lambda_1: 0.742, Lambda_2: 0.019312, Time: 0.30\n",
            "It: 1970, Loss: 2.491e-03, Lambda_1: 0.745, Lambda_2: 0.019475, Time: 0.29\n",
            "It: 1980, Loss: 1.045e-02, Lambda_1: 0.747, Lambda_2: 0.019634, Time: 0.28\n",
            "It: 1990, Loss: 2.987e-03, Lambda_1: 0.748, Lambda_2: 0.019762, Time: 0.30\n",
            "Loss: 3.188226e-03, l1: 0.74628, l2: 0.01992\n",
            "Loss: 8.344599e+00, l1: 0.73614, l2: 0.02005\n",
            "Loss: 3.087616e-02, l1: 0.74613, l2: 0.01992\n",
            "Loss: 2.452334e-03, l1: 0.74626, l2: 0.01992\n",
            "Loss: 2.447130e-03, l1: 0.74625, l2: 0.01992\n",
            "Loss: 2.442665e-03, l1: 0.74622, l2: 0.01992\n",
            "Loss: 2.441508e-03, l1: 0.74621, l2: 0.01992\n",
            "Loss: 2.436828e-03, l1: 0.74612, l2: 0.01993\n",
            "Loss: 2.434602e-03, l1: 0.74609, l2: 0.01993\n",
            "Loss: 2.432087e-03, l1: 0.74607, l2: 0.01993\n",
            "Loss: 2.428409e-03, l1: 0.74608, l2: 0.01994\n",
            "Loss: 2.424706e-03, l1: 0.74612, l2: 0.01995\n",
            "Loss: 2.416050e-03, l1: 0.74623, l2: 0.01998\n",
            "Loss: 2.402744e-03, l1: 0.74643, l2: 0.02002\n",
            "Loss: 2.390904e-03, l1: 0.74693, l2: 0.02010\n",
            "Loss: 2.378664e-03, l1: 0.74706, l2: 0.02011\n",
            "Loss: 2.369666e-03, l1: 0.74738, l2: 0.02013\n",
            "Loss: 2.362679e-03, l1: 0.74782, l2: 0.02015\n",
            "Loss: 2.345022e-03, l1: 0.74943, l2: 0.02023\n",
            "Loss: 2.318852e-03, l1: 0.75274, l2: 0.02035\n",
            "Loss: 2.280049e-03, l1: 0.76042, l2: 0.02061\n",
            "Loss: 2.237848e-03, l1: 0.76854, l2: 0.02085\n",
            "Loss: 2.185289e-03, l1: 0.77722, l2: 0.02110\n",
            "Loss: 2.140829e-03, l1: 0.79539, l2: 0.02163\n",
            "Loss: 2.091055e-03, l1: 0.79704, l2: 0.02167\n",
            "Loss: 2.074691e-03, l1: 0.79721, l2: 0.02169\n",
            "Loss: 2.054853e-03, l1: 0.80011, l2: 0.02180\n",
            "Loss: 2.021055e-03, l1: 0.80531, l2: 0.02200\n",
            "Loss: 1.936771e-03, l1: 0.82233, l2: 0.02267\n",
            "Loss: 1.848098e-03, l1: 0.83714, l2: 0.02334\n",
            "Loss: 1.785047e-03, l1: 0.84948, l2: 0.02385\n",
            "Loss: 1.756528e-03, l1: 0.85061, l2: 0.02391\n",
            "Loss: 1.740985e-03, l1: 0.85387, l2: 0.02408\n",
            "Loss: 1.705184e-03, l1: 0.85833, l2: 0.02435\n",
            "Loss: 1.664209e-03, l1: 0.86646, l2: 0.02481\n",
            "Loss: 1.626302e-03, l1: 0.87078, l2: 0.02511\n",
            "Loss: 1.601797e-03, l1: 0.87420, l2: 0.02532\n",
            "Loss: 1.579071e-03, l1: 0.87396, l2: 0.02535\n",
            "Loss: 1.550776e-03, l1: 0.87686, l2: 0.02553\n",
            "Loss: 1.515410e-03, l1: 0.88107, l2: 0.02579\n",
            "Loss: 1.465274e-03, l1: 0.89105, l2: 0.02639\n",
            "Loss: 1.415662e-03, l1: 0.89970, l2: 0.02696\n",
            "Loss: 1.366975e-03, l1: 0.91028, l2: 0.02767\n",
            "Loss: 1.341477e-03, l1: 0.91331, l2: 0.02791\n",
            "Loss: 1.320317e-03, l1: 0.91563, l2: 0.02809\n",
            "Loss: 1.295410e-03, l1: 0.91570, l2: 0.02818\n",
            "Loss: 1.273077e-03, l1: 0.91658, l2: 0.02833\n",
            "Loss: 1.260129e-03, l1: 0.91799, l2: 0.02848\n",
            "Loss: 1.242855e-03, l1: 0.92013, l2: 0.02867\n",
            "Loss: 1.212011e-03, l1: 0.92877, l2: 0.02931\n",
            "Loss: 1.189037e-03, l1: 0.93228, l2: 0.02963\n",
            "Loss: 1.177266e-03, l1: 0.93992, l2: 0.03013\n",
            "Loss: 1.167368e-03, l1: 0.93991, l2: 0.03014\n",
            "Loss: 1.152542e-03, l1: 0.94033, l2: 0.03021\n",
            "Loss: 1.133956e-03, l1: 0.94216, l2: 0.03048\n",
            "Loss: 1.146643e-03, l1: 0.94069, l2: 0.03065\n",
            "Loss: 1.128156e-03, l1: 0.94164, l2: 0.03054\n",
            "Loss: 1.124277e-03, l1: 0.94074, l2: 0.03056\n",
            "Loss: 1.123126e-03, l1: 0.94012, l2: 0.03055\n",
            "Loss: 1.119211e-03, l1: 0.93850, l2: 0.03052\n",
            "Loss: 1.107254e-03, l1: 0.93343, l2: 0.03045\n",
            "Loss: 1.096919e-03, l1: 0.92828, l2: 0.03045\n",
            "Loss: 1.082756e-03, l1: 0.92611, l2: 0.03046\n",
            "Loss: 1.058793e-03, l1: 0.92673, l2: 0.03078\n",
            "Loss: 1.051040e-03, l1: 0.92920, l2: 0.03106\n",
            "Loss: 1.045880e-03, l1: 0.93148, l2: 0.03128\n",
            "Loss: 1.042907e-03, l1: 0.93309, l2: 0.03144\n",
            "Loss: 1.039088e-03, l1: 0.93295, l2: 0.03150\n",
            "Loss: 1.032992e-03, l1: 0.93284, l2: 0.03158\n",
            "Loss: 1.017189e-03, l1: 0.93317, l2: 0.03181\n",
            "Loss: 1.020389e-03, l1: 0.92520, l2: 0.03170\n",
            "Loss: 1.009785e-03, l1: 0.92956, l2: 0.03176\n",
            "Loss: 1.002643e-03, l1: 0.92958, l2: 0.03178\n",
            "Loss: 9.942313e-04, l1: 0.92878, l2: 0.03175\n",
            "Loss: 9.818557e-04, l1: 0.92758, l2: 0.03181\n",
            "Loss: 9.589636e-04, l1: 0.92472, l2: 0.03202\n",
            "Loss: 9.387462e-04, l1: 0.92064, l2: 0.03233\n",
            "Loss: 9.285682e-04, l1: 0.91861, l2: 0.03257\n",
            "Loss: 9.246868e-04, l1: 0.91819, l2: 0.03249\n",
            "Loss: 9.235101e-04, l1: 0.92000, l2: 0.03264\n",
            "Loss: 9.227954e-04, l1: 0.91974, l2: 0.03264\n",
            "Loss: 9.195239e-04, l1: 0.91953, l2: 0.03271\n",
            "Loss: 9.140846e-04, l1: 0.91954, l2: 0.03284\n",
            "Loss: 9.049990e-04, l1: 0.92008, l2: 0.03305\n",
            "Loss: 8.933513e-04, l1: 0.92056, l2: 0.03324\n",
            "Loss: 8.834044e-04, l1: 0.91628, l2: 0.03314\n",
            "Loss: 8.712017e-04, l1: 0.91775, l2: 0.03319\n",
            "Loss: 8.624061e-04, l1: 0.91685, l2: 0.03304\n",
            "Loss: 8.550867e-04, l1: 0.91581, l2: 0.03290\n",
            "Loss: 8.445043e-04, l1: 0.91406, l2: 0.03281\n",
            "Loss: 8.302617e-04, l1: 0.91170, l2: 0.03288\n",
            "Loss: 8.612805e-04, l1: 0.91347, l2: 0.03330\n",
            "Loss: 8.262573e-04, l1: 0.91215, l2: 0.03299\n",
            "Loss: 8.134205e-04, l1: 0.90936, l2: 0.03323\n",
            "Loss: 8.061274e-04, l1: 0.90928, l2: 0.03355\n",
            "Loss: 8.010669e-04, l1: 0.90776, l2: 0.03377\n",
            "Loss: 7.978977e-04, l1: 0.90800, l2: 0.03404\n",
            "Loss: 7.945659e-04, l1: 0.90587, l2: 0.03403\n",
            "Loss: 7.894394e-04, l1: 0.90301, l2: 0.03405\n",
            "Loss: 7.824377e-04, l1: 0.89913, l2: 0.03370\n",
            "Loss: 7.758603e-04, l1: 0.89575, l2: 0.03344\n",
            "Loss: 7.722296e-04, l1: 0.89503, l2: 0.03318\n",
            "Loss: 7.699134e-04, l1: 0.89461, l2: 0.03307\n",
            "Loss: 7.671235e-04, l1: 0.89358, l2: 0.03298\n",
            "Loss: 7.634746e-04, l1: 0.89217, l2: 0.03292\n",
            "Loss: 7.577324e-04, l1: 0.88996, l2: 0.03290\n",
            "Loss: 7.529827e-04, l1: 0.88827, l2: 0.03297\n",
            "Loss: 7.472860e-04, l1: 0.88907, l2: 0.03306\n",
            "Loss: 7.401032e-04, l1: 0.89248, l2: 0.03332\n",
            "Loss: 7.340816e-04, l1: 0.89563, l2: 0.03357\n",
            "Loss: 7.271162e-04, l1: 0.89763, l2: 0.03376\n",
            "Loss: 9.789063e-04, l1: 0.93135, l2: 0.03615\n",
            "Loss: 7.248598e-04, l1: 0.90063, l2: 0.03396\n",
            "Loss: 7.210873e-04, l1: 0.89985, l2: 0.03393\n",
            "Loss: 7.168460e-04, l1: 0.89859, l2: 0.03391\n",
            "Loss: 7.153297e-04, l1: 0.89736, l2: 0.03377\n",
            "Loss: 7.139987e-04, l1: 0.89688, l2: 0.03377\n",
            "Loss: 7.127487e-04, l1: 0.89679, l2: 0.03373\n",
            "Loss: 7.113087e-04, l1: 0.89726, l2: 0.03374\n",
            "Loss: 7.095097e-04, l1: 0.89810, l2: 0.03368\n",
            "Loss: 7.063830e-04, l1: 0.89916, l2: 0.03366\n",
            "Loss: 6.994392e-04, l1: 0.90126, l2: 0.03369\n",
            "Loss: 6.888802e-04, l1: 0.90153, l2: 0.03372\n",
            "Loss: 6.792622e-04, l1: 0.91888, l2: 0.03404\n",
            "Loss: 6.509794e-04, l1: 0.90766, l2: 0.03390\n",
            "Loss: 6.301004e-04, l1: 0.90063, l2: 0.03366\n",
            "Loss: 6.134178e-04, l1: 0.90301, l2: 0.03400\n",
            "Loss: 6.081518e-04, l1: 0.90358, l2: 0.03397\n",
            "Loss: 6.061124e-04, l1: 0.90663, l2: 0.03397\n",
            "Loss: 6.042966e-04, l1: 0.90728, l2: 0.03399\n",
            "Loss: 6.012631e-04, l1: 0.90876, l2: 0.03400\n",
            "Loss: 5.993228e-04, l1: 0.91063, l2: 0.03403\n",
            "Loss: 5.980926e-04, l1: 0.91128, l2: 0.03403\n",
            "Loss: 5.965118e-04, l1: 0.91171, l2: 0.03397\n",
            "Loss: 5.972217e-04, l1: 0.91171, l2: 0.03391\n",
            "Loss: 5.953458e-04, l1: 0.91171, l2: 0.03394\n",
            "Loss: 5.933385e-04, l1: 0.91181, l2: 0.03388\n",
            "Loss: 5.898850e-04, l1: 0.91223, l2: 0.03376\n",
            "Loss: 5.871025e-04, l1: 0.91313, l2: 0.03367\n",
            "Loss: 5.836816e-04, l1: 0.91453, l2: 0.03362\n",
            "Loss: 5.779996e-04, l1: 0.91617, l2: 0.03359\n",
            "Loss: 5.691199e-04, l1: 0.91809, l2: 0.03345\n",
            "Loss: 5.595923e-04, l1: 0.91864, l2: 0.03346\n",
            "Loss: 5.458476e-04, l1: 0.92021, l2: 0.03356\n",
            "Loss: 5.417670e-04, l1: 0.91759, l2: 0.03348\n",
            "Loss: 5.351823e-04, l1: 0.91987, l2: 0.03359\n",
            "Loss: 5.339275e-04, l1: 0.92044, l2: 0.03365\n",
            "Loss: 5.323283e-04, l1: 0.92176, l2: 0.03377\n",
            "Loss: 5.307777e-04, l1: 0.92306, l2: 0.03381\n",
            "Loss: 5.351549e-04, l1: 0.92692, l2: 0.03412\n",
            "Loss: 5.290501e-04, l1: 0.92440, l2: 0.03392\n",
            "Loss: 5.244992e-04, l1: 0.92548, l2: 0.03397\n",
            "Loss: 5.123066e-04, l1: 0.92796, l2: 0.03385\n",
            "Loss: 5.040208e-04, l1: 0.92662, l2: 0.03358\n",
            "Loss: 4.941899e-04, l1: 0.92549, l2: 0.03324\n",
            "Loss: 4.898991e-04, l1: 0.92518, l2: 0.03286\n",
            "Loss: 4.832002e-04, l1: 0.92530, l2: 0.03288\n",
            "Loss: 4.784709e-04, l1: 0.92689, l2: 0.03297\n",
            "Loss: 4.755327e-04, l1: 0.92829, l2: 0.03304\n",
            "Loss: 4.738529e-04, l1: 0.93033, l2: 0.03327\n",
            "Loss: 4.695767e-04, l1: 0.93039, l2: 0.03319\n",
            "Loss: 4.675415e-04, l1: 0.93145, l2: 0.03324\n",
            "Loss: 4.649899e-04, l1: 0.93246, l2: 0.03330\n",
            "Loss: 4.614124e-04, l1: 0.93445, l2: 0.03343\n",
            "Loss: 4.529612e-04, l1: 0.93579, l2: 0.03349\n",
            "Loss: 4.427266e-04, l1: 0.94070, l2: 0.03366\n",
            "Loss: 4.309281e-04, l1: 0.94443, l2: 0.03372\n",
            "Loss: 4.304599e-04, l1: 0.93762, l2: 0.03327\n",
            "Loss: 4.256619e-04, l1: 0.94096, l2: 0.03349\n",
            "Loss: 4.177440e-04, l1: 0.93972, l2: 0.03340\n",
            "Loss: 4.108454e-04, l1: 0.93825, l2: 0.03333\n",
            "Loss: 4.054999e-04, l1: 0.93812, l2: 0.03320\n",
            "Loss: 4.013097e-04, l1: 0.93627, l2: 0.03321\n",
            "Loss: 3.982931e-04, l1: 0.93726, l2: 0.03318\n",
            "Loss: 3.924635e-04, l1: 0.93815, l2: 0.03308\n",
            "Loss: 3.910013e-04, l1: 0.93833, l2: 0.03301\n",
            "Loss: 3.864736e-04, l1: 0.93985, l2: 0.03299\n",
            "Loss: 3.847222e-04, l1: 0.93796, l2: 0.03299\n",
            "Loss: 3.832586e-04, l1: 0.93649, l2: 0.03299\n",
            "Loss: 3.811763e-04, l1: 0.93442, l2: 0.03314\n",
            "Loss: 3.759693e-04, l1: 0.93340, l2: 0.03301\n",
            "Loss: 3.726816e-04, l1: 0.93642, l2: 0.03309\n",
            "Loss: 3.721424e-04, l1: 0.93776, l2: 0.03326\n",
            "Loss: 3.695516e-04, l1: 0.94006, l2: 0.03331\n",
            "Loss: 3.685692e-04, l1: 0.94005, l2: 0.03332\n",
            "Loss: 3.658358e-04, l1: 0.94037, l2: 0.03336\n",
            "Loss: 3.628834e-04, l1: 0.94101, l2: 0.03340\n",
            "Loss: 3.613921e-04, l1: 0.94262, l2: 0.03348\n",
            "Loss: 3.520715e-04, l1: 0.94155, l2: 0.03338\n",
            "Loss: 3.445633e-04, l1: 0.94110, l2: 0.03324\n",
            "Loss: 3.340497e-04, l1: 0.93884, l2: 0.03298\n",
            "Loss: 3.266082e-04, l1: 0.93789, l2: 0.03259\n",
            "Loss: 3.201733e-04, l1: 0.93572, l2: 0.03249\n",
            "Loss: 3.166292e-04, l1: 0.93683, l2: 0.03256\n",
            "Loss: 3.137568e-04, l1: 0.93838, l2: 0.03253\n",
            "Loss: 3.108188e-04, l1: 0.93974, l2: 0.03260\n",
            "Loss: 3.063192e-04, l1: 0.94120, l2: 0.03270\n",
            "Loss: 3.015101e-04, l1: 0.94337, l2: 0.03287\n",
            "Loss: 2.978031e-04, l1: 0.94440, l2: 0.03292\n",
            "Loss: 2.955973e-04, l1: 0.94482, l2: 0.03296\n",
            "Loss: 2.930364e-04, l1: 0.94479, l2: 0.03291\n",
            "Loss: 2.891304e-04, l1: 0.94532, l2: 0.03293\n",
            "Loss: 2.841892e-04, l1: 0.94523, l2: 0.03289\n",
            "Loss: 2.801686e-04, l1: 0.94566, l2: 0.03283\n",
            "Loss: 2.742844e-04, l1: 0.94685, l2: 0.03275\n",
            "Loss: 2.663762e-04, l1: 0.94921, l2: 0.03259\n",
            "Loss: 2.589983e-04, l1: 0.95308, l2: 0.03262\n",
            "Loss: 2.460539e-04, l1: 0.95785, l2: 0.03243\n",
            "Loss: 2.384050e-04, l1: 0.95796, l2: 0.03243\n",
            "Loss: 2.340260e-04, l1: 0.95846, l2: 0.03233\n",
            "Loss: 2.318166e-04, l1: 0.95641, l2: 0.03220\n",
            "Loss: 2.308477e-04, l1: 0.95501, l2: 0.03210\n",
            "Loss: 2.301353e-04, l1: 0.95509, l2: 0.03198\n",
            "Loss: 2.293597e-04, l1: 0.95604, l2: 0.03200\n",
            "Loss: 2.286028e-04, l1: 0.95777, l2: 0.03197\n",
            "Loss: 2.283611e-04, l1: 0.95895, l2: 0.03193\n",
            "Loss: 2.269692e-04, l1: 0.95931, l2: 0.03190\n",
            "Loss: 2.257247e-04, l1: 0.95936, l2: 0.03185\n",
            "Loss: 2.241159e-04, l1: 0.95888, l2: 0.03180\n",
            "Loss: 2.223785e-04, l1: 0.95815, l2: 0.03176\n",
            "Loss: 2.200520e-04, l1: 0.95918, l2: 0.03185\n",
            "Loss: 2.173845e-04, l1: 0.96076, l2: 0.03190\n",
            "Loss: 2.138580e-04, l1: 0.96333, l2: 0.03205\n",
            "Loss: 2.348007e-04, l1: 0.95637, l2: 0.03180\n",
            "Loss: 2.115756e-04, l1: 0.96169, l2: 0.03199\n",
            "Loss: 2.087737e-04, l1: 0.96336, l2: 0.03196\n",
            "Loss: 2.048291e-04, l1: 0.96423, l2: 0.03196\n",
            "Loss: 2.020718e-04, l1: 0.96483, l2: 0.03188\n",
            "Loss: 1.993678e-04, l1: 0.96502, l2: 0.03189\n",
            "Loss: 1.960710e-04, l1: 0.96740, l2: 0.03198\n",
            "Loss: 1.933967e-04, l1: 0.96874, l2: 0.03201\n",
            "Loss: 1.908967e-04, l1: 0.96982, l2: 0.03212\n",
            "Loss: 1.886223e-04, l1: 0.97032, l2: 0.03219\n",
            "Loss: 1.872257e-04, l1: 0.96959, l2: 0.03221\n",
            "Loss: 1.861860e-04, l1: 0.96931, l2: 0.03219\n",
            "Loss: 1.851466e-04, l1: 0.96921, l2: 0.03213\n",
            "Loss: 1.833686e-04, l1: 0.97055, l2: 0.03209\n",
            "Loss: 1.822063e-04, l1: 0.97341, l2: 0.03199\n",
            "Loss: 1.816234e-04, l1: 0.97560, l2: 0.03202\n",
            "Loss: 1.798789e-04, l1: 0.97495, l2: 0.03202\n",
            "Loss: 1.783730e-04, l1: 0.97468, l2: 0.03204\n",
            "Loss: 1.772144e-04, l1: 0.97477, l2: 0.03202\n",
            "Loss: 1.762525e-04, l1: 0.97460, l2: 0.03199\n",
            "Loss: 1.750865e-04, l1: 0.97423, l2: 0.03197\n",
            "Loss: 1.744196e-04, l1: 0.97511, l2: 0.03200\n",
            "Loss: 1.735675e-04, l1: 0.97507, l2: 0.03193\n",
            "Loss: 1.724952e-04, l1: 0.97561, l2: 0.03193\n",
            "Loss: 1.711153e-04, l1: 0.97652, l2: 0.03192\n",
            "Loss: 1.695093e-04, l1: 0.97764, l2: 0.03194\n",
            "Loss: 1.680060e-04, l1: 0.97909, l2: 0.03193\n",
            "Loss: 1.665265e-04, l1: 0.97991, l2: 0.03203\n",
            "Loss: 1.651216e-04, l1: 0.98005, l2: 0.03204\n",
            "Loss: 1.635818e-04, l1: 0.97979, l2: 0.03208\n",
            "Loss: 1.625941e-04, l1: 0.98033, l2: 0.03210\n",
            "Loss: 1.614967e-04, l1: 0.98116, l2: 0.03220\n",
            "Loss: 1.601988e-04, l1: 0.98179, l2: 0.03214\n",
            "Loss: 1.591201e-04, l1: 0.98207, l2: 0.03210\n",
            "Loss: 1.576527e-04, l1: 0.98214, l2: 0.03206\n",
            "Loss: 1.555839e-04, l1: 0.98228, l2: 0.03199\n",
            "Loss: 1.533278e-04, l1: 0.98076, l2: 0.03196\n",
            "Loss: 1.517208e-04, l1: 0.97882, l2: 0.03189\n",
            "Loss: 1.509005e-04, l1: 0.97746, l2: 0.03191\n",
            "Loss: 1.504081e-04, l1: 0.97704, l2: 0.03193\n",
            "Loss: 1.499840e-04, l1: 0.97682, l2: 0.03190\n",
            "Loss: 1.494612e-04, l1: 0.97617, l2: 0.03185\n",
            "Loss: 1.490186e-04, l1: 0.97576, l2: 0.03178\n",
            "Loss: 1.491439e-04, l1: 0.97424, l2: 0.03162\n",
            "Loss: 1.487708e-04, l1: 0.97508, l2: 0.03170\n",
            "Loss: 1.484792e-04, l1: 0.97496, l2: 0.03168\n",
            "Loss: 1.482212e-04, l1: 0.97483, l2: 0.03165\n",
            "Loss: 1.478572e-04, l1: 0.97427, l2: 0.03164\n",
            "Loss: 1.471206e-04, l1: 0.97315, l2: 0.03164\n",
            "Loss: 1.463990e-04, l1: 0.97268, l2: 0.03164\n",
            "Loss: 1.454703e-04, l1: 0.97250, l2: 0.03166\n",
            "Loss: 1.444134e-04, l1: 0.97333, l2: 0.03169\n",
            "Loss: 1.432882e-04, l1: 0.97482, l2: 0.03173\n",
            "Loss: 1.427020e-04, l1: 0.97467, l2: 0.03173\n",
            "Loss: 1.414419e-04, l1: 0.97507, l2: 0.03181\n",
            "Loss: 1.402992e-04, l1: 0.97428, l2: 0.03179\n",
            "Loss: 1.390098e-04, l1: 0.97380, l2: 0.03181\n",
            "Loss: 1.380625e-04, l1: 0.97427, l2: 0.03184\n",
            "Loss: 1.366533e-04, l1: 0.97610, l2: 0.03192\n",
            "Loss: 1.382795e-04, l1: 0.97876, l2: 0.03195\n",
            "Loss: 1.362290e-04, l1: 0.97693, l2: 0.03193\n",
            "Loss: 1.353630e-04, l1: 0.97792, l2: 0.03196\n",
            "Loss: 1.346043e-04, l1: 0.97856, l2: 0.03194\n",
            "Loss: 1.341358e-04, l1: 0.97849, l2: 0.03190\n",
            "Loss: 1.351678e-04, l1: 0.97752, l2: 0.03174\n",
            "Loss: 1.338498e-04, l1: 0.97818, l2: 0.03185\n",
            "Loss: 1.333869e-04, l1: 0.97778, l2: 0.03180\n",
            "Loss: 1.330362e-04, l1: 0.97756, l2: 0.03175\n",
            "Loss: 1.328227e-04, l1: 0.97792, l2: 0.03174\n",
            "Loss: 1.327320e-04, l1: 0.97822, l2: 0.03175\n",
            "Loss: 1.326593e-04, l1: 0.97832, l2: 0.03175\n",
            "Loss: 1.323079e-04, l1: 0.97832, l2: 0.03172\n",
            "Loss: 1.319878e-04, l1: 0.97772, l2: 0.03165\n",
            "Loss: 1.313001e-04, l1: 0.97737, l2: 0.03163\n",
            "Loss: 1.307659e-04, l1: 0.97692, l2: 0.03161\n",
            "Loss: 1.302194e-04, l1: 0.97711, l2: 0.03158\n",
            "Loss: 1.298715e-04, l1: 0.97633, l2: 0.03154\n",
            "Loss: 1.294819e-04, l1: 0.97707, l2: 0.03154\n",
            "Loss: 1.289209e-04, l1: 0.97835, l2: 0.03155\n",
            "Loss: 1.283627e-04, l1: 0.97920, l2: 0.03153\n",
            "Loss: 1.282899e-04, l1: 0.98109, l2: 0.03156\n",
            "Loss: 1.278371e-04, l1: 0.98018, l2: 0.03154\n",
            "Loss: 1.270799e-04, l1: 0.97992, l2: 0.03149\n",
            "Loss: 1.262893e-04, l1: 0.97898, l2: 0.03148\n",
            "Loss: 1.259136e-04, l1: 0.97737, l2: 0.03136\n",
            "Loss: 1.252678e-04, l1: 0.97756, l2: 0.03143\n",
            "Loss: 1.248002e-04, l1: 0.97800, l2: 0.03148\n",
            "Loss: 1.242911e-04, l1: 0.97809, l2: 0.03152\n",
            "Loss: 1.237538e-04, l1: 0.97826, l2: 0.03153\n",
            "Loss: 1.254282e-04, l1: 0.97792, l2: 0.03157\n",
            "Loss: 1.232868e-04, l1: 0.97815, l2: 0.03155\n",
            "Loss: 1.218851e-04, l1: 0.97883, l2: 0.03168\n",
            "Loss: 1.208903e-04, l1: 0.97898, l2: 0.03171\n",
            "Loss: 1.201750e-04, l1: 0.97828, l2: 0.03175\n",
            "Loss: 1.198857e-04, l1: 0.97817, l2: 0.03181\n",
            "Loss: 1.197100e-04, l1: 0.97828, l2: 0.03185\n",
            "Loss: 1.196959e-04, l1: 0.97867, l2: 0.03186\n",
            "Loss: 1.196187e-04, l1: 0.97849, l2: 0.03185\n",
            "Loss: 1.194989e-04, l1: 0.97879, l2: 0.03187\n",
            "Loss: 1.194222e-04, l1: 0.97893, l2: 0.03187\n",
            "Loss: 1.193029e-04, l1: 0.97903, l2: 0.03187\n",
            "Loss: 1.192941e-04, l1: 0.97871, l2: 0.03183\n",
            "Loss: 1.192423e-04, l1: 0.97886, l2: 0.03185\n",
            "Loss: 1.191623e-04, l1: 0.97875, l2: 0.03185\n",
            "Loss: 1.190272e-04, l1: 0.97848, l2: 0.03188\n",
            "Loss: 1.189122e-04, l1: 0.97828, l2: 0.03190\n",
            "Loss: 1.186298e-04, l1: 0.97801, l2: 0.03192\n",
            "Loss: 1.184155e-04, l1: 0.97752, l2: 0.03197\n",
            "Loss: 1.176935e-04, l1: 0.97751, l2: 0.03196\n",
            "Loss: 1.171451e-04, l1: 0.97746, l2: 0.03192\n",
            "Loss: 1.158731e-04, l1: 0.97701, l2: 0.03182\n",
            "Loss: 1.151974e-04, l1: 0.97710, l2: 0.03175\n",
            "Loss: 1.144678e-04, l1: 0.97747, l2: 0.03177\n",
            "Loss: 1.134161e-04, l1: 0.97866, l2: 0.03181\n",
            "Loss: 1.127703e-04, l1: 0.97970, l2: 0.03184\n",
            "Loss: 1.119990e-04, l1: 0.98152, l2: 0.03186\n",
            "Loss: 1.115195e-04, l1: 0.98261, l2: 0.03186\n",
            "Loss: 1.112216e-04, l1: 0.98307, l2: 0.03185\n",
            "Loss: 1.109513e-04, l1: 0.98326, l2: 0.03185\n",
            "Loss: 1.106989e-04, l1: 0.98359, l2: 0.03185\n",
            "Loss: 1.104472e-04, l1: 0.98363, l2: 0.03187\n",
            "Loss: 1.104314e-04, l1: 0.98389, l2: 0.03194\n",
            "Loss: 1.103542e-04, l1: 0.98377, l2: 0.03191\n",
            "Loss: 1.102065e-04, l1: 0.98405, l2: 0.03194\n",
            "Loss: 1.099919e-04, l1: 0.98398, l2: 0.03193\n",
            "Loss: 1.098409e-04, l1: 0.98474, l2: 0.03192\n",
            "Loss: 1.096895e-04, l1: 0.98463, l2: 0.03193\n",
            "Loss: 1.096182e-04, l1: 0.98465, l2: 0.03193\n",
            "Loss: 1.094047e-04, l1: 0.98442, l2: 0.03191\n",
            "Loss: 1.091504e-04, l1: 0.98447, l2: 0.03190\n",
            "Loss: 1.089018e-04, l1: 0.98453, l2: 0.03188\n",
            "Loss: 1.085938e-04, l1: 0.98463, l2: 0.03186\n",
            "Loss: 1.082202e-04, l1: 0.98469, l2: 0.03183\n",
            "Loss: 1.075015e-04, l1: 0.98504, l2: 0.03185\n",
            "Loss: 1.067278e-04, l1: 0.98505, l2: 0.03175\n",
            "Loss: 1.057401e-04, l1: 0.98548, l2: 0.03186\n",
            "Loss: 1.052618e-04, l1: 0.98632, l2: 0.03193\n",
            "Loss: 1.049291e-04, l1: 0.98640, l2: 0.03194\n",
            "Loss: 1.044975e-04, l1: 0.98677, l2: 0.03194\n",
            "Loss: 1.037933e-04, l1: 0.98777, l2: 0.03192\n",
            "Loss: 1.033419e-04, l1: 0.98809, l2: 0.03189\n",
            "Loss: 1.038390e-04, l1: 0.98879, l2: 0.03191\n",
            "Loss: 1.030444e-04, l1: 0.98836, l2: 0.03190\n",
            "Loss: 1.026485e-04, l1: 0.98827, l2: 0.03188\n",
            "Loss: 1.021485e-04, l1: 0.98778, l2: 0.03185\n",
            "Loss: 1.016408e-04, l1: 0.98771, l2: 0.03186\n",
            "Loss: 1.010279e-04, l1: 0.98661, l2: 0.03188\n",
            "Loss: 1.005537e-04, l1: 0.98677, l2: 0.03192\n",
            "Loss: 1.001756e-04, l1: 0.98631, l2: 0.03195\n",
            "Loss: 9.968755e-05, l1: 0.98535, l2: 0.03197\n",
            "Loss: 9.934128e-05, l1: 0.98465, l2: 0.03198\n",
            "Loss: 9.892831e-05, l1: 0.98369, l2: 0.03199\n",
            "Loss: 9.902236e-05, l1: 0.98285, l2: 0.03201\n",
            "Loss: 9.881433e-05, l1: 0.98333, l2: 0.03200\n",
            "Loss: 9.884025e-05, l1: 0.98373, l2: 0.03198\n",
            "Loss: 9.866989e-05, l1: 0.98352, l2: 0.03199\n",
            "Loss: 9.856171e-05, l1: 0.98347, l2: 0.03199\n",
            "Loss: 9.848579e-05, l1: 0.98363, l2: 0.03199\n",
            "Loss: 9.839066e-05, l1: 0.98383, l2: 0.03199\n",
            "Loss: 9.827064e-05, l1: 0.98425, l2: 0.03200\n",
            "Loss: 9.815152e-05, l1: 0.98430, l2: 0.03200\n",
            "Loss: 9.801335e-05, l1: 0.98425, l2: 0.03200\n",
            "Loss: 9.803206e-05, l1: 0.98357, l2: 0.03197\n",
            "Loss: 9.795278e-05, l1: 0.98393, l2: 0.03199\n",
            "Loss: 9.787481e-05, l1: 0.98387, l2: 0.03197\n",
            "Loss: 9.778045e-05, l1: 0.98363, l2: 0.03197\n",
            "Loss: 9.766628e-05, l1: 0.98334, l2: 0.03197\n",
            "Loss: 9.753335e-05, l1: 0.98305, l2: 0.03197\n",
            "Loss: 9.736262e-05, l1: 0.98243, l2: 0.03194\n",
            "Loss: 9.717558e-05, l1: 0.98278, l2: 0.03196\n",
            "Loss: 9.691207e-05, l1: 0.98354, l2: 0.03197\n",
            "Loss: 9.673529e-05, l1: 0.98391, l2: 0.03197\n",
            "Loss: 9.644427e-05, l1: 0.98429, l2: 0.03197\n",
            "Loss: 9.620433e-05, l1: 0.98424, l2: 0.03198\n",
            "Loss: 9.586685e-05, l1: 0.98387, l2: 0.03199\n",
            "Loss: 9.589401e-05, l1: 0.98279, l2: 0.03211\n",
            "Loss: 9.560195e-05, l1: 0.98335, l2: 0.03205\n",
            "Loss: 9.505622e-05, l1: 0.98280, l2: 0.03208\n",
            "Loss: 9.435412e-05, l1: 0.98224, l2: 0.03213\n",
            "Loss: 9.381331e-05, l1: 0.98222, l2: 0.03220\n",
            "Loss: 9.335372e-05, l1: 0.98216, l2: 0.03222\n",
            "Loss: 9.304871e-05, l1: 0.98239, l2: 0.03230\n",
            "Loss: 9.240402e-05, l1: 0.98301, l2: 0.03224\n",
            "Loss: 9.203040e-05, l1: 0.98293, l2: 0.03221\n",
            "Loss: 9.156185e-05, l1: 0.98278, l2: 0.03216\n",
            "Loss: 9.104329e-05, l1: 0.98249, l2: 0.03210\n",
            "Loss: 9.044854e-05, l1: 0.98201, l2: 0.03205\n",
            "Loss: 9.059054e-05, l1: 0.98073, l2: 0.03204\n",
            "Loss: 9.022537e-05, l1: 0.98145, l2: 0.03204\n",
            "Loss: 8.992552e-05, l1: 0.98107, l2: 0.03204\n",
            "Loss: 9.044229e-05, l1: 0.98103, l2: 0.03218\n",
            "Loss: 8.980490e-05, l1: 0.98106, l2: 0.03208\n",
            "Loss: 8.965540e-05, l1: 0.98100, l2: 0.03208\n",
            "Loss: 8.948683e-05, l1: 0.98123, l2: 0.03213\n",
            "Loss: 8.936440e-05, l1: 0.98168, l2: 0.03218\n",
            "Loss: 8.921100e-05, l1: 0.98212, l2: 0.03221\n",
            "Loss: 8.905308e-05, l1: 0.98246, l2: 0.03224\n",
            "Loss: 8.897115e-05, l1: 0.98216, l2: 0.03226\n",
            "Loss: 8.883650e-05, l1: 0.98234, l2: 0.03224\n",
            "Loss: 8.875583e-05, l1: 0.98232, l2: 0.03222\n",
            "Loss: 8.869560e-05, l1: 0.98236, l2: 0.03221\n",
            "Loss: 8.862193e-05, l1: 0.98258, l2: 0.03222\n",
            "Loss: 8.845347e-05, l1: 0.98334, l2: 0.03224\n",
            "Loss: 8.830796e-05, l1: 0.98423, l2: 0.03227\n",
            "Loss: 8.809658e-05, l1: 0.98443, l2: 0.03229\n",
            "Loss: 8.781369e-05, l1: 0.98458, l2: 0.03228\n",
            "Loss: 8.767869e-05, l1: 0.98470, l2: 0.03228\n",
            "Loss: 8.758849e-05, l1: 0.98465, l2: 0.03226\n",
            "Loss: 8.759052e-05, l1: 0.98524, l2: 0.03228\n",
            "Loss: 8.751566e-05, l1: 0.98495, l2: 0.03227\n",
            "Loss: 8.740870e-05, l1: 0.98493, l2: 0.03226\n",
            "Loss: 8.717938e-05, l1: 0.98439, l2: 0.03220\n",
            "Loss: 8.690928e-05, l1: 0.98451, l2: 0.03225\n",
            "Loss: 8.654179e-05, l1: 0.98509, l2: 0.03229\n",
            "Loss: 8.620867e-05, l1: 0.98566, l2: 0.03231\n",
            "Loss: 8.568758e-05, l1: 0.98643, l2: 0.03230\n",
            "Loss: 8.532805e-05, l1: 0.98650, l2: 0.03227\n",
            "Loss: 8.485267e-05, l1: 0.98658, l2: 0.03223\n",
            "Loss: 8.427184e-05, l1: 0.98631, l2: 0.03220\n",
            "Loss: 8.413199e-05, l1: 0.98774, l2: 0.03221\n",
            "Loss: 8.370299e-05, l1: 0.98706, l2: 0.03220\n",
            "Loss: 8.278259e-05, l1: 0.98636, l2: 0.03217\n",
            "Loss: 8.218200e-05, l1: 0.98634, l2: 0.03220\n",
            "Loss: 8.161107e-05, l1: 0.98681, l2: 0.03223\n",
            "Loss: 8.123588e-05, l1: 0.98753, l2: 0.03228\n",
            "Loss: 8.077229e-05, l1: 0.98802, l2: 0.03227\n",
            "Loss: 8.017652e-05, l1: 0.98878, l2: 0.03223\n",
            "Loss: 7.983750e-05, l1: 0.98834, l2: 0.03215\n",
            "Loss: 8.036756e-05, l1: 0.98942, l2: 0.03208\n",
            "Loss: 7.954339e-05, l1: 0.98875, l2: 0.03212\n",
            "Loss: 7.917224e-05, l1: 0.98867, l2: 0.03213\n",
            "Loss: 7.887284e-05, l1: 0.98812, l2: 0.03211\n",
            "Loss: 7.869647e-05, l1: 0.98774, l2: 0.03210\n",
            "Loss: 7.838863e-05, l1: 0.98715, l2: 0.03206\n",
            "Loss: 7.828421e-05, l1: 0.98648, l2: 0.03207\n",
            "Loss: 7.819496e-05, l1: 0.98682, l2: 0.03207\n",
            "Loss: 7.813813e-05, l1: 0.98701, l2: 0.03208\n",
            "Loss: 7.805131e-05, l1: 0.98724, l2: 0.03210\n",
            "Loss: 7.796250e-05, l1: 0.98724, l2: 0.03213\n",
            "Loss: 7.791523e-05, l1: 0.98782, l2: 0.03215\n",
            "Loss: 7.776934e-05, l1: 0.98744, l2: 0.03214\n",
            "Loss: 7.767697e-05, l1: 0.98721, l2: 0.03213\n",
            "Loss: 7.758911e-05, l1: 0.98713, l2: 0.03212\n",
            "Loss: 7.741149e-05, l1: 0.98717, l2: 0.03209\n",
            "Loss: 7.713689e-05, l1: 0.98724, l2: 0.03206\n",
            "Loss: 7.705073e-05, l1: 0.98776, l2: 0.03202\n",
            "Loss: 7.684032e-05, l1: 0.98756, l2: 0.03203\n",
            "Loss: 7.671550e-05, l1: 0.98748, l2: 0.03204\n",
            "Loss: 7.653195e-05, l1: 0.98729, l2: 0.03205\n",
            "Loss: 7.632843e-05, l1: 0.98767, l2: 0.03210\n",
            "Loss: 7.595346e-05, l1: 0.98723, l2: 0.03208\n",
            "Loss: 7.549651e-05, l1: 0.98729, l2: 0.03207\n",
            "Loss: 7.475221e-05, l1: 0.98821, l2: 0.03205\n",
            "Loss: 7.508676e-05, l1: 0.98661, l2: 0.03208\n",
            "Loss: 7.456544e-05, l1: 0.98761, l2: 0.03206\n",
            "Loss: 7.391661e-05, l1: 0.98806, l2: 0.03205\n",
            "Loss: 7.374252e-05, l1: 0.98859, l2: 0.03209\n",
            "Loss: 7.289187e-05, l1: 0.98846, l2: 0.03204\n",
            "Loss: 7.272096e-05, l1: 0.98841, l2: 0.03203\n",
            "Loss: 7.252578e-05, l1: 0.98848, l2: 0.03202\n",
            "Loss: 7.223991e-05, l1: 0.98901, l2: 0.03201\n",
            "Loss: 7.210924e-05, l1: 0.98925, l2: 0.03201\n",
            "Loss: 7.167960e-05, l1: 0.99013, l2: 0.03206\n",
            "Loss: 7.223208e-05, l1: 0.99003, l2: 0.03208\n",
            "Loss: 7.158407e-05, l1: 0.99010, l2: 0.03207\n",
            "Loss: 7.138295e-05, l1: 0.99022, l2: 0.03211\n",
            "Loss: 7.125925e-05, l1: 0.99022, l2: 0.03212\n",
            "Loss: 7.107220e-05, l1: 0.98980, l2: 0.03212\n",
            "Loss: 7.085121e-05, l1: 0.98935, l2: 0.03210\n",
            "Loss: 7.060357e-05, l1: 0.98883, l2: 0.03207\n",
            "Loss: 7.039103e-05, l1: 0.98852, l2: 0.03203\n",
            "Loss: 7.028866e-05, l1: 0.98822, l2: 0.03197\n",
            "Loss: 6.991655e-05, l1: 0.98799, l2: 0.03192\n",
            "Loss: 6.966638e-05, l1: 0.98790, l2: 0.03191\n",
            "Loss: 6.942647e-05, l1: 0.98796, l2: 0.03190\n",
            "Loss: 6.917587e-05, l1: 0.98782, l2: 0.03187\n",
            "Loss: 6.904537e-05, l1: 0.98776, l2: 0.03185\n",
            "Loss: 6.876249e-05, l1: 0.98775, l2: 0.03184\n",
            "Loss: 6.840251e-05, l1: 0.98792, l2: 0.03184\n",
            "Loss: 6.804036e-05, l1: 0.98783, l2: 0.03185\n",
            "Loss: 6.794102e-05, l1: 0.98842, l2: 0.03181\n",
            "Loss: 6.738780e-05, l1: 0.98792, l2: 0.03183\n",
            "Loss: 6.715715e-05, l1: 0.98762, l2: 0.03184\n",
            "Loss: 6.768454e-05, l1: 0.98762, l2: 0.03186\n",
            "Loss: 6.712127e-05, l1: 0.98762, l2: 0.03184\n",
            "Loss: 6.700073e-05, l1: 0.98735, l2: 0.03183\n",
            "Loss: 6.692992e-05, l1: 0.98719, l2: 0.03182\n",
            "Loss: 6.680035e-05, l1: 0.98694, l2: 0.03179\n",
            "Loss: 6.671486e-05, l1: 0.98659, l2: 0.03175\n",
            "Loss: 6.656394e-05, l1: 0.98658, l2: 0.03175\n",
            "Loss: 6.639448e-05, l1: 0.98656, l2: 0.03176\n",
            "Loss: 6.624154e-05, l1: 0.98632, l2: 0.03176\n",
            "Loss: 6.609019e-05, l1: 0.98629, l2: 0.03178\n",
            "Loss: 6.596123e-05, l1: 0.98614, l2: 0.03178\n",
            "Loss: 6.582894e-05, l1: 0.98607, l2: 0.03178\n",
            "Loss: 6.558676e-05, l1: 0.98592, l2: 0.03179\n",
            "Loss: 6.758136e-05, l1: 0.98717, l2: 0.03184\n",
            "Loss: 6.548676e-05, l1: 0.98615, l2: 0.03180\n",
            "Loss: 6.512423e-05, l1: 0.98607, l2: 0.03179\n",
            "Loss: 6.473492e-05, l1: 0.98679, l2: 0.03179\n",
            "Loss: 6.436600e-05, l1: 0.98776, l2: 0.03179\n",
            "Loss: 6.408767e-05, l1: 0.98862, l2: 0.03179\n",
            "Loss: 6.377884e-05, l1: 0.98942, l2: 0.03179\n",
            "Loss: 6.341034e-05, l1: 0.99021, l2: 0.03180\n",
            "Loss: 6.303659e-05, l1: 0.99091, l2: 0.03182\n",
            "Loss: 6.267460e-05, l1: 0.99148, l2: 0.03185\n",
            "Loss: 6.223915e-05, l1: 0.99215, l2: 0.03188\n",
            "Loss: 6.181578e-05, l1: 0.99307, l2: 0.03187\n",
            "Loss: 6.150780e-05, l1: 0.99352, l2: 0.03188\n",
            "Loss: 6.124657e-05, l1: 0.99377, l2: 0.03186\n",
            "Loss: 6.126179e-05, l1: 0.99601, l2: 0.03193\n",
            "Loss: 6.099551e-05, l1: 0.99488, l2: 0.03189\n",
            "Loss: 6.084244e-05, l1: 0.99521, l2: 0.03189\n",
            "Loss: 6.068075e-05, l1: 0.99533, l2: 0.03188\n",
            "Loss: 6.050774e-05, l1: 0.99564, l2: 0.03186\n",
            "Loss: 6.033173e-05, l1: 0.99602, l2: 0.03185\n",
            "Loss: 6.014962e-05, l1: 0.99632, l2: 0.03183\n",
            "Loss: 5.996985e-05, l1: 0.99646, l2: 0.03180\n",
            "Loss: 5.983709e-05, l1: 0.99635, l2: 0.03177\n",
            "Loss: 5.979554e-05, l1: 0.99588, l2: 0.03179\n",
            "Loss: 5.958369e-05, l1: 0.99571, l2: 0.03173\n",
            "Loss: 5.945404e-05, l1: 0.99580, l2: 0.03175\n",
            "Loss: 5.930181e-05, l1: 0.99583, l2: 0.03177\n",
            "Loss: 5.909358e-05, l1: 0.99580, l2: 0.03178\n",
            "Loss: 5.886854e-05, l1: 0.99578, l2: 0.03178\n",
            "Loss: 5.859111e-05, l1: 0.99562, l2: 0.03176\n",
            "Loss: 7.349592e-05, l1: 0.99653, l2: 0.03198\n",
            "Loss: 5.849924e-05, l1: 0.99569, l2: 0.03177\n",
            "Loss: 5.816978e-05, l1: 0.99546, l2: 0.03173\n",
            "Loss: 5.761956e-05, l1: 0.99482, l2: 0.03173\n",
            "Loss: 5.721013e-05, l1: 0.99445, l2: 0.03160\n",
            "Loss: 5.638265e-05, l1: 0.99401, l2: 0.03169\n",
            "Loss: 5.611532e-05, l1: 0.99417, l2: 0.03174\n",
            "Loss: 5.597503e-05, l1: 0.99437, l2: 0.03176\n",
            "Loss: 5.588369e-05, l1: 0.99445, l2: 0.03176\n",
            "Loss: 5.572896e-05, l1: 0.99456, l2: 0.03174\n",
            "Loss: 5.559011e-05, l1: 0.99448, l2: 0.03174\n",
            "Loss: 5.536085e-05, l1: 0.99430, l2: 0.03173\n",
            "Loss: 5.508294e-05, l1: 0.99434, l2: 0.03175\n",
            "Loss: 5.460525e-05, l1: 0.99400, l2: 0.03175\n",
            "Loss: 5.441395e-05, l1: 0.99460, l2: 0.03173\n",
            "Loss: 5.881043e-05, l1: 0.99463, l2: 0.03194\n",
            "Loss: 5.387919e-05, l1: 0.99460, l2: 0.03178\n",
            "Loss: 5.339939e-05, l1: 0.99433, l2: 0.03178\n",
            "Loss: 5.309887e-05, l1: 0.99479, l2: 0.03179\n",
            "Loss: 5.288883e-05, l1: 0.99503, l2: 0.03179\n",
            "Loss: 5.270751e-05, l1: 0.99536, l2: 0.03179\n",
            "Loss: 5.249655e-05, l1: 0.99547, l2: 0.03179\n",
            "Loss: 5.232557e-05, l1: 0.99534, l2: 0.03179\n",
            "Loss: 5.221329e-05, l1: 0.99508, l2: 0.03178\n",
            "Loss: 5.196825e-05, l1: 0.99468, l2: 0.03175\n",
            "Loss: 5.142970e-05, l1: 0.99323, l2: 0.03169\n",
            "Loss: 5.092144e-05, l1: 0.99322, l2: 0.03162\n",
            "Loss: 5.055084e-05, l1: 0.99324, l2: 0.03162\n",
            "Loss: 5.016083e-05, l1: 0.99360, l2: 0.03162\n",
            "Loss: 4.995241e-05, l1: 0.99374, l2: 0.03163\n",
            "Loss: 4.937501e-05, l1: 0.99420, l2: 0.03165\n",
            "Loss: 4.898966e-05, l1: 0.99474, l2: 0.03167\n",
            "Loss: 4.837367e-05, l1: 0.99581, l2: 0.03167\n",
            "Loss: 4.979695e-05, l1: 0.99590, l2: 0.03167\n",
            "Loss: 4.821537e-05, l1: 0.99583, l2: 0.03167\n",
            "Loss: 4.788121e-05, l1: 0.99647, l2: 0.03166\n",
            "Loss: 4.764632e-05, l1: 0.99654, l2: 0.03164\n",
            "Loss: 4.755549e-05, l1: 0.99648, l2: 0.03162\n",
            "Loss: 4.750532e-05, l1: 0.99631, l2: 0.03162\n",
            "Loss: 4.745623e-05, l1: 0.99629, l2: 0.03162\n",
            "Loss: 4.737116e-05, l1: 0.99638, l2: 0.03161\n",
            "Loss: 4.739739e-05, l1: 0.99536, l2: 0.03159\n",
            "Loss: 4.728736e-05, l1: 0.99591, l2: 0.03160\n",
            "Loss: 4.698646e-05, l1: 0.99664, l2: 0.03161\n",
            "Loss: 4.668745e-05, l1: 0.99673, l2: 0.03161\n",
            "Loss: 4.619901e-05, l1: 0.99631, l2: 0.03158\n",
            "Loss: 4.572187e-05, l1: 0.99594, l2: 0.03154\n",
            "Loss: 4.555083e-05, l1: 0.99574, l2: 0.03154\n",
            "Loss: 4.525429e-05, l1: 0.99572, l2: 0.03154\n",
            "Loss: 4.465696e-05, l1: 0.99603, l2: 0.03155\n",
            "Loss: 4.438813e-05, l1: 0.99618, l2: 0.03155\n",
            "Loss: 4.412816e-05, l1: 0.99657, l2: 0.03156\n",
            "Loss: 4.401347e-05, l1: 0.99685, l2: 0.03156\n",
            "Loss: 4.387006e-05, l1: 0.99703, l2: 0.03157\n",
            "Loss: 4.374903e-05, l1: 0.99709, l2: 0.03157\n",
            "Loss: 4.361560e-05, l1: 0.99706, l2: 0.03159\n",
            "Loss: 4.354106e-05, l1: 0.99703, l2: 0.03161\n",
            "Loss: 4.334118e-05, l1: 0.99714, l2: 0.03163\n",
            "Loss: 4.335232e-05, l1: 0.99687, l2: 0.03163\n",
            "Loss: 4.320353e-05, l1: 0.99701, l2: 0.03163\n",
            "Loss: 4.297882e-05, l1: 0.99714, l2: 0.03166\n",
            "Loss: 4.272267e-05, l1: 0.99724, l2: 0.03168\n",
            "Loss: 4.243065e-05, l1: 0.99667, l2: 0.03172\n",
            "Loss: 4.385278e-05, l1: 0.99788, l2: 0.03186\n",
            "Loss: 4.215098e-05, l1: 0.99702, l2: 0.03176\n",
            "Loss: 4.195279e-05, l1: 0.99685, l2: 0.03175\n",
            "Loss: 4.163399e-05, l1: 0.99647, l2: 0.03173\n",
            "Loss: 4.148274e-05, l1: 0.99459, l2: 0.03170\n",
            "Loss: 4.105840e-05, l1: 0.99494, l2: 0.03172\n",
            "Loss: 4.078443e-05, l1: 0.99500, l2: 0.03174\n",
            "Loss: 4.067074e-05, l1: 0.99471, l2: 0.03175\n",
            "Loss: 4.056865e-05, l1: 0.99439, l2: 0.03174\n",
            "Loss: 4.044185e-05, l1: 0.99419, l2: 0.03174\n",
            "Loss: 4.028875e-05, l1: 0.99419, l2: 0.03173\n",
            "Loss: 4.018270e-05, l1: 0.99430, l2: 0.03172\n",
            "Loss: 4.011268e-05, l1: 0.99452, l2: 0.03172\n",
            "Loss: 4.003888e-05, l1: 0.99466, l2: 0.03172\n",
            "Loss: 3.990763e-05, l1: 0.99455, l2: 0.03171\n",
            "Loss: 4.004177e-05, l1: 0.99554, l2: 0.03176\n",
            "Loss: 3.986153e-05, l1: 0.99488, l2: 0.03173\n",
            "Loss: 3.974277e-05, l1: 0.99416, l2: 0.03169\n",
            "Loss: 3.963686e-05, l1: 0.99391, l2: 0.03169\n",
            "Loss: 3.939834e-05, l1: 0.99320, l2: 0.03168\n",
            "Loss: 3.927095e-05, l1: 0.99304, l2: 0.03168\n",
            "Loss: 3.920351e-05, l1: 0.99274, l2: 0.03171\n",
            "Loss: 3.901293e-05, l1: 0.99307, l2: 0.03169\n",
            "Loss: 3.884684e-05, l1: 0.99350, l2: 0.03169\n",
            "Loss: 3.853676e-05, l1: 0.99442, l2: 0.03170\n",
            "Loss: 3.827803e-05, l1: 0.99490, l2: 0.03176\n",
            "Loss: 3.805586e-05, l1: 0.99525, l2: 0.03178\n",
            "Loss: 3.792849e-05, l1: 0.99536, l2: 0.03179\n",
            "Loss: 3.784285e-05, l1: 0.99537, l2: 0.03180\n",
            "Loss: 3.772810e-05, l1: 0.99561, l2: 0.03180\n",
            "Loss: 3.763800e-05, l1: 0.99588, l2: 0.03179\n",
            "Loss: 3.747343e-05, l1: 0.99642, l2: 0.03178\n",
            "Loss: 3.732007e-05, l1: 0.99702, l2: 0.03177\n",
            "Loss: 3.717454e-05, l1: 0.99708, l2: 0.03177\n",
            "Loss: 3.703667e-05, l1: 0.99706, l2: 0.03176\n",
            "Loss: 3.694477e-05, l1: 0.99698, l2: 0.03176\n",
            "Loss: 3.680027e-05, l1: 0.99693, l2: 0.03177\n",
            "Loss: 3.666896e-05, l1: 0.99699, l2: 0.03176\n",
            "Loss: 3.654417e-05, l1: 0.99697, l2: 0.03176\n",
            "Loss: 3.643567e-05, l1: 0.99703, l2: 0.03175\n",
            "Loss: 3.625378e-05, l1: 0.99697, l2: 0.03175\n",
            "Loss: 3.604074e-05, l1: 0.99680, l2: 0.03175\n",
            "Loss: 3.682710e-05, l1: 0.99782, l2: 0.03182\n",
            "Loss: 3.597880e-05, l1: 0.99702, l2: 0.03177\n",
            "Loss: 3.582489e-05, l1: 0.99660, l2: 0.03176\n",
            "Loss: 3.574840e-05, l1: 0.99661, l2: 0.03176\n",
            "Loss: 3.568599e-05, l1: 0.99650, l2: 0.03175\n",
            "Loss: 3.562415e-05, l1: 0.99639, l2: 0.03174\n",
            "Loss: 3.555117e-05, l1: 0.99616, l2: 0.03172\n",
            "Loss: 3.545584e-05, l1: 0.99578, l2: 0.03169\n",
            "Loss: 3.538524e-05, l1: 0.99568, l2: 0.03169\n",
            "Loss: 3.526574e-05, l1: 0.99568, l2: 0.03169\n",
            "Loss: 3.516910e-05, l1: 0.99522, l2: 0.03169\n",
            "Loss: 3.511208e-05, l1: 0.99506, l2: 0.03168\n",
            "Loss: 3.488005e-05, l1: 0.99521, l2: 0.03169\n",
            "Loss: 3.479018e-05, l1: 0.99517, l2: 0.03170\n",
            "Loss: 3.469295e-05, l1: 0.99507, l2: 0.03170\n",
            "Loss: 3.456714e-05, l1: 0.99497, l2: 0.03170\n",
            "Loss: 3.450488e-05, l1: 0.99494, l2: 0.03173\n",
            "Loss: 3.431686e-05, l1: 0.99526, l2: 0.03170\n",
            "Loss: 3.413342e-05, l1: 0.99536, l2: 0.03171\n",
            "Loss: 3.408256e-05, l1: 0.99545, l2: 0.03172\n",
            "Loss: 3.400326e-05, l1: 0.99562, l2: 0.03172\n",
            "Loss: 3.392392e-05, l1: 0.99576, l2: 0.03172\n",
            "Loss: 3.375480e-05, l1: 0.99590, l2: 0.03172\n",
            "Loss: 3.361896e-05, l1: 0.99593, l2: 0.03171\n",
            "Loss: 3.360486e-05, l1: 0.99565, l2: 0.03170\n",
            "Loss: 3.353378e-05, l1: 0.99578, l2: 0.03171\n",
            "Loss: 3.346401e-05, l1: 0.99561, l2: 0.03171\n",
            "Loss: 3.339990e-05, l1: 0.99528, l2: 0.03172\n",
            "Loss: 3.335130e-05, l1: 0.99521, l2: 0.03172\n",
            "Loss: 3.328648e-05, l1: 0.99518, l2: 0.03173\n",
            "Loss: 3.319634e-05, l1: 0.99517, l2: 0.03174\n",
            "Loss: 3.315286e-05, l1: 0.99502, l2: 0.03175\n",
            "Loss: 3.310003e-05, l1: 0.99479, l2: 0.03175\n",
            "Loss: 3.307970e-05, l1: 0.99454, l2: 0.03174\n",
            "Loss: 3.306149e-05, l1: 0.99446, l2: 0.03174\n",
            "Loss: 3.298126e-05, l1: 0.99441, l2: 0.03174\n",
            "Loss: 3.297931e-05, l1: 0.99508, l2: 0.03180\n",
            "Loss: 3.293135e-05, l1: 0.99475, l2: 0.03177\n",
            "Loss: 3.287070e-05, l1: 0.99504, l2: 0.03177\n",
            "Loss: 3.281068e-05, l1: 0.99498, l2: 0.03179\n",
            "Loss: 3.277059e-05, l1: 0.99515, l2: 0.03179\n",
            "Loss: 3.274010e-05, l1: 0.99528, l2: 0.03181\n",
            "Loss: 3.270418e-05, l1: 0.99530, l2: 0.03181\n",
            "Loss: 3.262673e-05, l1: 0.99520, l2: 0.03182\n",
            "Loss: 3.258966e-05, l1: 0.99488, l2: 0.03183\n",
            "Loss: 3.265730e-05, l1: 0.99488, l2: 0.03182\n",
            "Loss: 3.255637e-05, l1: 0.99488, l2: 0.03183\n",
            "Loss: 3.250659e-05, l1: 0.99477, l2: 0.03182\n",
            "Loss: 3.247601e-05, l1: 0.99472, l2: 0.03182\n",
            "Loss: 3.240624e-05, l1: 0.99462, l2: 0.03182\n",
            "Loss: 3.230185e-05, l1: 0.99450, l2: 0.03182\n",
            "Loss: 3.216965e-05, l1: 0.99444, l2: 0.03184\n",
            "Loss: 3.254335e-05, l1: 0.99353, l2: 0.03186\n",
            "Loss: 3.211309e-05, l1: 0.99420, l2: 0.03185\n",
            "Loss: 3.202342e-05, l1: 0.99426, l2: 0.03187\n",
            "Loss: 3.197879e-05, l1: 0.99425, l2: 0.03187\n",
            "Loss: 3.194876e-05, l1: 0.99420, l2: 0.03187\n",
            "Loss: 3.191514e-05, l1: 0.99414, l2: 0.03186\n",
            "Loss: 3.189344e-05, l1: 0.99413, l2: 0.03186\n",
            "Loss: 3.187058e-05, l1: 0.99416, l2: 0.03186\n",
            "Loss: 3.185806e-05, l1: 0.99422, l2: 0.03187\n",
            "Loss: 3.183671e-05, l1: 0.99430, l2: 0.03188\n",
            "Loss: 3.180205e-05, l1: 0.99437, l2: 0.03188\n",
            "Loss: 3.180858e-05, l1: 0.99413, l2: 0.03188\n",
            "Loss: 3.177163e-05, l1: 0.99426, l2: 0.03188\n",
            "Loss: 3.172069e-05, l1: 0.99438, l2: 0.03189\n",
            "Loss: 3.168843e-05, l1: 0.99425, l2: 0.03188\n",
            "Loss: 3.164094e-05, l1: 0.99407, l2: 0.03186\n",
            "Loss: 3.160982e-05, l1: 0.99404, l2: 0.03186\n",
            "Loss: 3.182101e-05, l1: 0.99446, l2: 0.03187\n",
            "Loss: 3.159509e-05, l1: 0.99413, l2: 0.03186\n",
            "Loss: 3.154937e-05, l1: 0.99425, l2: 0.03186\n",
            "Loss: 3.151502e-05, l1: 0.99447, l2: 0.03186\n",
            "Loss: 3.146967e-05, l1: 0.99478, l2: 0.03187\n",
            "Loss: 3.141155e-05, l1: 0.99510, l2: 0.03188\n",
            "Loss: 3.129360e-05, l1: 0.99543, l2: 0.03188\n",
            "Loss: 3.114620e-05, l1: 0.99560, l2: 0.03188\n",
            "Loss: 3.102789e-05, l1: 0.99514, l2: 0.03186\n",
            "Loss: 3.094512e-05, l1: 0.99469, l2: 0.03184\n",
            "Loss: 3.087667e-05, l1: 0.99408, l2: 0.03181\n",
            "Loss: 3.078583e-05, l1: 0.99401, l2: 0.03181\n",
            "Loss: 3.064890e-05, l1: 0.99422, l2: 0.03181\n",
            "Loss: 3.057934e-05, l1: 0.99450, l2: 0.03180\n",
            "Loss: 3.057180e-05, l1: 0.99518, l2: 0.03179\n",
            "Loss: 3.056130e-05, l1: 0.99570, l2: 0.03180\n",
            "Loss: 3.051243e-05, l1: 0.99526, l2: 0.03180\n",
            "Loss: 3.049551e-05, l1: 0.99517, l2: 0.03180\n",
            "Loss: 3.044841e-05, l1: 0.99518, l2: 0.03180\n",
            "Loss: 3.039775e-05, l1: 0.99539, l2: 0.03180\n",
            "Loss: 3.032276e-05, l1: 0.99630, l2: 0.03181\n",
            "Loss: 3.029502e-05, l1: 0.99712, l2: 0.03178\n",
            "Loss: 3.009762e-05, l1: 0.99674, l2: 0.03179\n",
            "Loss: 3.003406e-05, l1: 0.99680, l2: 0.03179\n",
            "Loss: 2.995865e-05, l1: 0.99708, l2: 0.03178\n",
            "Loss: 2.994174e-05, l1: 0.99731, l2: 0.03181\n",
            "Loss: 2.988886e-05, l1: 0.99749, l2: 0.03178\n",
            "Loss: 2.985486e-05, l1: 0.99743, l2: 0.03178\n",
            "Loss: 2.983282e-05, l1: 0.99737, l2: 0.03178\n",
            "Loss: 2.980282e-05, l1: 0.99729, l2: 0.03178\n",
            "Loss: 2.975591e-05, l1: 0.99713, l2: 0.03177\n",
            "Loss: 2.972150e-05, l1: 0.99692, l2: 0.03177\n",
            "Loss: 2.969309e-05, l1: 0.99686, l2: 0.03177\n",
            "Loss: 2.967362e-05, l1: 0.99696, l2: 0.03177\n",
            "Loss: 2.964782e-05, l1: 0.99704, l2: 0.03177\n",
            "Loss: 2.962528e-05, l1: 0.99705, l2: 0.03177\n",
            "Loss: 2.958673e-05, l1: 0.99693, l2: 0.03177\n",
            "Loss: 2.952826e-05, l1: 0.99665, l2: 0.03177\n",
            "Loss: 2.947088e-05, l1: 0.99647, l2: 0.03175\n",
            "Loss: 2.936558e-05, l1: 0.99611, l2: 0.03173\n",
            "Loss: 2.928862e-05, l1: 0.99595, l2: 0.03171\n",
            "Loss: 2.916107e-05, l1: 0.99569, l2: 0.03169\n",
            "Loss: 2.918925e-05, l1: 0.99554, l2: 0.03167\n",
            "Loss: 2.910663e-05, l1: 0.99563, l2: 0.03168\n",
            "Loss: 2.906228e-05, l1: 0.99600, l2: 0.03167\n",
            "Loss: 2.901470e-05, l1: 0.99590, l2: 0.03168\n",
            "Loss: 2.898888e-05, l1: 0.99588, l2: 0.03168\n",
            "Loss: 2.895187e-05, l1: 0.99580, l2: 0.03167\n",
            "Loss: 2.889941e-05, l1: 0.99565, l2: 0.03166\n",
            "Loss: 2.885437e-05, l1: 0.99541, l2: 0.03166\n",
            "Loss: 3.056919e-05, l1: 0.99493, l2: 0.03157\n",
            "Loss: 2.883365e-05, l1: 0.99536, l2: 0.03165\n",
            "Loss: 2.878789e-05, l1: 0.99526, l2: 0.03165\n",
            "Loss: 2.873955e-05, l1: 0.99517, l2: 0.03166\n",
            "Loss: 2.871363e-05, l1: 0.99526, l2: 0.03166\n",
            "Loss: 2.868399e-05, l1: 0.99534, l2: 0.03166\n",
            "Loss: 2.865932e-05, l1: 0.99540, l2: 0.03167\n",
            "Loss: 2.862703e-05, l1: 0.99539, l2: 0.03167\n",
            "Loss: 2.862455e-05, l1: 0.99545, l2: 0.03169\n",
            "Loss: 2.859619e-05, l1: 0.99542, l2: 0.03168\n",
            "Loss: 2.855724e-05, l1: 0.99522, l2: 0.03168\n",
            "Loss: 2.851772e-05, l1: 0.99501, l2: 0.03169\n",
            "Loss: 2.845329e-05, l1: 0.99485, l2: 0.03170\n",
            "Loss: 2.838200e-05, l1: 0.99456, l2: 0.03172\n",
            "Loss: 2.830434e-05, l1: 0.99480, l2: 0.03173\n",
            "Loss: 2.821924e-05, l1: 0.99535, l2: 0.03174\n",
            "Loss: 2.813575e-05, l1: 0.99539, l2: 0.03175\n",
            "Loss: 2.803417e-05, l1: 0.99618, l2: 0.03175\n",
            "Loss: 2.792597e-05, l1: 0.99606, l2: 0.03175\n",
            "Loss: 2.861910e-05, l1: 0.99400, l2: 0.03182\n",
            "Loss: 2.785232e-05, l1: 0.99557, l2: 0.03177\n",
            "Loss: 2.770344e-05, l1: 0.99559, l2: 0.03179\n",
            "Loss: 2.762378e-05, l1: 0.99584, l2: 0.03181\n",
            "Loss: 2.754268e-05, l1: 0.99588, l2: 0.03183\n",
            "Loss: 2.745481e-05, l1: 0.99606, l2: 0.03183\n",
            "Loss: 2.734918e-05, l1: 0.99615, l2: 0.03183\n",
            "Loss: 2.723183e-05, l1: 0.99613, l2: 0.03182\n",
            "Loss: 2.710153e-05, l1: 0.99596, l2: 0.03181\n",
            "Loss: 2.697981e-05, l1: 0.99563, l2: 0.03181\n",
            "Loss: 2.683438e-05, l1: 0.99518, l2: 0.03181\n",
            "Loss: 2.667455e-05, l1: 0.99479, l2: 0.03181\n",
            "Loss: 2.657964e-05, l1: 0.99467, l2: 0.03181\n",
            "Loss: 2.652967e-05, l1: 0.99468, l2: 0.03182\n",
            "Loss: 2.650631e-05, l1: 0.99483, l2: 0.03182\n",
            "Loss: 2.648225e-05, l1: 0.99499, l2: 0.03181\n",
            "Loss: 2.650071e-05, l1: 0.99509, l2: 0.03180\n",
            "Loss: 2.647012e-05, l1: 0.99503, l2: 0.03181\n",
            "Loss: 2.644654e-05, l1: 0.99505, l2: 0.03180\n",
            "Loss: 2.640681e-05, l1: 0.99497, l2: 0.03180\n",
            "Loss: 2.637881e-05, l1: 0.99510, l2: 0.03181\n",
            "Loss: 2.631474e-05, l1: 0.99488, l2: 0.03180\n",
            "Loss: 2.625141e-05, l1: 0.99471, l2: 0.03179\n",
            "Loss: 2.616711e-05, l1: 0.99459, l2: 0.03179\n",
            "Loss: 2.604985e-05, l1: 0.99447, l2: 0.03179\n",
            "Loss: 2.662980e-05, l1: 0.99423, l2: 0.03177\n",
            "Loss: 2.601904e-05, l1: 0.99443, l2: 0.03179\n",
            "Loss: 2.590742e-05, l1: 0.99470, l2: 0.03181\n",
            "Loss: 2.586065e-05, l1: 0.99453, l2: 0.03181\n",
            "Loss: 2.577612e-05, l1: 0.99488, l2: 0.03183\n",
            "Loss: 2.565792e-05, l1: 0.99544, l2: 0.03185\n",
            "Loss: 2.553948e-05, l1: 0.99588, l2: 0.03188\n",
            "Loss: 2.537672e-05, l1: 0.99612, l2: 0.03190\n",
            "Loss: 2.667617e-05, l1: 0.99679, l2: 0.03193\n",
            "Loss: 2.533714e-05, l1: 0.99622, l2: 0.03190\n",
            "Loss: 2.524104e-05, l1: 0.99589, l2: 0.03190\n",
            "Loss: 2.517651e-05, l1: 0.99550, l2: 0.03189\n",
            "Loss: 2.512396e-05, l1: 0.99502, l2: 0.03187\n",
            "Loss: 2.508234e-05, l1: 0.99497, l2: 0.03187\n",
            "Loss: 2.504988e-05, l1: 0.99510, l2: 0.03187\n",
            "Loss: 2.501490e-05, l1: 0.99517, l2: 0.03187\n",
            "Loss: 2.494634e-05, l1: 0.99536, l2: 0.03186\n",
            "Loss: 2.492893e-05, l1: 0.99567, l2: 0.03186\n",
            "Loss: 2.485656e-05, l1: 0.99558, l2: 0.03185\n",
            "Loss: 2.483351e-05, l1: 0.99554, l2: 0.03185\n",
            "Loss: 2.481118e-05, l1: 0.99554, l2: 0.03185\n",
            "Loss: 2.475754e-05, l1: 0.99547, l2: 0.03185\n",
            "Loss: 2.475504e-05, l1: 0.99561, l2: 0.03186\n",
            "Loss: 2.471255e-05, l1: 0.99555, l2: 0.03185\n",
            "Loss: 2.465414e-05, l1: 0.99556, l2: 0.03186\n",
            "Loss: 2.461180e-05, l1: 0.99563, l2: 0.03186\n",
            "Loss: 2.458731e-05, l1: 0.99575, l2: 0.03187\n",
            "Loss: 2.489537e-05, l1: 0.99613, l2: 0.03181\n",
            "Loss: 2.457083e-05, l1: 0.99582, l2: 0.03186\n",
            "Loss: 2.452867e-05, l1: 0.99588, l2: 0.03187\n",
            "Loss: 2.446373e-05, l1: 0.99603, l2: 0.03189\n",
            "Loss: 2.442066e-05, l1: 0.99605, l2: 0.03189\n",
            "Loss: 2.438751e-05, l1: 0.99608, l2: 0.03190\n",
            "Loss: 2.435262e-05, l1: 0.99632, l2: 0.03190\n",
            "Loss: 2.434158e-05, l1: 0.99605, l2: 0.03189\n",
            "Loss: 2.429685e-05, l1: 0.99639, l2: 0.03190\n",
            "Loss: 2.426537e-05, l1: 0.99651, l2: 0.03190\n",
            "Loss: 2.420684e-05, l1: 0.99670, l2: 0.03189\n",
            "Loss: 2.418505e-05, l1: 0.99672, l2: 0.03189\n",
            "Loss: 2.415318e-05, l1: 0.99711, l2: 0.03189\n",
            "Loss: 2.409260e-05, l1: 0.99706, l2: 0.03188\n",
            "Loss: 2.401711e-05, l1: 0.99702, l2: 0.03188\n",
            "Loss: 2.388798e-05, l1: 0.99690, l2: 0.03188\n",
            "Loss: 2.376874e-05, l1: 0.99668, l2: 0.03189\n",
            "Loss: 2.524689e-05, l1: 0.99608, l2: 0.03194\n",
            "Loss: 2.374185e-05, l1: 0.99661, l2: 0.03190\n",
            "Loss: 2.356947e-05, l1: 0.99608, l2: 0.03191\n",
            "Loss: 2.346915e-05, l1: 0.99572, l2: 0.03191\n",
            "Loss: 2.371211e-05, l1: 0.99504, l2: 0.03196\n",
            "Loss: 2.344435e-05, l1: 0.99556, l2: 0.03192\n",
            "Loss: 2.338118e-05, l1: 0.99542, l2: 0.03193\n",
            "Loss: 2.332973e-05, l1: 0.99532, l2: 0.03192\n",
            "Loss: 2.325796e-05, l1: 0.99537, l2: 0.03193\n",
            "Loss: 2.318837e-05, l1: 0.99528, l2: 0.03193\n",
            "Loss: 2.316290e-05, l1: 0.99571, l2: 0.03195\n",
            "Loss: 2.309089e-05, l1: 0.99550, l2: 0.03195\n",
            "Loss: 2.307382e-05, l1: 0.99542, l2: 0.03194\n",
            "Loss: 2.305185e-05, l1: 0.99530, l2: 0.03195\n",
            "Loss: 2.303155e-05, l1: 0.99519, l2: 0.03195\n",
            "Loss: 2.301305e-05, l1: 0.99508, l2: 0.03195\n",
            "Loss: 2.297300e-05, l1: 0.99485, l2: 0.03196\n",
            "Loss: 2.293212e-05, l1: 0.99464, l2: 0.03196\n",
            "Loss: 2.287989e-05, l1: 0.99435, l2: 0.03197\n",
            "Loss: 2.281648e-05, l1: 0.99434, l2: 0.03197\n",
            "Loss: 2.274855e-05, l1: 0.99439, l2: 0.03196\n",
            "Loss: 2.267938e-05, l1: 0.99441, l2: 0.03195\n",
            "Loss: 2.260203e-05, l1: 0.99439, l2: 0.03193\n",
            "Loss: 2.319995e-05, l1: 0.99382, l2: 0.03186\n",
            "Loss: 2.258439e-05, l1: 0.99430, l2: 0.03192\n",
            "Loss: 2.253993e-05, l1: 0.99398, l2: 0.03186\n",
            "Loss: 2.244868e-05, l1: 0.99391, l2: 0.03189\n",
            "Loss: 2.239815e-05, l1: 0.99385, l2: 0.03190\n",
            "Loss: 2.233876e-05, l1: 0.99378, l2: 0.03191\n",
            "Loss: 2.228613e-05, l1: 0.99387, l2: 0.03190\n",
            "Loss: 2.221512e-05, l1: 0.99406, l2: 0.03189\n",
            "Loss: 2.231185e-05, l1: 0.99450, l2: 0.03188\n",
            "Loss: 2.219569e-05, l1: 0.99419, l2: 0.03189\n",
            "Loss: 2.214787e-05, l1: 0.99450, l2: 0.03188\n",
            "Loss: 2.211500e-05, l1: 0.99460, l2: 0.03187\n",
            "Loss: 2.208353e-05, l1: 0.99470, l2: 0.03187\n",
            "Loss: 2.202781e-05, l1: 0.99462, l2: 0.03186\n",
            "Loss: 2.195683e-05, l1: 0.99444, l2: 0.03186\n",
            "Loss: 2.195016e-05, l1: 0.99415, l2: 0.03188\n",
            "Loss: 2.191882e-05, l1: 0.99429, l2: 0.03187\n",
            "Loss: 2.187034e-05, l1: 0.99418, l2: 0.03188\n",
            "Loss: 2.178710e-05, l1: 0.99416, l2: 0.03188\n",
            "Loss: 2.173760e-05, l1: 0.99423, l2: 0.03188\n",
            "Loss: 2.163106e-05, l1: 0.99467, l2: 0.03187\n",
            "Loss: 2.154605e-05, l1: 0.99562, l2: 0.03192\n",
            "Loss: 2.140499e-05, l1: 0.99537, l2: 0.03190\n",
            "Loss: 2.128212e-05, l1: 0.99558, l2: 0.03189\n",
            "Loss: 2.116739e-05, l1: 0.99596, l2: 0.03189\n",
            "Loss: 2.095994e-05, l1: 0.99663, l2: 0.03191\n",
            "Loss: 2.088654e-05, l1: 0.99707, l2: 0.03193\n",
            "Loss: 2.073150e-05, l1: 0.99684, l2: 0.03191\n",
            "Loss: 2.067080e-05, l1: 0.99643, l2: 0.03190\n",
            "Loss: 2.057800e-05, l1: 0.99606, l2: 0.03188\n",
            "Loss: 2.043905e-05, l1: 0.99582, l2: 0.03186\n",
            "Loss: 2.032748e-05, l1: 0.99581, l2: 0.03184\n",
            "Loss: 2.033742e-05, l1: 0.99587, l2: 0.03183\n",
            "Loss: 2.026247e-05, l1: 0.99584, l2: 0.03184\n",
            "Loss: 2.017880e-05, l1: 0.99592, l2: 0.03183\n",
            "Loss: 2.010007e-05, l1: 0.99594, l2: 0.03184\n",
            "Loss: 2.004430e-05, l1: 0.99584, l2: 0.03185\n",
            "Loss: 1.997357e-05, l1: 0.99557, l2: 0.03185\n",
            "Loss: 8.788531e-05, l1: 0.99940, l2: 0.03174\n",
            "Loss: 1.996509e-05, l1: 0.99561, l2: 0.03185\n",
            "Loss: 1.990774e-05, l1: 0.99535, l2: 0.03185\n",
            "Loss: 1.984697e-05, l1: 0.99524, l2: 0.03185\n",
            "Loss: 1.978170e-05, l1: 0.99523, l2: 0.03184\n",
            "Loss: 1.970985e-05, l1: 0.99539, l2: 0.03182\n",
            "Loss: 1.961382e-05, l1: 0.99570, l2: 0.03181\n",
            "Loss: 1.951016e-05, l1: 0.99590, l2: 0.03179\n",
            "Loss: 1.945218e-05, l1: 0.99605, l2: 0.03177\n",
            "Loss: 1.941237e-05, l1: 0.99590, l2: 0.03177\n",
            "Loss: 1.937972e-05, l1: 0.99567, l2: 0.03177\n",
            "Loss: 1.930878e-05, l1: 0.99522, l2: 0.03177\n",
            "Loss: 1.931663e-05, l1: 0.99525, l2: 0.03176\n",
            "Loss: 1.925202e-05, l1: 0.99523, l2: 0.03177\n",
            "Loss: 1.920375e-05, l1: 0.99424, l2: 0.03177\n",
            "Loss: 1.913067e-05, l1: 0.99467, l2: 0.03175\n",
            "Loss: 1.908222e-05, l1: 0.99489, l2: 0.03175\n",
            "Loss: 1.901221e-05, l1: 0.99513, l2: 0.03174\n",
            "Loss: 1.895366e-05, l1: 0.99532, l2: 0.03174\n",
            "Loss: 1.884877e-05, l1: 0.99547, l2: 0.03174\n",
            "Loss: 1.911136e-05, l1: 0.99667, l2: 0.03175\n",
            "Loss: 1.880069e-05, l1: 0.99581, l2: 0.03174\n",
            "Loss: 1.873602e-05, l1: 0.99589, l2: 0.03177\n",
            "Loss: 1.881053e-05, l1: 0.99566, l2: 0.03174\n",
            "Loss: 1.869639e-05, l1: 0.99580, l2: 0.03175\n",
            "Loss: 1.864739e-05, l1: 0.99574, l2: 0.03176\n",
            "Loss: 1.857926e-05, l1: 0.99568, l2: 0.03176\n",
            "Loss: 1.849543e-05, l1: 0.99574, l2: 0.03176\n",
            "Loss: 1.855557e-05, l1: 0.99610, l2: 0.03177\n",
            "Loss: 1.845642e-05, l1: 0.99588, l2: 0.03177\n",
            "Loss: 1.839545e-05, l1: 0.99596, l2: 0.03177\n",
            "Loss: 1.835332e-05, l1: 0.99592, l2: 0.03178\n",
            "Loss: 1.830705e-05, l1: 0.99589, l2: 0.03178\n",
            "Loss: 1.824230e-05, l1: 0.99586, l2: 0.03179\n",
            "Loss: 1.813462e-05, l1: 0.99571, l2: 0.03180\n",
            "Loss: 1.809871e-05, l1: 0.99572, l2: 0.03179\n",
            "Loss: 1.805420e-05, l1: 0.99560, l2: 0.03179\n",
            "Loss: 1.799343e-05, l1: 0.99583, l2: 0.03178\n",
            "Loss: 1.790943e-05, l1: 0.99601, l2: 0.03177\n",
            "Loss: 1.775323e-05, l1: 0.99572, l2: 0.03179\n",
            "Loss: 1.919140e-05, l1: 0.99801, l2: 0.03186\n",
            "Loss: 1.768047e-05, l1: 0.99614, l2: 0.03180\n",
            "Loss: 1.757847e-05, l1: 0.99614, l2: 0.03181\n",
            "Loss: 1.751570e-05, l1: 0.99609, l2: 0.03183\n",
            "Loss: 1.748483e-05, l1: 0.99619, l2: 0.03184\n",
            "Loss: 1.745238e-05, l1: 0.99603, l2: 0.03187\n",
            "Loss: 1.734202e-05, l1: 0.99634, l2: 0.03187\n",
            "Loss: 1.726773e-05, l1: 0.99648, l2: 0.03188\n",
            "Loss: 1.716395e-05, l1: 0.99652, l2: 0.03189\n",
            "Loss: 1.704114e-05, l1: 0.99638, l2: 0.03190\n",
            "Loss: 1.697813e-05, l1: 0.99593, l2: 0.03190\n",
            "Loss: 1.686075e-05, l1: 0.99582, l2: 0.03191\n",
            "Loss: 1.681622e-05, l1: 0.99575, l2: 0.03190\n",
            "Loss: 1.676568e-05, l1: 0.99560, l2: 0.03189\n",
            "Loss: 1.670800e-05, l1: 0.99541, l2: 0.03189\n",
            "Loss: 1.699631e-05, l1: 0.99493, l2: 0.03185\n",
            "Loss: 1.667236e-05, l1: 0.99529, l2: 0.03188\n",
            "Loss: 1.662955e-05, l1: 0.99526, l2: 0.03188\n",
            "Loss: 1.656743e-05, l1: 0.99530, l2: 0.03189\n",
            "Loss: 1.660293e-05, l1: 0.99539, l2: 0.03190\n",
            "Loss: 1.654909e-05, l1: 0.99533, l2: 0.03189\n",
            "Loss: 1.651730e-05, l1: 0.99536, l2: 0.03190\n",
            "Loss: 1.648746e-05, l1: 0.99538, l2: 0.03190\n",
            "Loss: 1.645230e-05, l1: 0.99539, l2: 0.03190\n",
            "Loss: 1.640519e-05, l1: 0.99554, l2: 0.03190\n",
            "Loss: 1.635736e-05, l1: 0.99553, l2: 0.03190\n",
            "Loss: 1.636335e-05, l1: 0.99583, l2: 0.03188\n",
            "Loss: 1.633836e-05, l1: 0.99567, l2: 0.03189\n",
            "Loss: 1.644909e-05, l1: 0.99528, l2: 0.03189\n",
            "Loss: 1.629933e-05, l1: 0.99554, l2: 0.03189\n",
            "Loss: 1.625562e-05, l1: 0.99562, l2: 0.03189\n",
            "Loss: 1.619917e-05, l1: 0.99560, l2: 0.03189\n",
            "Loss: 1.613253e-05, l1: 0.99563, l2: 0.03188\n",
            "Loss: 1.604180e-05, l1: 0.99576, l2: 0.03189\n",
            "Loss: 1.597769e-05, l1: 0.99590, l2: 0.03189\n",
            "Loss: 1.589586e-05, l1: 0.99615, l2: 0.03189\n",
            "Loss: 1.600234e-05, l1: 0.99712, l2: 0.03187\n",
            "Loss: 1.586781e-05, l1: 0.99646, l2: 0.03189\n",
            "Loss: 1.583629e-05, l1: 0.99641, l2: 0.03189\n",
            "Loss: 1.581599e-05, l1: 0.99638, l2: 0.03189\n",
            "Loss: 1.578864e-05, l1: 0.99643, l2: 0.03189\n",
            "Loss: 1.575598e-05, l1: 0.99652, l2: 0.03189\n",
            "Loss: 1.571663e-05, l1: 0.99643, l2: 0.03188\n",
            "Loss: 1.568285e-05, l1: 0.99644, l2: 0.03186\n",
            "Loss: 1.565166e-05, l1: 0.99643, l2: 0.03185\n",
            "Loss: 1.561318e-05, l1: 0.99644, l2: 0.03185\n",
            "Loss: 1.557775e-05, l1: 0.99653, l2: 0.03184\n",
            "Loss: 1.555045e-05, l1: 0.99648, l2: 0.03184\n",
            "Loss: 1.556263e-05, l1: 0.99697, l2: 0.03186\n",
            "Loss: 1.553049e-05, l1: 0.99670, l2: 0.03185\n",
            "Loss: 1.550264e-05, l1: 0.99671, l2: 0.03185\n",
            "Loss: 1.607282e-05, l1: 0.99746, l2: 0.03187\n",
            "Loss: 1.548534e-05, l1: 0.99682, l2: 0.03185\n",
            "Loss: 1.544038e-05, l1: 0.99683, l2: 0.03186\n",
            "Loss: 1.539826e-05, l1: 0.99692, l2: 0.03187\n",
            "Loss: 1.533709e-05, l1: 0.99712, l2: 0.03188\n",
            "Loss: 1.529318e-05, l1: 0.99736, l2: 0.03188\n",
            "Loss: 1.525678e-05, l1: 0.99763, l2: 0.03189\n",
            "Loss: 1.523745e-05, l1: 0.99780, l2: 0.03189\n",
            "Loss: 1.522171e-05, l1: 0.99790, l2: 0.03188\n",
            "Loss: 1.521521e-05, l1: 0.99785, l2: 0.03188\n",
            "Loss: 1.520443e-05, l1: 0.99789, l2: 0.03188\n",
            "Loss: 1.519091e-05, l1: 0.99789, l2: 0.03188\n",
            "Loss: 1.517342e-05, l1: 0.99783, l2: 0.03188\n",
            "Loss: 1.514100e-05, l1: 0.99766, l2: 0.03187\n",
            "Loss: 1.512056e-05, l1: 0.99760, l2: 0.03187\n",
            "Loss: 1.509590e-05, l1: 0.99769, l2: 0.03186\n",
            "Loss: 1.507852e-05, l1: 0.99772, l2: 0.03186\n",
            "Loss: 1.506173e-05, l1: 0.99782, l2: 0.03186\n",
            "Loss: 1.502442e-05, l1: 0.99799, l2: 0.03187\n",
            "Loss: 1.530942e-05, l1: 0.99931, l2: 0.03187\n",
            "Loss: 1.500956e-05, l1: 0.99823, l2: 0.03187\n",
            "Loss: 1.496499e-05, l1: 0.99842, l2: 0.03188\n",
            "Loss: 1.493385e-05, l1: 0.99849, l2: 0.03189\n",
            "Loss: 1.489620e-05, l1: 0.99844, l2: 0.03189\n",
            "Loss: 1.539715e-05, l1: 0.99823, l2: 0.03186\n",
            "Loss: 1.488645e-05, l1: 0.99842, l2: 0.03189\n",
            "Loss: 1.483292e-05, l1: 0.99817, l2: 0.03189\n",
            "Loss: 1.478881e-05, l1: 0.99786, l2: 0.03188\n",
            "Loss: 1.473721e-05, l1: 0.99737, l2: 0.03189\n",
            "Loss: 1.469476e-05, l1: 0.99730, l2: 0.03189\n",
            "Loss: 1.466358e-05, l1: 0.99741, l2: 0.03190\n",
            "Loss: 1.461930e-05, l1: 0.99760, l2: 0.03191\n",
            "Loss: 1.456060e-05, l1: 0.99772, l2: 0.03191\n",
            "Loss: 1.450366e-05, l1: 0.99779, l2: 0.03191\n",
            "Loss: 1.446381e-05, l1: 0.99808, l2: 0.03189\n",
            "Loss: 1.442236e-05, l1: 0.99740, l2: 0.03188\n",
            "Loss: 1.436004e-05, l1: 0.99755, l2: 0.03188\n",
            "Loss: 1.431497e-05, l1: 0.99756, l2: 0.03187\n",
            "Loss: 1.424629e-05, l1: 0.99759, l2: 0.03186\n",
            "Loss: 1.418129e-05, l1: 0.99773, l2: 0.03185\n",
            "Loss: 1.424308e-05, l1: 0.99830, l2: 0.03187\n",
            "Loss: 1.413357e-05, l1: 0.99796, l2: 0.03186\n",
            "Loss: 1.407956e-05, l1: 0.99803, l2: 0.03186\n",
            "Loss: 1.404173e-05, l1: 0.99805, l2: 0.03186\n",
            "Loss: 1.401777e-05, l1: 0.99802, l2: 0.03186\n",
            "Loss: 1.398409e-05, l1: 0.99778, l2: 0.03185\n",
            "Loss: 1.393835e-05, l1: 0.99784, l2: 0.03185\n",
            "Loss: 1.390810e-05, l1: 0.99776, l2: 0.03186\n",
            "Loss: 1.388103e-05, l1: 0.99766, l2: 0.03186\n",
            "Loss: 1.386011e-05, l1: 0.99759, l2: 0.03187\n",
            "Loss: 1.382767e-05, l1: 0.99748, l2: 0.03188\n",
            "Loss: 1.381539e-05, l1: 0.99749, l2: 0.03188\n",
            "Loss: 1.380004e-05, l1: 0.99753, l2: 0.03188\n",
            "Loss: 1.379030e-05, l1: 0.99757, l2: 0.03188\n",
            "Loss: 1.377979e-05, l1: 0.99761, l2: 0.03188\n",
            "Loss: 1.376804e-05, l1: 0.99766, l2: 0.03189\n",
            "Loss: 1.374614e-05, l1: 0.99764, l2: 0.03189\n",
            "Loss: 1.378473e-05, l1: 0.99792, l2: 0.03190\n",
            "Loss: 1.373358e-05, l1: 0.99773, l2: 0.03189\n",
            "Loss: 1.370861e-05, l1: 0.99769, l2: 0.03189\n",
            "Loss: 1.367596e-05, l1: 0.99762, l2: 0.03189\n",
            "Loss: 1.366200e-05, l1: 0.99759, l2: 0.03189\n",
            "Loss: 1.363791e-05, l1: 0.99761, l2: 0.03188\n",
            "Loss: 1.366474e-05, l1: 0.99696, l2: 0.03188\n",
            "Loss: 1.362679e-05, l1: 0.99738, l2: 0.03188\n",
            "Loss: 1.360661e-05, l1: 0.99751, l2: 0.03188\n",
            "Loss: 1.358826e-05, l1: 0.99755, l2: 0.03187\n",
            "Loss: 1.355599e-05, l1: 0.99758, l2: 0.03187\n",
            "Loss: 1.352003e-05, l1: 0.99745, l2: 0.03185\n",
            "Loss: 1.347975e-05, l1: 0.99736, l2: 0.03186\n",
            "Loss: 1.343117e-05, l1: 0.99714, l2: 0.03185\n",
            "Loss: 1.338483e-05, l1: 0.99703, l2: 0.03186\n",
            "Loss: 1.335695e-05, l1: 0.99679, l2: 0.03185\n",
            "Loss: 1.332532e-05, l1: 0.99692, l2: 0.03185\n",
            "Loss: 1.329866e-05, l1: 0.99704, l2: 0.03185\n",
            "Loss: 1.327220e-05, l1: 0.99718, l2: 0.03184\n",
            "Loss: 1.366327e-05, l1: 0.99693, l2: 0.03187\n",
            "Loss: 1.326365e-05, l1: 0.99715, l2: 0.03185\n",
            "Loss: 1.323982e-05, l1: 0.99731, l2: 0.03185\n",
            "Loss: 1.321252e-05, l1: 0.99736, l2: 0.03185\n",
            "Loss: 1.314750e-05, l1: 0.99738, l2: 0.03184\n",
            "Loss: 1.309842e-05, l1: 0.99725, l2: 0.03184\n",
            "Loss: 1.305397e-05, l1: 0.99717, l2: 0.03183\n",
            "Loss: 1.303103e-05, l1: 0.99668, l2: 0.03182\n",
            "Loss: 1.300956e-05, l1: 0.99689, l2: 0.03181\n",
            "Loss: 1.297728e-05, l1: 0.99697, l2: 0.03181\n",
            "Loss: 1.296659e-05, l1: 0.99700, l2: 0.03181\n",
            "Loss: 1.295306e-05, l1: 0.99708, l2: 0.03181\n",
            "Loss: 1.293810e-05, l1: 0.99719, l2: 0.03182\n",
            "Loss: 1.291376e-05, l1: 0.99731, l2: 0.03182\n",
            "Loss: 1.296571e-05, l1: 0.99722, l2: 0.03182\n",
            "Loss: 1.290302e-05, l1: 0.99728, l2: 0.03182\n",
            "Loss: 1.288153e-05, l1: 0.99737, l2: 0.03182\n",
            "Loss: 1.287285e-05, l1: 0.99733, l2: 0.03182\n",
            "Loss: 1.284997e-05, l1: 0.99731, l2: 0.03182\n",
            "Loss: 1.281565e-05, l1: 0.99722, l2: 0.03181\n",
            "Loss: 1.278697e-05, l1: 0.99717, l2: 0.03180\n",
            "Loss: 1.276484e-05, l1: 0.99719, l2: 0.03179\n",
            "Loss: 1.276372e-05, l1: 0.99715, l2: 0.03177\n",
            "Loss: 1.275620e-05, l1: 0.99717, l2: 0.03178\n",
            "Loss: 1.274191e-05, l1: 0.99715, l2: 0.03178\n",
            "Loss: 1.270723e-05, l1: 0.99697, l2: 0.03177\n",
            "Loss: 1.271211e-05, l1: 0.99656, l2: 0.03177\n",
            "Loss: 1.269683e-05, l1: 0.99678, l2: 0.03177\n",
            "Loss: 1.268032e-05, l1: 0.99667, l2: 0.03176\n",
            "Loss: 1.266139e-05, l1: 0.99658, l2: 0.03176\n",
            "Loss: 1.264482e-05, l1: 0.99649, l2: 0.03176\n",
            "Loss: 1.262181e-05, l1: 0.99634, l2: 0.03176\n",
            "Loss: 1.257379e-05, l1: 0.99607, l2: 0.03176\n",
            "Loss: 1.253801e-05, l1: 0.99588, l2: 0.03176\n",
            "Loss: 1.490527e-05, l1: 0.99557, l2: 0.03174\n",
            "Loss: 1.252510e-05, l1: 0.99586, l2: 0.03176\n",
            "Loss: 1.249944e-05, l1: 0.99588, l2: 0.03176\n",
            "Loss: 1.246777e-05, l1: 0.99598, l2: 0.03177\n",
            "Loss: 1.243483e-05, l1: 0.99615, l2: 0.03177\n",
            "Loss: 1.239091e-05, l1: 0.99620, l2: 0.03177\n",
            "Loss: 1.277168e-05, l1: 0.99756, l2: 0.03180\n",
            "Loss: 1.237083e-05, l1: 0.99645, l2: 0.03177\n",
            "Loss: 1.234616e-05, l1: 0.99644, l2: 0.03177\n",
            "Loss: 1.233188e-05, l1: 0.99640, l2: 0.03177\n",
            "Loss: 1.234861e-05, l1: 0.99652, l2: 0.03176\n",
            "Loss: 1.232612e-05, l1: 0.99644, l2: 0.03177\n",
            "Loss: 1.231978e-05, l1: 0.99641, l2: 0.03177\n",
            "Loss: 1.230988e-05, l1: 0.99641, l2: 0.03177\n",
            "Loss: 1.229507e-05, l1: 0.99631, l2: 0.03177\n",
            "Loss: 1.228029e-05, l1: 0.99653, l2: 0.03177\n",
            "Loss: 1.227118e-05, l1: 0.99659, l2: 0.03177\n",
            "Loss: 1.226374e-05, l1: 0.99665, l2: 0.03177\n",
            "Loss: 1.225170e-05, l1: 0.99666, l2: 0.03177\n",
            "Loss: 1.222234e-05, l1: 0.99666, l2: 0.03177\n",
            "Loss: 1.220403e-05, l1: 0.99656, l2: 0.03178\n",
            "Loss: 1.218818e-05, l1: 0.99651, l2: 0.03178\n",
            "Loss: 1.217695e-05, l1: 0.99653, l2: 0.03178\n",
            "Loss: 1.215428e-05, l1: 0.99659, l2: 0.03178\n",
            "Loss: 1.212428e-05, l1: 0.99671, l2: 0.03179\n",
            "Loss: 1.208490e-05, l1: 0.99694, l2: 0.03180\n",
            "Loss: 1.284087e-05, l1: 0.99694, l2: 0.03188\n",
            "Loss: 1.207206e-05, l1: 0.99694, l2: 0.03181\n",
            "Loss: 1.204722e-05, l1: 0.99705, l2: 0.03181\n",
            "Loss: 1.200921e-05, l1: 0.99723, l2: 0.03182\n",
            "Loss: 1.198503e-05, l1: 0.99727, l2: 0.03182\n",
            "Loss: 1.195997e-05, l1: 0.99734, l2: 0.03182\n",
            "Loss: 1.193929e-05, l1: 0.99719, l2: 0.03182\n",
            "Loss: 1.196561e-05, l1: 0.99721, l2: 0.03180\n",
            "Loss: 1.192951e-05, l1: 0.99720, l2: 0.03182\n",
            "Loss: 1.191985e-05, l1: 0.99714, l2: 0.03182\n",
            "Loss: 1.190234e-05, l1: 0.99706, l2: 0.03182\n",
            "Loss: 1.188501e-05, l1: 0.99690, l2: 0.03182\n",
            "Loss: 1.186830e-05, l1: 0.99692, l2: 0.03182\n",
            "Loss: 1.186042e-05, l1: 0.99689, l2: 0.03182\n",
            "Loss: 1.185559e-05, l1: 0.99686, l2: 0.03182\n",
            "Loss: 1.184906e-05, l1: 0.99681, l2: 0.03183\n",
            "Loss: 1.183387e-05, l1: 0.99682, l2: 0.03183\n",
            "Loss: 1.181675e-05, l1: 0.99684, l2: 0.03184\n",
            "Loss: 1.179699e-05, l1: 0.99693, l2: 0.03184\n",
            "Loss: 1.176565e-05, l1: 0.99699, l2: 0.03184\n",
            "Loss: 1.172462e-05, l1: 0.99715, l2: 0.03184\n",
            "Loss: 1.169267e-05, l1: 0.99725, l2: 0.03184\n",
            "Loss: 1.167305e-05, l1: 0.99719, l2: 0.03183\n",
            "Loss: 1.165766e-05, l1: 0.99714, l2: 0.03183\n",
            "Loss: 1.164348e-05, l1: 0.99707, l2: 0.03183\n",
            "Loss: 1.162895e-05, l1: 0.99712, l2: 0.03183\n",
            "Loss: 1.161221e-05, l1: 0.99712, l2: 0.03183\n",
            "Loss: 1.159349e-05, l1: 0.99725, l2: 0.03184\n",
            "Loss: 1.157685e-05, l1: 0.99740, l2: 0.03185\n",
            "Loss: 1.155345e-05, l1: 0.99750, l2: 0.03185\n",
            "Loss: 1.153618e-05, l1: 0.99759, l2: 0.03185\n",
            "Loss: 1.153249e-05, l1: 0.99762, l2: 0.03185\n",
            "Loss: 1.151884e-05, l1: 0.99767, l2: 0.03185\n",
            "Loss: 1.151238e-05, l1: 0.99765, l2: 0.03185\n",
            "Loss: 1.150125e-05, l1: 0.99763, l2: 0.03185\n",
            "Loss: 1.148572e-05, l1: 0.99767, l2: 0.03185\n",
            "Loss: 1.147147e-05, l1: 0.99738, l2: 0.03184\n",
            "Loss: 1.145789e-05, l1: 0.99747, l2: 0.03184\n",
            "Loss: 1.144927e-05, l1: 0.99753, l2: 0.03184\n",
            "Loss: 1.143810e-05, l1: 0.99760, l2: 0.03184\n",
            "Loss: 1.166588e-05, l1: 0.99802, l2: 0.03187\n",
            "Loss: 1.143124e-05, l1: 0.99766, l2: 0.03184\n",
            "Loss: 1.140630e-05, l1: 0.99781, l2: 0.03184\n",
            "Loss: 1.137162e-05, l1: 0.99795, l2: 0.03185\n",
            "Loss: 1.165915e-05, l1: 0.99942, l2: 0.03189\n",
            "Loss: 1.136346e-05, l1: 0.99817, l2: 0.03185\n",
            "Loss: 1.134590e-05, l1: 0.99820, l2: 0.03186\n",
            "Loss: 1.133327e-05, l1: 0.99820, l2: 0.03186\n",
            "Loss: 1.129513e-05, l1: 0.99826, l2: 0.03186\n",
            "Loss: 1.123279e-05, l1: 0.99844, l2: 0.03187\n",
            "Loss: 1.123894e-05, l1: 0.99855, l2: 0.03186\n",
            "Loss: 1.120232e-05, l1: 0.99849, l2: 0.03187\n",
            "Loss: 1.116760e-05, l1: 0.99854, l2: 0.03187\n",
            "Loss: 1.112571e-05, l1: 0.99857, l2: 0.03187\n",
            "Loss: 1.108675e-05, l1: 0.99859, l2: 0.03187\n",
            "Loss: 1.103688e-05, l1: 0.99849, l2: 0.03187\n",
            "Loss: 1.100654e-05, l1: 0.99855, l2: 0.03187\n",
            "Loss: 1.096822e-05, l1: 0.99858, l2: 0.03187\n",
            "Loss: 1.094070e-05, l1: 0.99868, l2: 0.03186\n",
            "Loss: 1.089943e-05, l1: 0.99881, l2: 0.03186\n",
            "Loss: 1.086203e-05, l1: 0.99899, l2: 0.03186\n",
            "Loss: 1.083920e-05, l1: 0.99902, l2: 0.03186\n",
            "Loss: 1.081367e-05, l1: 0.99920, l2: 0.03185\n",
            "Loss: 1.077437e-05, l1: 0.99918, l2: 0.03186\n",
            "Loss: 1.073290e-05, l1: 0.99925, l2: 0.03187\n",
            "Loss: 1.067905e-05, l1: 0.99950, l2: 0.03188\n",
            "Loss: 1.064119e-05, l1: 0.99982, l2: 0.03190\n",
            "Loss: 1.060994e-05, l1: 0.99989, l2: 0.03190\n",
            "Loss: 1.058672e-05, l1: 0.99986, l2: 0.03190\n",
            "Loss: 1.056647e-05, l1: 0.99976, l2: 0.03190\n",
            "Loss: 1.054763e-05, l1: 0.99963, l2: 0.03190\n",
            "Loss: 1.053411e-05, l1: 0.99934, l2: 0.03191\n",
            "Loss: 1.049193e-05, l1: 0.99935, l2: 0.03191\n",
            "Loss: 1.046705e-05, l1: 0.99938, l2: 0.03191\n",
            "Loss: 1.043811e-05, l1: 0.99945, l2: 0.03191\n",
            "Loss: 1.041919e-05, l1: 0.99944, l2: 0.03191\n",
            "Loss: 1.039751e-05, l1: 0.99939, l2: 0.03192\n",
            "Loss: 1.037848e-05, l1: 0.99935, l2: 0.03193\n",
            "Loss: 1.034828e-05, l1: 0.99925, l2: 0.03193\n",
            "Loss: 1.032128e-05, l1: 0.99922, l2: 0.03194\n",
            "Loss: 1.041849e-05, l1: 0.99890, l2: 0.03194\n",
            "Loss: 1.030821e-05, l1: 0.99914, l2: 0.03194\n",
            "Loss: 1.027667e-05, l1: 0.99912, l2: 0.03194\n",
            "Loss: 1.021811e-05, l1: 0.99916, l2: 0.03194\n",
            "Loss: 1.017465e-05, l1: 0.99910, l2: 0.03193\n",
            "Loss: 1.012555e-05, l1: 0.99903, l2: 0.03193\n",
            "Loss: 1.008098e-05, l1: 0.99889, l2: 0.03192\n",
            "Loss: 1.009566e-05, l1: 0.99908, l2: 0.03195\n",
            "Loss: 1.005866e-05, l1: 0.99897, l2: 0.03193\n",
            "Loss: 1.000548e-05, l1: 0.99882, l2: 0.03194\n",
            "Loss: 9.963383e-06, l1: 0.99882, l2: 0.03194\n",
            "Loss: 9.909491e-06, l1: 0.99886, l2: 0.03196\n",
            "Loss: 9.863371e-06, l1: 0.99892, l2: 0.03197\n",
            "Loss: 9.791730e-06, l1: 0.99896, l2: 0.03198\n",
            "Loss: 9.738201e-06, l1: 0.99897, l2: 0.03198\n",
            "Loss: 9.713630e-06, l1: 0.99870, l2: 0.03196\n",
            "Loss: 9.684717e-06, l1: 0.99884, l2: 0.03196\n",
            "Loss: 9.667054e-06, l1: 0.99883, l2: 0.03195\n",
            "Loss: 9.657277e-06, l1: 0.99894, l2: 0.03195\n",
            "Loss: 9.639105e-06, l1: 0.99882, l2: 0.03194\n",
            "Loss: 9.631910e-06, l1: 0.99840, l2: 0.03194\n",
            "Loss: 9.610103e-06, l1: 0.99844, l2: 0.03193\n",
            "Loss: 9.586951e-06, l1: 0.99843, l2: 0.03193\n",
            "Loss: 9.566726e-06, l1: 0.99840, l2: 0.03194\n",
            "Loss: 9.546859e-06, l1: 0.99826, l2: 0.03193\n",
            "Loss: 9.530304e-06, l1: 0.99817, l2: 0.03193\n",
            "Loss: 9.518833e-06, l1: 0.99814, l2: 0.03193\n",
            "Loss: 9.510261e-06, l1: 0.99819, l2: 0.03192\n",
            "Loss: 9.497944e-06, l1: 0.99824, l2: 0.03191\n",
            "Loss: 9.485017e-06, l1: 0.99839, l2: 0.03191\n",
            "Loss: 9.461844e-06, l1: 0.99834, l2: 0.03191\n",
            "Loss: 9.451098e-06, l1: 0.99831, l2: 0.03192\n",
            "Loss: 9.436732e-06, l1: 0.99827, l2: 0.03192\n",
            "Loss: 9.406855e-06, l1: 0.99824, l2: 0.03191\n",
            "Loss: 9.409502e-06, l1: 0.99826, l2: 0.03191\n",
            "Loss: 9.385348e-06, l1: 0.99825, l2: 0.03191\n",
            "Loss: 9.333276e-06, l1: 0.99819, l2: 0.03191\n",
            "Loss: 9.536344e-06, l1: 0.99860, l2: 0.03190\n",
            "Loss: 9.319747e-06, l1: 0.99827, l2: 0.03190\n",
            "Loss: 9.278223e-06, l1: 0.99828, l2: 0.03189\n",
            "Loss: 9.231370e-06, l1: 0.99820, l2: 0.03187\n",
            "Loss: 9.197609e-06, l1: 0.99805, l2: 0.03186\n",
            "Loss: 9.178326e-06, l1: 0.99796, l2: 0.03186\n",
            "Loss: 9.152601e-06, l1: 0.99787, l2: 0.03186\n",
            "Loss: 9.545251e-06, l1: 0.99733, l2: 0.03183\n",
            "Loss: 9.136589e-06, l1: 0.99778, l2: 0.03185\n",
            "Loss: 9.107691e-06, l1: 0.99781, l2: 0.03185\n",
            "Loss: 9.059499e-06, l1: 0.99795, l2: 0.03185\n",
            "Loss: 9.046915e-06, l1: 0.99800, l2: 0.03185\n",
            "Loss: 9.022966e-06, l1: 0.99804, l2: 0.03184\n",
            "Loss: 9.009369e-06, l1: 0.99807, l2: 0.03184\n",
            "Loss: 8.991799e-06, l1: 0.99802, l2: 0.03183\n",
            "Loss: 8.976447e-06, l1: 0.99798, l2: 0.03183\n",
            "Loss: 8.954416e-06, l1: 0.99792, l2: 0.03183\n",
            "Loss: 8.924795e-06, l1: 0.99798, l2: 0.03184\n",
            "Loss: 8.901976e-06, l1: 0.99784, l2: 0.03183\n",
            "Loss: 8.894686e-06, l1: 0.99806, l2: 0.03183\n",
            "Loss: 8.877651e-06, l1: 0.99798, l2: 0.03183\n",
            "Loss: 8.866781e-06, l1: 0.99794, l2: 0.03182\n",
            "Loss: 8.844647e-06, l1: 0.99792, l2: 0.03181\n",
            "Loss: 8.819588e-06, l1: 0.99790, l2: 0.03181\n",
            "Loss: 8.833362e-06, l1: 0.99785, l2: 0.03179\n",
            "Loss: 8.805424e-06, l1: 0.99788, l2: 0.03180\n",
            "Loss: 8.784212e-06, l1: 0.99795, l2: 0.03180\n",
            "Loss: 8.772219e-06, l1: 0.99807, l2: 0.03180\n",
            "Loss: 8.765147e-06, l1: 0.99814, l2: 0.03181\n",
            "Loss: 9.156069e-06, l1: 0.99850, l2: 0.03182\n",
            "Loss: 8.761857e-06, l1: 0.99817, l2: 0.03181\n",
            "Loss: 8.751713e-06, l1: 0.99822, l2: 0.03181\n",
            "Loss: 8.742544e-06, l1: 0.99823, l2: 0.03181\n",
            "Loss: 8.734130e-06, l1: 0.99821, l2: 0.03181\n",
            "Loss: 8.718874e-06, l1: 0.99815, l2: 0.03181\n",
            "Loss: 8.702799e-06, l1: 0.99802, l2: 0.03179\n",
            "Loss: 8.673518e-06, l1: 0.99794, l2: 0.03180\n",
            "Loss: 8.639002e-06, l1: 0.99789, l2: 0.03181\n",
            "Loss: 8.603693e-06, l1: 0.99794, l2: 0.03182\n",
            "Loss: 8.623027e-06, l1: 0.99800, l2: 0.03184\n",
            "Loss: 8.590810e-06, l1: 0.99796, l2: 0.03183\n",
            "Loss: 8.572164e-06, l1: 0.99799, l2: 0.03184\n",
            "Loss: 8.553237e-06, l1: 0.99803, l2: 0.03184\n",
            "Loss: 8.544926e-06, l1: 0.99793, l2: 0.03184\n",
            "Loss: 8.539102e-06, l1: 0.99790, l2: 0.03184\n",
            "Loss: 8.533869e-06, l1: 0.99788, l2: 0.03184\n",
            "Loss: 8.526785e-06, l1: 0.99791, l2: 0.03183\n",
            "Loss: 8.507370e-06, l1: 0.99792, l2: 0.03183\n",
            "Loss: 8.663788e-06, l1: 0.99785, l2: 0.03183\n",
            "Loss: 8.491404e-06, l1: 0.99790, l2: 0.03183\n",
            "Loss: 8.441584e-06, l1: 0.99805, l2: 0.03183\n",
            "Loss: 8.339283e-06, l1: 0.99799, l2: 0.03181\n",
            "Loss: 8.786094e-06, l1: 0.99902, l2: 0.03183\n",
            "Loss: 8.301876e-06, l1: 0.99821, l2: 0.03182\n",
            "Loss: 8.264848e-06, l1: 0.99817, l2: 0.03182\n",
            "Loss: 8.227918e-06, l1: 0.99808, l2: 0.03182\n",
            "Loss: 8.188215e-06, l1: 0.99806, l2: 0.03183\n",
            "Loss: 8.160659e-06, l1: 0.99806, l2: 0.03183\n",
            "Loss: 8.278686e-06, l1: 0.99826, l2: 0.03186\n",
            "Loss: 8.137108e-06, l1: 0.99812, l2: 0.03184\n",
            "Loss: 8.112846e-06, l1: 0.99824, l2: 0.03185\n",
            "Loss: 8.082602e-06, l1: 0.99839, l2: 0.03185\n",
            "Loss: 8.066089e-06, l1: 0.99846, l2: 0.03185\n",
            "Loss: 8.040509e-06, l1: 0.99851, l2: 0.03185\n",
            "Loss: 8.025857e-06, l1: 0.99848, l2: 0.03184\n",
            "Loss: 8.016303e-06, l1: 0.99846, l2: 0.03184\n",
            "Loss: 8.002426e-06, l1: 0.99846, l2: 0.03184\n",
            "Loss: 7.990875e-06, l1: 0.99844, l2: 0.03184\n",
            "Loss: 7.978693e-06, l1: 0.99842, l2: 0.03184\n",
            "Loss: 7.964488e-06, l1: 0.99837, l2: 0.03184\n",
            "Loss: 7.952356e-06, l1: 0.99824, l2: 0.03184\n",
            "Loss: 8.936362e-06, l1: 0.99804, l2: 0.03182\n",
            "Loss: 7.947986e-06, l1: 0.99823, l2: 0.03184\n",
            "Loss: 7.936896e-06, l1: 0.99817, l2: 0.03184\n",
            "Loss: 7.919198e-06, l1: 0.99810, l2: 0.03185\n",
            "Loss: 7.909347e-06, l1: 0.99811, l2: 0.03185\n",
            "Loss: 7.888477e-06, l1: 0.99812, l2: 0.03185\n",
            "Loss: 7.867799e-06, l1: 0.99820, l2: 0.03185\n",
            "Loss: 7.848404e-06, l1: 0.99809, l2: 0.03184\n",
            "Loss: 7.827335e-06, l1: 0.99811, l2: 0.03184\n",
            "Loss: 7.813642e-06, l1: 0.99814, l2: 0.03184\n",
            "Loss: 7.805565e-06, l1: 0.99810, l2: 0.03183\n",
            "Loss: 7.787972e-06, l1: 0.99800, l2: 0.03183\n",
            "Loss: 7.769932e-06, l1: 0.99789, l2: 0.03182\n",
            "Loss: 7.763852e-06, l1: 0.99785, l2: 0.03182\n",
            "Loss: 7.754246e-06, l1: 0.99783, l2: 0.03182\n",
            "Loss: 7.748410e-06, l1: 0.99784, l2: 0.03182\n",
            "Loss: 7.735627e-06, l1: 0.99786, l2: 0.03182\n",
            "Loss: 7.723096e-06, l1: 0.99787, l2: 0.03182\n",
            "Loss: 7.711893e-06, l1: 0.99790, l2: 0.03182\n",
            "Loss: 7.718955e-06, l1: 0.99767, l2: 0.03180\n",
            "Loss: 7.698431e-06, l1: 0.99779, l2: 0.03181\n",
            "Loss: 7.688345e-06, l1: 0.99773, l2: 0.03181\n",
            "Loss: 7.671974e-06, l1: 0.99759, l2: 0.03181\n",
            "Loss: 7.661969e-06, l1: 0.99754, l2: 0.03181\n",
            "Loss: 7.645403e-06, l1: 0.99750, l2: 0.03181\n",
            "Loss: 7.631755e-06, l1: 0.99752, l2: 0.03182\n",
            "Loss: 7.619134e-06, l1: 0.99762, l2: 0.03182\n",
            "Loss: 7.600549e-06, l1: 0.99767, l2: 0.03183\n",
            "Loss: 7.590884e-06, l1: 0.99779, l2: 0.03183\n",
            "Loss: 7.661605e-06, l1: 0.99778, l2: 0.03182\n",
            "Loss: 7.587403e-06, l1: 0.99779, l2: 0.03183\n",
            "Loss: 7.582860e-06, l1: 0.99782, l2: 0.03183\n",
            "Loss: 7.576900e-06, l1: 0.99782, l2: 0.03183\n",
            "Loss: 7.569896e-06, l1: 0.99779, l2: 0.03183\n",
            "Loss: 7.559408e-06, l1: 0.99774, l2: 0.03183\n",
            "Loss: 7.546697e-06, l1: 0.99764, l2: 0.03182\n",
            "Loss: 7.535026e-06, l1: 0.99756, l2: 0.03182\n",
            "Loss: 7.518646e-06, l1: 0.99750, l2: 0.03182\n",
            "Loss: 7.520756e-06, l1: 0.99724, l2: 0.03181\n",
            "Loss: 7.508308e-06, l1: 0.99738, l2: 0.03182\n",
            "Loss: 7.489461e-06, l1: 0.99739, l2: 0.03181\n",
            "Loss: 7.476264e-06, l1: 0.99745, l2: 0.03182\n",
            "Loss: 7.471354e-06, l1: 0.99745, l2: 0.03181\n",
            "Loss: 7.466650e-06, l1: 0.99751, l2: 0.03182\n",
            "Loss: 7.462510e-06, l1: 0.99753, l2: 0.03182\n",
            "Loss: 7.455924e-06, l1: 0.99755, l2: 0.03182\n",
            "Loss: 7.449821e-06, l1: 0.99757, l2: 0.03182\n",
            "Loss: 7.450521e-06, l1: 0.99758, l2: 0.03182\n",
            "Loss: 7.445976e-06, l1: 0.99757, l2: 0.03182\n",
            "Loss: 7.438862e-06, l1: 0.99761, l2: 0.03182\n",
            "Loss: 7.433368e-06, l1: 0.99764, l2: 0.03181\n",
            "Loss: 7.427894e-06, l1: 0.99765, l2: 0.03181\n",
            "Loss: 7.422148e-06, l1: 0.99766, l2: 0.03182\n",
            "Loss: 7.421490e-06, l1: 0.99788, l2: 0.03183\n",
            "Loss: 7.416011e-06, l1: 0.99777, l2: 0.03182\n",
            "Loss: 7.404384e-06, l1: 0.99778, l2: 0.03182\n",
            "Loss: 7.395349e-06, l1: 0.99780, l2: 0.03183\n",
            "Loss: 7.381368e-06, l1: 0.99787, l2: 0.03183\n",
            "Loss: 7.368735e-06, l1: 0.99796, l2: 0.03183\n",
            "Loss: 7.359956e-06, l1: 0.99810, l2: 0.03183\n",
            "Loss: 7.353519e-06, l1: 0.99817, l2: 0.03183\n",
            "Loss: 7.346043e-06, l1: 0.99824, l2: 0.03183\n",
            "Loss: 7.329780e-06, l1: 0.99834, l2: 0.03182\n",
            "Loss: 7.325886e-06, l1: 0.99852, l2: 0.03182\n",
            "Loss: 7.312408e-06, l1: 0.99846, l2: 0.03182\n",
            "Loss: 7.308753e-06, l1: 0.99845, l2: 0.03183\n",
            "Loss: 7.298134e-06, l1: 0.99848, l2: 0.03183\n",
            "Loss: 7.286337e-06, l1: 0.99856, l2: 0.03183\n",
            "Loss: 7.262608e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 7.248085e-06, l1: 0.99884, l2: 0.03182\n",
            "Loss: 7.239141e-06, l1: 0.99894, l2: 0.03182\n",
            "Loss: 7.236124e-06, l1: 0.99893, l2: 0.03182\n",
            "Loss: 7.232888e-06, l1: 0.99894, l2: 0.03182\n",
            "Loss: 7.229180e-06, l1: 0.99894, l2: 0.03182\n",
            "Loss: 7.225408e-06, l1: 0.99893, l2: 0.03182\n",
            "Loss: 7.219487e-06, l1: 0.99890, l2: 0.03182\n",
            "Loss: 7.240767e-06, l1: 0.99901, l2: 0.03184\n",
            "Loss: 7.215554e-06, l1: 0.99893, l2: 0.03183\n",
            "Loss: 7.208480e-06, l1: 0.99889, l2: 0.03183\n",
            "Loss: 7.200713e-06, l1: 0.99893, l2: 0.03183\n",
            "Loss: 7.198985e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 7.190990e-06, l1: 0.99898, l2: 0.03183\n",
            "Loss: 7.186859e-06, l1: 0.99904, l2: 0.03183\n",
            "Loss: 7.179549e-06, l1: 0.99910, l2: 0.03183\n",
            "Loss: 7.173599e-06, l1: 0.99917, l2: 0.03184\n",
            "Loss: 7.165360e-06, l1: 0.99915, l2: 0.03184\n",
            "Loss: 7.142963e-06, l1: 0.99903, l2: 0.03183\n",
            "Loss: 7.127013e-06, l1: 0.99895, l2: 0.03183\n",
            "Loss: 7.137867e-06, l1: 0.99883, l2: 0.03183\n",
            "Loss: 7.118068e-06, l1: 0.99890, l2: 0.03183\n",
            "Loss: 7.099970e-06, l1: 0.99885, l2: 0.03182\n",
            "Loss: 7.091739e-06, l1: 0.99887, l2: 0.03182\n",
            "Loss: 7.080069e-06, l1: 0.99890, l2: 0.03182\n",
            "Loss: 7.069574e-06, l1: 0.99893, l2: 0.03181\n",
            "Loss: 7.054527e-06, l1: 0.99897, l2: 0.03181\n",
            "Loss: 7.053661e-06, l1: 0.99899, l2: 0.03181\n",
            "Loss: 7.049407e-06, l1: 0.99898, l2: 0.03181\n",
            "Loss: 7.040422e-06, l1: 0.99901, l2: 0.03181\n",
            "Loss: 7.033598e-06, l1: 0.99905, l2: 0.03182\n",
            "Loss: 7.025280e-06, l1: 0.99907, l2: 0.03182\n",
            "Loss: 7.013658e-06, l1: 0.99912, l2: 0.03182\n",
            "Loss: 7.014581e-06, l1: 0.99903, l2: 0.03181\n",
            "Loss: 7.008892e-06, l1: 0.99908, l2: 0.03182\n",
            "Loss: 7.000362e-06, l1: 0.99910, l2: 0.03182\n",
            "Loss: 6.992358e-06, l1: 0.99911, l2: 0.03182\n",
            "Loss: 6.982910e-06, l1: 0.99910, l2: 0.03181\n",
            "Loss: 6.976702e-06, l1: 0.99906, l2: 0.03182\n",
            "Loss: 6.970858e-06, l1: 0.99903, l2: 0.03181\n",
            "Loss: 6.956506e-06, l1: 0.99905, l2: 0.03182\n",
            "Loss: 6.940193e-06, l1: 0.99905, l2: 0.03182\n",
            "Loss: 6.929430e-06, l1: 0.99904, l2: 0.03183\n",
            "Loss: 6.915456e-06, l1: 0.99899, l2: 0.03183\n",
            "Loss: 6.899884e-06, l1: 0.99898, l2: 0.03183\n",
            "Loss: 6.880491e-06, l1: 0.99906, l2: 0.03183\n",
            "Loss: 6.879530e-06, l1: 0.99908, l2: 0.03182\n",
            "Loss: 6.874080e-06, l1: 0.99907, l2: 0.03182\n",
            "Loss: 6.866985e-06, l1: 0.99911, l2: 0.03182\n",
            "Loss: 6.852661e-06, l1: 0.99917, l2: 0.03182\n",
            "Loss: 6.832833e-06, l1: 0.99918, l2: 0.03182\n",
            "Loss: 6.801315e-06, l1: 0.99908, l2: 0.03183\n",
            "Loss: 6.787197e-06, l1: 0.99889, l2: 0.03182\n",
            "Loss: 6.766008e-06, l1: 0.99879, l2: 0.03183\n",
            "Loss: 6.753561e-06, l1: 0.99872, l2: 0.03183\n",
            "Loss: 6.737238e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 6.716722e-06, l1: 0.99864, l2: 0.03184\n",
            "Loss: 6.703124e-06, l1: 0.99866, l2: 0.03184\n",
            "Loss: 6.699539e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 6.695139e-06, l1: 0.99869, l2: 0.03184\n",
            "Loss: 6.692361e-06, l1: 0.99868, l2: 0.03184\n",
            "Loss: 6.689795e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 6.681129e-06, l1: 0.99862, l2: 0.03184\n",
            "Loss: 6.673801e-06, l1: 0.99856, l2: 0.03183\n",
            "Loss: 6.663002e-06, l1: 0.99843, l2: 0.03183\n",
            "Loss: 6.679027e-06, l1: 0.99833, l2: 0.03182\n",
            "Loss: 6.657189e-06, l1: 0.99839, l2: 0.03183\n",
            "Loss: 6.649002e-06, l1: 0.99837, l2: 0.03183\n",
            "Loss: 6.639656e-06, l1: 0.99836, l2: 0.03183\n",
            "Loss: 6.634021e-06, l1: 0.99837, l2: 0.03183\n",
            "Loss: 6.622419e-06, l1: 0.99836, l2: 0.03183\n",
            "Loss: 6.607266e-06, l1: 0.99835, l2: 0.03183\n",
            "Loss: 6.586626e-06, l1: 0.99832, l2: 0.03184\n",
            "Loss: 6.570924e-06, l1: 0.99830, l2: 0.03184\n",
            "Loss: 6.556113e-06, l1: 0.99824, l2: 0.03185\n",
            "Loss: 6.549442e-06, l1: 0.99825, l2: 0.03184\n",
            "Loss: 6.538521e-06, l1: 0.99825, l2: 0.03184\n",
            "Loss: 6.532499e-06, l1: 0.99824, l2: 0.03185\n",
            "Loss: 6.528479e-06, l1: 0.99822, l2: 0.03184\n",
            "Loss: 6.525767e-06, l1: 0.99818, l2: 0.03185\n",
            "Loss: 6.521351e-06, l1: 0.99815, l2: 0.03185\n",
            "Loss: 6.518921e-06, l1: 0.99814, l2: 0.03185\n",
            "Loss: 6.512878e-06, l1: 0.99809, l2: 0.03185\n",
            "Loss: 6.563357e-06, l1: 0.99799, l2: 0.03185\n",
            "Loss: 6.511803e-06, l1: 0.99808, l2: 0.03185\n",
            "Loss: 6.506732e-06, l1: 0.99802, l2: 0.03185\n",
            "Loss: 6.497195e-06, l1: 0.99794, l2: 0.03185\n",
            "Loss: 6.480215e-06, l1: 0.99786, l2: 0.03185\n",
            "Loss: 6.463732e-06, l1: 0.99785, l2: 0.03185\n",
            "Loss: 6.450594e-06, l1: 0.99789, l2: 0.03184\n",
            "Loss: 6.441250e-06, l1: 0.99802, l2: 0.03184\n",
            "Loss: 6.432917e-06, l1: 0.99812, l2: 0.03184\n",
            "Loss: 6.423682e-06, l1: 0.99823, l2: 0.03185\n",
            "Loss: 6.427004e-06, l1: 0.99833, l2: 0.03185\n",
            "Loss: 6.417557e-06, l1: 0.99827, l2: 0.03185\n",
            "Loss: 6.409153e-06, l1: 0.99830, l2: 0.03185\n",
            "Loss: 6.402209e-06, l1: 0.99827, l2: 0.03185\n",
            "Loss: 6.394253e-06, l1: 0.99820, l2: 0.03185\n",
            "Loss: 6.387453e-06, l1: 0.99813, l2: 0.03185\n",
            "Loss: 6.378810e-06, l1: 0.99802, l2: 0.03185\n",
            "Loss: 6.370125e-06, l1: 0.99801, l2: 0.03184\n",
            "Loss: 6.361295e-06, l1: 0.99796, l2: 0.03184\n",
            "Loss: 6.354817e-06, l1: 0.99795, l2: 0.03184\n",
            "Loss: 6.349867e-06, l1: 0.99794, l2: 0.03184\n",
            "Loss: 6.340910e-06, l1: 0.99787, l2: 0.03183\n",
            "Loss: 6.335795e-06, l1: 0.99784, l2: 0.03183\n",
            "Loss: 6.324726e-06, l1: 0.99780, l2: 0.03183\n",
            "Loss: 6.313591e-06, l1: 0.99771, l2: 0.03183\n",
            "Loss: 6.299220e-06, l1: 0.99769, l2: 0.03182\n",
            "Loss: 6.290862e-06, l1: 0.99766, l2: 0.03182\n",
            "Loss: 6.282539e-06, l1: 0.99765, l2: 0.03182\n",
            "Loss: 6.275348e-06, l1: 0.99761, l2: 0.03182\n",
            "Loss: 6.265242e-06, l1: 0.99756, l2: 0.03183\n",
            "Loss: 6.252257e-06, l1: 0.99754, l2: 0.03183\n",
            "Loss: 6.662624e-06, l1: 0.99764, l2: 0.03183\n",
            "Loss: 6.249481e-06, l1: 0.99755, l2: 0.03183\n",
            "Loss: 6.238667e-06, l1: 0.99757, l2: 0.03183\n",
            "Loss: 6.233053e-06, l1: 0.99761, l2: 0.03183\n",
            "Loss: 6.228089e-06, l1: 0.99767, l2: 0.03183\n",
            "Loss: 6.222678e-06, l1: 0.99774, l2: 0.03183\n",
            "Loss: 6.211831e-06, l1: 0.99776, l2: 0.03182\n",
            "Loss: 6.212402e-06, l1: 0.99780, l2: 0.03183\n",
            "Loss: 6.201398e-06, l1: 0.99778, l2: 0.03183\n",
            "Loss: 6.182233e-06, l1: 0.99775, l2: 0.03183\n",
            "Loss: 6.170479e-06, l1: 0.99769, l2: 0.03183\n",
            "Loss: 6.148886e-06, l1: 0.99757, l2: 0.03184\n",
            "Loss: 6.127693e-06, l1: 0.99752, l2: 0.03184\n",
            "Loss: 6.075618e-06, l1: 0.99743, l2: 0.03184\n",
            "Loss: 6.039348e-06, l1: 0.99745, l2: 0.03183\n",
            "Loss: 6.012270e-06, l1: 0.99748, l2: 0.03182\n",
            "Loss: 5.998927e-06, l1: 0.99758, l2: 0.03182\n",
            "Loss: 5.986497e-06, l1: 0.99764, l2: 0.03181\n",
            "Loss: 5.968038e-06, l1: 0.99772, l2: 0.03181\n",
            "Loss: 5.948603e-06, l1: 0.99779, l2: 0.03181\n",
            "Loss: 5.952725e-06, l1: 0.99784, l2: 0.03182\n",
            "Loss: 5.938587e-06, l1: 0.99781, l2: 0.03182\n",
            "Loss: 5.921594e-06, l1: 0.99789, l2: 0.03182\n",
            "Loss: 5.913717e-06, l1: 0.99791, l2: 0.03182\n",
            "Loss: 5.907742e-06, l1: 0.99793, l2: 0.03182\n",
            "Loss: 5.900363e-06, l1: 0.99796, l2: 0.03182\n",
            "Loss: 5.894110e-06, l1: 0.99801, l2: 0.03182\n",
            "Loss: 5.880443e-06, l1: 0.99808, l2: 0.03182\n",
            "Loss: 5.863895e-06, l1: 0.99816, l2: 0.03182\n",
            "Loss: 5.844739e-06, l1: 0.99824, l2: 0.03182\n",
            "Loss: 5.827056e-06, l1: 0.99826, l2: 0.03182\n",
            "Loss: 5.823290e-06, l1: 0.99831, l2: 0.03182\n",
            "Loss: 5.804106e-06, l1: 0.99829, l2: 0.03182\n",
            "Loss: 5.793073e-06, l1: 0.99826, l2: 0.03183\n",
            "Loss: 5.772246e-06, l1: 0.99817, l2: 0.03183\n",
            "Loss: 5.754079e-06, l1: 0.99812, l2: 0.03184\n",
            "Loss: 5.741767e-06, l1: 0.99808, l2: 0.03185\n",
            "Loss: 5.733291e-06, l1: 0.99811, l2: 0.03185\n",
            "Loss: 5.729615e-06, l1: 0.99811, l2: 0.03186\n",
            "Loss: 5.714744e-06, l1: 0.99815, l2: 0.03185\n",
            "Loss: 5.708509e-06, l1: 0.99816, l2: 0.03185\n",
            "Loss: 5.701605e-06, l1: 0.99815, l2: 0.03186\n",
            "Loss: 5.695550e-06, l1: 0.99814, l2: 0.03186\n",
            "Loss: 6.400979e-06, l1: 0.99772, l2: 0.03191\n",
            "Loss: 5.693480e-06, l1: 0.99812, l2: 0.03186\n",
            "Loss: 5.681471e-06, l1: 0.99810, l2: 0.03186\n",
            "Loss: 5.660171e-06, l1: 0.99803, l2: 0.03186\n",
            "Loss: 5.664128e-06, l1: 0.99803, l2: 0.03186\n",
            "Loss: 5.651190e-06, l1: 0.99803, l2: 0.03186\n",
            "Loss: 5.639630e-06, l1: 0.99800, l2: 0.03186\n",
            "Loss: 5.627975e-06, l1: 0.99797, l2: 0.03185\n",
            "Loss: 5.620841e-06, l1: 0.99797, l2: 0.03185\n",
            "Loss: 5.613715e-06, l1: 0.99792, l2: 0.03185\n",
            "Loss: 5.603031e-06, l1: 0.99780, l2: 0.03185\n",
            "Loss: 5.594311e-06, l1: 0.99766, l2: 0.03185\n",
            "Loss: 5.583822e-06, l1: 0.99761, l2: 0.03185\n",
            "Loss: 5.573514e-06, l1: 0.99764, l2: 0.03185\n",
            "Loss: 5.569078e-06, l1: 0.99773, l2: 0.03185\n",
            "Loss: 5.558741e-06, l1: 0.99776, l2: 0.03185\n",
            "Loss: 5.553404e-06, l1: 0.99782, l2: 0.03185\n",
            "Loss: 5.546742e-06, l1: 0.99790, l2: 0.03186\n",
            "Loss: 5.542237e-06, l1: 0.99795, l2: 0.03186\n",
            "Loss: 5.536399e-06, l1: 0.99798, l2: 0.03187\n",
            "Loss: 5.545530e-06, l1: 0.99794, l2: 0.03187\n",
            "Loss: 5.534049e-06, l1: 0.99797, l2: 0.03187\n",
            "Loss: 5.529660e-06, l1: 0.99796, l2: 0.03187\n",
            "Loss: 5.524410e-06, l1: 0.99796, l2: 0.03187\n",
            "Loss: 5.519623e-06, l1: 0.99796, l2: 0.03187\n",
            "Loss: 5.513833e-06, l1: 0.99799, l2: 0.03187\n",
            "Loss: 5.517751e-06, l1: 0.99805, l2: 0.03188\n",
            "Loss: 5.511396e-06, l1: 0.99801, l2: 0.03187\n",
            "Loss: 5.506688e-06, l1: 0.99806, l2: 0.03187\n",
            "Loss: 5.503846e-06, l1: 0.99808, l2: 0.03187\n",
            "Loss: 5.501748e-06, l1: 0.99809, l2: 0.03187\n",
            "Loss: 5.500637e-06, l1: 0.99809, l2: 0.03187\n",
            "Loss: 5.493288e-06, l1: 0.99810, l2: 0.03187\n",
            "Loss: 5.482774e-06, l1: 0.99813, l2: 0.03187\n",
            "Loss: 5.474114e-06, l1: 0.99814, l2: 0.03188\n",
            "Loss: 5.466277e-06, l1: 0.99816, l2: 0.03188\n",
            "Loss: 5.460671e-06, l1: 0.99816, l2: 0.03188\n",
            "Loss: 5.455693e-06, l1: 0.99816, l2: 0.03188\n",
            "Loss: 5.452500e-06, l1: 0.99816, l2: 0.03188\n",
            "Loss: 5.449859e-06, l1: 0.99815, l2: 0.03188\n",
            "Loss: 5.445695e-06, l1: 0.99814, l2: 0.03188\n",
            "Loss: 5.440262e-06, l1: 0.99808, l2: 0.03187\n",
            "Loss: 5.435407e-06, l1: 0.99809, l2: 0.03187\n",
            "Loss: 5.431558e-06, l1: 0.99811, l2: 0.03187\n",
            "Loss: 5.429536e-06, l1: 0.99811, l2: 0.03187\n",
            "Loss: 5.426718e-06, l1: 0.99809, l2: 0.03187\n",
            "Loss: 5.420643e-06, l1: 0.99809, l2: 0.03187\n",
            "Loss: 5.416036e-06, l1: 0.99804, l2: 0.03187\n",
            "Loss: 5.411076e-06, l1: 0.99804, l2: 0.03187\n",
            "Loss: 5.405308e-06, l1: 0.99801, l2: 0.03187\n",
            "Loss: 5.398684e-06, l1: 0.99796, l2: 0.03187\n",
            "Loss: 5.396891e-06, l1: 0.99794, l2: 0.03186\n",
            "Loss: 5.392684e-06, l1: 0.99788, l2: 0.03187\n",
            "Loss: 5.385325e-06, l1: 0.99791, l2: 0.03187\n",
            "Loss: 5.381833e-06, l1: 0.99793, l2: 0.03187\n",
            "Loss: 5.381528e-06, l1: 0.99788, l2: 0.03187\n",
            "Loss: 5.374392e-06, l1: 0.99790, l2: 0.03187\n",
            "Loss: 5.417300e-06, l1: 0.99813, l2: 0.03186\n",
            "Loss: 5.364137e-06, l1: 0.99797, l2: 0.03187\n",
            "Loss: 5.355187e-06, l1: 0.99798, l2: 0.03187\n",
            "Loss: 5.337976e-06, l1: 0.99795, l2: 0.03187\n",
            "Loss: 5.421971e-06, l1: 0.99789, l2: 0.03189\n",
            "Loss: 5.333512e-06, l1: 0.99794, l2: 0.03188\n",
            "Loss: 5.323478e-06, l1: 0.99791, l2: 0.03188\n",
            "Loss: 5.314884e-06, l1: 0.99788, l2: 0.03188\n",
            "Loss: 5.309538e-06, l1: 0.99787, l2: 0.03188\n",
            "Loss: 5.303976e-06, l1: 0.99786, l2: 0.03188\n",
            "Loss: 5.294760e-06, l1: 0.99780, l2: 0.03188\n",
            "Loss: 5.286422e-06, l1: 0.99770, l2: 0.03188\n",
            "Loss: 5.273972e-06, l1: 0.99774, l2: 0.03188\n",
            "Loss: 5.265184e-06, l1: 0.99778, l2: 0.03188\n",
            "Loss: 5.254941e-06, l1: 0.99782, l2: 0.03188\n",
            "Loss: 5.284460e-06, l1: 0.99788, l2: 0.03189\n",
            "Loss: 5.248816e-06, l1: 0.99783, l2: 0.03188\n",
            "Loss: 5.228961e-06, l1: 0.99789, l2: 0.03188\n",
            "Loss: 5.214699e-06, l1: 0.99792, l2: 0.03187\n",
            "Loss: 5.204466e-06, l1: 0.99794, l2: 0.03187\n",
            "Loss: 5.198798e-06, l1: 0.99794, l2: 0.03187\n",
            "Loss: 5.191973e-06, l1: 0.99796, l2: 0.03186\n",
            "Loss: 5.184810e-06, l1: 0.99798, l2: 0.03186\n",
            "Loss: 5.177903e-06, l1: 0.99801, l2: 0.03186\n",
            "Loss: 5.169150e-06, l1: 0.99808, l2: 0.03187\n",
            "Loss: 5.164407e-06, l1: 0.99812, l2: 0.03187\n",
            "Loss: 5.152325e-06, l1: 0.99823, l2: 0.03186\n",
            "Loss: 5.658178e-06, l1: 0.99862, l2: 0.03188\n",
            "Loss: 5.151753e-06, l1: 0.99824, l2: 0.03187\n",
            "Loss: 5.142655e-06, l1: 0.99832, l2: 0.03186\n",
            "Loss: 5.134544e-06, l1: 0.99837, l2: 0.03186\n",
            "Loss: 5.128065e-06, l1: 0.99839, l2: 0.03187\n",
            "Loss: 5.122635e-06, l1: 0.99840, l2: 0.03187\n",
            "Loss: 5.115117e-06, l1: 0.99840, l2: 0.03187\n",
            "Loss: 5.101973e-06, l1: 0.99840, l2: 0.03187\n",
            "Loss: 5.093121e-06, l1: 0.99844, l2: 0.03188\n",
            "Loss: 5.080183e-06, l1: 0.99844, l2: 0.03187\n",
            "Loss: 5.073758e-06, l1: 0.99846, l2: 0.03187\n",
            "Loss: 5.066221e-06, l1: 0.99850, l2: 0.03187\n",
            "Loss: 5.151099e-06, l1: 0.99863, l2: 0.03185\n",
            "Loss: 5.063935e-06, l1: 0.99852, l2: 0.03186\n",
            "Loss: 5.059417e-06, l1: 0.99855, l2: 0.03186\n",
            "Loss: 5.052818e-06, l1: 0.99861, l2: 0.03187\n",
            "Loss: 5.048648e-06, l1: 0.99865, l2: 0.03187\n",
            "Loss: 5.040722e-06, l1: 0.99874, l2: 0.03187\n",
            "Loss: 5.072618e-06, l1: 0.99891, l2: 0.03187\n",
            "Loss: 5.036652e-06, l1: 0.99878, l2: 0.03187\n",
            "Loss: 5.026384e-06, l1: 0.99888, l2: 0.03188\n",
            "Loss: 5.013026e-06, l1: 0.99898, l2: 0.03188\n",
            "Loss: 4.998333e-06, l1: 0.99910, l2: 0.03188\n",
            "Loss: 4.993284e-06, l1: 0.99923, l2: 0.03188\n",
            "Loss: 4.980463e-06, l1: 0.99924, l2: 0.03187\n",
            "Loss: 4.972047e-06, l1: 0.99925, l2: 0.03187\n",
            "Loss: 4.965630e-06, l1: 0.99930, l2: 0.03187\n",
            "Loss: 4.959712e-06, l1: 0.99935, l2: 0.03187\n",
            "Loss: 4.954990e-06, l1: 0.99942, l2: 0.03187\n",
            "Loss: 4.951339e-06, l1: 0.99953, l2: 0.03187\n",
            "Loss: 4.942032e-06, l1: 0.99952, l2: 0.03188\n",
            "Loss: 4.936674e-06, l1: 0.99949, l2: 0.03187\n",
            "Loss: 4.934029e-06, l1: 0.99947, l2: 0.03187\n",
            "Loss: 4.930915e-06, l1: 0.99945, l2: 0.03187\n",
            "Loss: 4.926760e-06, l1: 0.99942, l2: 0.03187\n",
            "Loss: 4.921245e-06, l1: 0.99941, l2: 0.03187\n",
            "Loss: 4.932577e-06, l1: 0.99936, l2: 0.03187\n",
            "Loss: 4.919358e-06, l1: 0.99939, l2: 0.03187\n",
            "Loss: 4.915685e-06, l1: 0.99948, l2: 0.03187\n",
            "Loss: 4.904035e-06, l1: 0.99943, l2: 0.03187\n",
            "Loss: 4.899414e-06, l1: 0.99942, l2: 0.03187\n",
            "Loss: 4.886283e-06, l1: 0.99938, l2: 0.03187\n",
            "Loss: 4.902113e-06, l1: 0.99925, l2: 0.03188\n",
            "Loss: 4.881294e-06, l1: 0.99934, l2: 0.03188\n",
            "Loss: 4.865026e-06, l1: 0.99931, l2: 0.03187\n",
            "Loss: 4.853327e-06, l1: 0.99926, l2: 0.03187\n",
            "Loss: 4.859076e-06, l1: 0.99920, l2: 0.03186\n",
            "Loss: 4.836309e-06, l1: 0.99923, l2: 0.03186\n",
            "Loss: 4.826998e-06, l1: 0.99924, l2: 0.03186\n",
            "Loss: 4.817639e-06, l1: 0.99925, l2: 0.03186\n",
            "Loss: 4.812035e-06, l1: 0.99926, l2: 0.03186\n",
            "Loss: 4.807244e-06, l1: 0.99927, l2: 0.03186\n",
            "Loss: 4.804523e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 4.800469e-06, l1: 0.99926, l2: 0.03185\n",
            "Loss: 4.796039e-06, l1: 0.99923, l2: 0.03185\n",
            "Loss: 4.791333e-06, l1: 0.99917, l2: 0.03185\n",
            "Loss: 4.789190e-06, l1: 0.99913, l2: 0.03185\n",
            "Loss: 4.791343e-06, l1: 0.99905, l2: 0.03184\n",
            "Loss: 4.787884e-06, l1: 0.99910, l2: 0.03185\n",
            "Loss: 4.785888e-06, l1: 0.99908, l2: 0.03185\n",
            "Loss: 4.783964e-06, l1: 0.99907, l2: 0.03185\n",
            "Loss: 4.780957e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 4.772746e-06, l1: 0.99901, l2: 0.03185\n",
            "Loss: 4.767052e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 4.759674e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 4.755215e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 4.760312e-06, l1: 0.99896, l2: 0.03183\n",
            "Loss: 4.749530e-06, l1: 0.99900, l2: 0.03183\n",
            "Loss: 4.738116e-06, l1: 0.99896, l2: 0.03183\n",
            "Loss: 4.733785e-06, l1: 0.99894, l2: 0.03183\n",
            "Loss: 4.726573e-06, l1: 0.99890, l2: 0.03183\n",
            "Loss: 4.725477e-06, l1: 0.99887, l2: 0.03183\n",
            "Loss: 4.719114e-06, l1: 0.99885, l2: 0.03183\n",
            "Loss: 4.713816e-06, l1: 0.99882, l2: 0.03183\n",
            "Loss: 4.707589e-06, l1: 0.99875, l2: 0.03182\n",
            "Loss: 4.704793e-06, l1: 0.99872, l2: 0.03182\n",
            "Loss: 4.699413e-06, l1: 0.99865, l2: 0.03182\n",
            "Loss: 4.694782e-06, l1: 0.99866, l2: 0.03182\n",
            "Loss: 4.686526e-06, l1: 0.99868, l2: 0.03182\n",
            "Loss: 4.702204e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 4.684346e-06, l1: 0.99871, l2: 0.03182\n",
            "Loss: 4.679155e-06, l1: 0.99876, l2: 0.03182\n",
            "Loss: 4.676870e-06, l1: 0.99875, l2: 0.03182\n",
            "Loss: 4.674592e-06, l1: 0.99872, l2: 0.03182\n",
            "Loss: 4.672909e-06, l1: 0.99870, l2: 0.03182\n",
            "Loss: 4.673091e-06, l1: 0.99861, l2: 0.03182\n",
            "Loss: 4.671265e-06, l1: 0.99866, l2: 0.03182\n",
            "Loss: 4.668868e-06, l1: 0.99864, l2: 0.03182\n",
            "Loss: 4.667159e-06, l1: 0.99864, l2: 0.03182\n",
            "Loss: 4.664946e-06, l1: 0.99863, l2: 0.03182\n",
            "Loss: 4.662348e-06, l1: 0.99863, l2: 0.03182\n",
            "Loss: 4.658302e-06, l1: 0.99864, l2: 0.03182\n",
            "Loss: 4.656666e-06, l1: 0.99863, l2: 0.03181\n",
            "Loss: 4.654300e-06, l1: 0.99867, l2: 0.03182\n",
            "Loss: 4.653633e-06, l1: 0.99866, l2: 0.03181\n",
            "Loss: 4.651279e-06, l1: 0.99869, l2: 0.03181\n",
            "Loss: 4.648779e-06, l1: 0.99868, l2: 0.03181\n",
            "Loss: 4.646947e-06, l1: 0.99867, l2: 0.03181\n",
            "Loss: 4.644681e-06, l1: 0.99866, l2: 0.03181\n",
            "Loss: 4.642127e-06, l1: 0.99864, l2: 0.03181\n",
            "Loss: 4.638712e-06, l1: 0.99859, l2: 0.03181\n",
            "Loss: 4.642781e-06, l1: 0.99853, l2: 0.03181\n",
            "Loss: 4.635918e-06, l1: 0.99857, l2: 0.03181\n",
            "Loss: 4.633071e-06, l1: 0.99856, l2: 0.03181\n",
            "Loss: 4.627886e-06, l1: 0.99859, l2: 0.03181\n",
            "Loss: 4.630586e-06, l1: 0.99847, l2: 0.03181\n",
            "Loss: 4.624672e-06, l1: 0.99854, l2: 0.03181\n",
            "Loss: 4.621615e-06, l1: 0.99853, l2: 0.03181\n",
            "Loss: 4.616207e-06, l1: 0.99850, l2: 0.03181\n",
            "Loss: 4.613152e-06, l1: 0.99848, l2: 0.03181\n",
            "Loss: 4.608885e-06, l1: 0.99844, l2: 0.03181\n",
            "Loss: 4.598565e-06, l1: 0.99829, l2: 0.03182\n",
            "Loss: 4.607738e-06, l1: 0.99829, l2: 0.03183\n",
            "Loss: 4.596278e-06, l1: 0.99829, l2: 0.03182\n",
            "Loss: 4.591975e-06, l1: 0.99821, l2: 0.03182\n",
            "Loss: 4.591808e-06, l1: 0.99816, l2: 0.03183\n",
            "Loss: 4.589541e-06, l1: 0.99818, l2: 0.03182\n",
            "Loss: 4.584997e-06, l1: 0.99811, l2: 0.03182\n",
            "Loss: 4.581912e-06, l1: 0.99808, l2: 0.03182\n",
            "Loss: 4.578913e-06, l1: 0.99808, l2: 0.03182\n",
            "Loss: 4.576258e-06, l1: 0.99809, l2: 0.03182\n",
            "Loss: 4.571165e-06, l1: 0.99810, l2: 0.03182\n",
            "Loss: 4.570656e-06, l1: 0.99808, l2: 0.03182\n",
            "Loss: 4.567108e-06, l1: 0.99809, l2: 0.03182\n",
            "Loss: 4.561525e-06, l1: 0.99810, l2: 0.03182\n",
            "Loss: 4.554753e-06, l1: 0.99811, l2: 0.03182\n",
            "Loss: 4.546275e-06, l1: 0.99811, l2: 0.03182\n",
            "Loss: 4.553483e-06, l1: 0.99813, l2: 0.03183\n",
            "Loss: 4.541212e-06, l1: 0.99812, l2: 0.03183\n",
            "Loss: 4.535787e-06, l1: 0.99810, l2: 0.03182\n",
            "Loss: 4.529080e-06, l1: 0.99807, l2: 0.03182\n",
            "Loss: 4.524395e-06, l1: 0.99805, l2: 0.03182\n",
            "Loss: 4.514072e-06, l1: 0.99798, l2: 0.03181\n",
            "Loss: 4.506232e-06, l1: 0.99793, l2: 0.03181\n",
            "Loss: 4.506451e-06, l1: 0.99786, l2: 0.03181\n",
            "Loss: 4.503173e-06, l1: 0.99789, l2: 0.03181\n",
            "Loss: 4.499430e-06, l1: 0.99788, l2: 0.03181\n",
            "Loss: 4.497331e-06, l1: 0.99786, l2: 0.03181\n",
            "Loss: 4.494772e-06, l1: 0.99784, l2: 0.03181\n",
            "Loss: 4.490269e-06, l1: 0.99779, l2: 0.03181\n",
            "Loss: 4.484735e-06, l1: 0.99775, l2: 0.03181\n",
            "Loss: 4.476675e-06, l1: 0.99771, l2: 0.03181\n",
            "Loss: 4.470537e-06, l1: 0.99772, l2: 0.03181\n",
            "Loss: 4.464257e-06, l1: 0.99777, l2: 0.03181\n",
            "Loss: 4.479633e-06, l1: 0.99771, l2: 0.03181\n",
            "Loss: 4.462162e-06, l1: 0.99775, l2: 0.03181\n",
            "Loss: 4.456698e-06, l1: 0.99781, l2: 0.03181\n",
            "Loss: 4.447608e-06, l1: 0.99789, l2: 0.03181\n",
            "Loss: 4.439300e-06, l1: 0.99795, l2: 0.03181\n",
            "Loss: 4.432229e-06, l1: 0.99797, l2: 0.03181\n",
            "Loss: 4.425016e-06, l1: 0.99796, l2: 0.03181\n",
            "Loss: 4.414816e-06, l1: 0.99794, l2: 0.03180\n",
            "Loss: 4.416591e-06, l1: 0.99801, l2: 0.03181\n",
            "Loss: 4.410673e-06, l1: 0.99797, l2: 0.03181\n",
            "Loss: 4.398044e-06, l1: 0.99796, l2: 0.03180\n",
            "Loss: 4.394387e-06, l1: 0.99797, l2: 0.03180\n",
            "Loss: 4.390913e-06, l1: 0.99803, l2: 0.03181\n",
            "Loss: 4.386263e-06, l1: 0.99803, l2: 0.03181\n",
            "Loss: 4.383006e-06, l1: 0.99803, l2: 0.03181\n",
            "Loss: 4.383288e-06, l1: 0.99795, l2: 0.03180\n",
            "Loss: 4.381509e-06, l1: 0.99799, l2: 0.03180\n",
            "Loss: 4.379816e-06, l1: 0.99798, l2: 0.03180\n",
            "Loss: 4.377157e-06, l1: 0.99797, l2: 0.03181\n",
            "Loss: 4.376683e-06, l1: 0.99795, l2: 0.03181\n",
            "Loss: 4.372128e-06, l1: 0.99796, l2: 0.03181\n",
            "Loss: 4.370175e-06, l1: 0.99798, l2: 0.03181\n",
            "Loss: 4.368024e-06, l1: 0.99801, l2: 0.03181\n",
            "Loss: 4.364378e-06, l1: 0.99802, l2: 0.03181\n",
            "Loss: 4.361751e-06, l1: 0.99803, l2: 0.03181\n",
            "Loss: 4.357122e-06, l1: 0.99804, l2: 0.03181\n",
            "Loss: 4.352321e-06, l1: 0.99805, l2: 0.03181\n",
            "Loss: 4.345176e-06, l1: 0.99810, l2: 0.03181\n",
            "Loss: 5.461088e-06, l1: 0.99733, l2: 0.03179\n",
            "Loss: 4.342358e-06, l1: 0.99806, l2: 0.03181\n",
            "Loss: 4.336601e-06, l1: 0.99810, l2: 0.03181\n",
            "Loss: 4.336008e-06, l1: 0.99811, l2: 0.03181\n",
            "Loss: 4.329888e-06, l1: 0.99814, l2: 0.03181\n",
            "Loss: 4.327467e-06, l1: 0.99813, l2: 0.03181\n",
            "Loss: 4.322131e-06, l1: 0.99810, l2: 0.03181\n",
            "Loss: 4.315355e-06, l1: 0.99808, l2: 0.03181\n",
            "Loss: 4.305168e-06, l1: 0.99807, l2: 0.03181\n",
            "Loss: 4.315832e-06, l1: 0.99813, l2: 0.03181\n",
            "Loss: 4.299099e-06, l1: 0.99809, l2: 0.03181\n",
            "Loss: 4.288638e-06, l1: 0.99811, l2: 0.03180\n",
            "Loss: 4.278155e-06, l1: 0.99813, l2: 0.03180\n",
            "Loss: 4.269843e-06, l1: 0.99815, l2: 0.03180\n",
            "Loss: 4.258859e-06, l1: 0.99817, l2: 0.03180\n",
            "Loss: 4.254183e-06, l1: 0.99817, l2: 0.03180\n",
            "Loss: 4.243543e-06, l1: 0.99821, l2: 0.03180\n",
            "Loss: 4.234643e-06, l1: 0.99824, l2: 0.03180\n",
            "Loss: 4.229240e-06, l1: 0.99831, l2: 0.03180\n",
            "Loss: 4.225442e-06, l1: 0.99835, l2: 0.03180\n",
            "Loss: 4.222744e-06, l1: 0.99840, l2: 0.03180\n",
            "Loss: 4.220668e-06, l1: 0.99843, l2: 0.03180\n",
            "Loss: 4.216578e-06, l1: 0.99847, l2: 0.03180\n",
            "Loss: 4.210785e-06, l1: 0.99855, l2: 0.03180\n",
            "Loss: 4.215212e-06, l1: 0.99861, l2: 0.03180\n",
            "Loss: 4.203434e-06, l1: 0.99857, l2: 0.03180\n",
            "Loss: 4.194306e-06, l1: 0.99858, l2: 0.03180\n",
            "Loss: 4.164235e-06, l1: 0.99859, l2: 0.03180\n",
            "Loss: 4.144734e-06, l1: 0.99875, l2: 0.03180\n",
            "Loss: 4.161499e-06, l1: 0.99884, l2: 0.03180\n",
            "Loss: 4.138427e-06, l1: 0.99878, l2: 0.03180\n",
            "Loss: 4.113927e-06, l1: 0.99887, l2: 0.03180\n",
            "Loss: 4.097515e-06, l1: 0.99897, l2: 0.03180\n",
            "Loss: 4.092054e-06, l1: 0.99902, l2: 0.03180\n",
            "Loss: 4.086957e-06, l1: 0.99901, l2: 0.03180\n",
            "Loss: 4.084717e-06, l1: 0.99903, l2: 0.03180\n",
            "Loss: 4.083495e-06, l1: 0.99905, l2: 0.03180\n",
            "Loss: 4.080326e-06, l1: 0.99909, l2: 0.03180\n",
            "Loss: 4.076010e-06, l1: 0.99911, l2: 0.03180\n",
            "Loss: 4.080092e-06, l1: 0.99906, l2: 0.03181\n",
            "Loss: 4.074520e-06, l1: 0.99910, l2: 0.03181\n",
            "Loss: 4.073860e-06, l1: 0.99922, l2: 0.03180\n",
            "Loss: 4.070341e-06, l1: 0.99916, l2: 0.03181\n",
            "Loss: 4.068022e-06, l1: 0.99913, l2: 0.03180\n",
            "Loss: 4.063719e-06, l1: 0.99910, l2: 0.03180\n",
            "Loss: 4.059169e-06, l1: 0.99910, l2: 0.03180\n",
            "Loss: 4.072916e-06, l1: 0.99900, l2: 0.03179\n",
            "Loss: 4.055265e-06, l1: 0.99907, l2: 0.03179\n",
            "Loss: 4.046652e-06, l1: 0.99915, l2: 0.03179\n",
            "Loss: 4.041712e-06, l1: 0.99923, l2: 0.03179\n",
            "Loss: 4.039344e-06, l1: 0.99928, l2: 0.03179\n",
            "Loss: 4.037258e-06, l1: 0.99931, l2: 0.03179\n",
            "Loss: 4.033676e-06, l1: 0.99933, l2: 0.03180\n",
            "Loss: 4.025661e-06, l1: 0.99936, l2: 0.03179\n",
            "Loss: 4.020470e-06, l1: 0.99945, l2: 0.03180\n",
            "Loss: 4.007565e-06, l1: 0.99943, l2: 0.03179\n",
            "Loss: 4.002055e-06, l1: 0.99942, l2: 0.03179\n",
            "Loss: 3.996371e-06, l1: 0.99941, l2: 0.03179\n",
            "Loss: 3.990763e-06, l1: 0.99943, l2: 0.03179\n",
            "Loss: 3.981955e-06, l1: 0.99945, l2: 0.03179\n",
            "Loss: 3.975464e-06, l1: 0.99948, l2: 0.03179\n",
            "Loss: 3.971939e-06, l1: 0.99950, l2: 0.03179\n",
            "Loss: 3.969561e-06, l1: 0.99948, l2: 0.03180\n",
            "Loss: 3.967781e-06, l1: 0.99945, l2: 0.03180\n",
            "Loss: 3.964877e-06, l1: 0.99936, l2: 0.03180\n",
            "Loss: 3.962643e-06, l1: 0.99930, l2: 0.03180\n",
            "Loss: 3.957954e-06, l1: 0.99935, l2: 0.03180\n",
            "Loss: 3.954645e-06, l1: 0.99939, l2: 0.03180\n",
            "Loss: 3.951930e-06, l1: 0.99939, l2: 0.03180\n",
            "Loss: 3.949533e-06, l1: 0.99939, l2: 0.03180\n",
            "Loss: 3.946062e-06, l1: 0.99936, l2: 0.03181\n",
            "Loss: 3.939419e-06, l1: 0.99928, l2: 0.03181\n",
            "Loss: 3.934716e-06, l1: 0.99923, l2: 0.03181\n",
            "Loss: 3.929765e-06, l1: 0.99922, l2: 0.03181\n",
            "Loss: 3.940134e-06, l1: 0.99910, l2: 0.03181\n",
            "Loss: 3.927308e-06, l1: 0.99918, l2: 0.03181\n",
            "Loss: 3.920854e-06, l1: 0.99921, l2: 0.03181\n",
            "Loss: 3.911895e-06, l1: 0.99927, l2: 0.03181\n",
            "Loss: 3.906748e-06, l1: 0.99932, l2: 0.03181\n",
            "Loss: 3.903025e-06, l1: 0.99931, l2: 0.03182\n",
            "Loss: 3.898763e-06, l1: 0.99941, l2: 0.03181\n",
            "Loss: 3.894452e-06, l1: 0.99937, l2: 0.03181\n",
            "Loss: 3.886733e-06, l1: 0.99927, l2: 0.03181\n",
            "Loss: 3.882918e-06, l1: 0.99929, l2: 0.03181\n",
            "Loss: 3.879446e-06, l1: 0.99933, l2: 0.03182\n",
            "Loss: 3.874158e-06, l1: 0.99936, l2: 0.03182\n",
            "Loss: 3.868237e-06, l1: 0.99937, l2: 0.03182\n",
            "Loss: 3.865816e-06, l1: 0.99938, l2: 0.03182\n",
            "Loss: 3.863402e-06, l1: 0.99936, l2: 0.03182\n",
            "Loss: 3.865752e-06, l1: 0.99941, l2: 0.03182\n",
            "Loss: 3.861853e-06, l1: 0.99938, l2: 0.03182\n",
            "Loss: 3.859773e-06, l1: 0.99938, l2: 0.03182\n",
            "Loss: 3.857597e-06, l1: 0.99937, l2: 0.03182\n",
            "Loss: 3.857018e-06, l1: 0.99943, l2: 0.03182\n",
            "Loss: 3.854158e-06, l1: 0.99943, l2: 0.03182\n",
            "Loss: 3.851999e-06, l1: 0.99942, l2: 0.03182\n",
            "Loss: 3.848993e-06, l1: 0.99950, l2: 0.03182\n",
            "Loss: 3.851816e-06, l1: 0.99940, l2: 0.03182\n",
            "Loss: 3.847316e-06, l1: 0.99946, l2: 0.03182\n",
            "Loss: 3.844254e-06, l1: 0.99946, l2: 0.03182\n",
            "Loss: 3.841061e-06, l1: 0.99946, l2: 0.03181\n",
            "Loss: 3.836225e-06, l1: 0.99945, l2: 0.03181\n",
            "Loss: 3.840661e-06, l1: 0.99931, l2: 0.03181\n",
            "Loss: 3.833442e-06, l1: 0.99940, l2: 0.03181\n",
            "Loss: 3.826462e-06, l1: 0.99937, l2: 0.03180\n",
            "Loss: 3.821359e-06, l1: 0.99935, l2: 0.03180\n",
            "Loss: 3.817169e-06, l1: 0.99932, l2: 0.03180\n",
            "Loss: 3.813989e-06, l1: 0.99929, l2: 0.03180\n",
            "Loss: 3.807716e-06, l1: 0.99926, l2: 0.03180\n",
            "Loss: 3.803747e-06, l1: 0.99924, l2: 0.03180\n",
            "Loss: 3.802093e-06, l1: 0.99923, l2: 0.03180\n",
            "Loss: 3.800787e-06, l1: 0.99923, l2: 0.03180\n",
            "Loss: 3.798348e-06, l1: 0.99922, l2: 0.03180\n",
            "Loss: 3.794442e-06, l1: 0.99922, l2: 0.03180\n",
            "Loss: 3.789870e-06, l1: 0.99921, l2: 0.03181\n",
            "Loss: 3.786082e-06, l1: 0.99917, l2: 0.03181\n",
            "Loss: 3.782388e-06, l1: 0.99918, l2: 0.03181\n",
            "Loss: 3.779182e-06, l1: 0.99922, l2: 0.03181\n",
            "Loss: 3.776378e-06, l1: 0.99923, l2: 0.03181\n",
            "Loss: 3.771259e-06, l1: 0.99930, l2: 0.03181\n",
            "Loss: 3.766732e-06, l1: 0.99930, l2: 0.03181\n",
            "Loss: 3.762437e-06, l1: 0.99929, l2: 0.03181\n",
            "Loss: 3.757434e-06, l1: 0.99925, l2: 0.03181\n",
            "Loss: 3.751990e-06, l1: 0.99922, l2: 0.03181\n",
            "Loss: 3.748394e-06, l1: 0.99918, l2: 0.03181\n",
            "Loss: 3.744642e-06, l1: 0.99914, l2: 0.03181\n",
            "Loss: 3.737268e-06, l1: 0.99909, l2: 0.03181\n",
            "Loss: 3.726863e-06, l1: 0.99902, l2: 0.03181\n",
            "Loss: 3.715692e-06, l1: 0.99894, l2: 0.03181\n",
            "Loss: 3.706719e-06, l1: 0.99887, l2: 0.03181\n",
            "Loss: 3.701189e-06, l1: 0.99882, l2: 0.03181\n",
            "Loss: 3.695703e-06, l1: 0.99875, l2: 0.03181\n",
            "Loss: 3.697144e-06, l1: 0.99864, l2: 0.03180\n",
            "Loss: 3.693548e-06, l1: 0.99870, l2: 0.03181\n",
            "Loss: 3.690428e-06, l1: 0.99868, l2: 0.03181\n",
            "Loss: 3.687634e-06, l1: 0.99866, l2: 0.03181\n",
            "Loss: 3.685926e-06, l1: 0.99863, l2: 0.03181\n",
            "Loss: 3.684286e-06, l1: 0.99862, l2: 0.03181\n",
            "Loss: 3.682700e-06, l1: 0.99862, l2: 0.03181\n",
            "Loss: 3.680723e-06, l1: 0.99858, l2: 0.03181\n",
            "Loss: 3.679448e-06, l1: 0.99858, l2: 0.03181\n",
            "Loss: 3.676582e-06, l1: 0.99855, l2: 0.03181\n",
            "Loss: 3.678575e-06, l1: 0.99861, l2: 0.03180\n",
            "Loss: 3.674044e-06, l1: 0.99858, l2: 0.03180\n",
            "Loss: 3.671204e-06, l1: 0.99857, l2: 0.03180\n",
            "Loss: 3.666674e-06, l1: 0.99856, l2: 0.03180\n",
            "Loss: 3.665262e-06, l1: 0.99862, l2: 0.03180\n",
            "Loss: 3.659590e-06, l1: 0.99861, l2: 0.03180\n",
            "Loss: 3.656734e-06, l1: 0.99861, l2: 0.03180\n",
            "Loss: 3.652474e-06, l1: 0.99863, l2: 0.03180\n",
            "Loss: 3.647471e-06, l1: 0.99863, l2: 0.03180\n",
            "Loss: 3.639420e-06, l1: 0.99863, l2: 0.03180\n",
            "Loss: 3.632594e-06, l1: 0.99862, l2: 0.03181\n",
            "Loss: 3.624133e-06, l1: 0.99861, l2: 0.03181\n",
            "Loss: 3.612438e-06, l1: 0.99860, l2: 0.03181\n",
            "Loss: 3.596442e-06, l1: 0.99860, l2: 0.03181\n",
            "Loss: 3.578033e-06, l1: 0.99866, l2: 0.03182\n",
            "Loss: 3.563062e-06, l1: 0.99874, l2: 0.03182\n",
            "Loss: 3.552016e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 3.546099e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 3.540814e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 3.535712e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 3.530162e-06, l1: 0.99876, l2: 0.03182\n",
            "Loss: 3.529021e-06, l1: 0.99881, l2: 0.03182\n",
            "Loss: 3.522229e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 3.518752e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 3.513664e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 3.507552e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 3.507542e-06, l1: 0.99879, l2: 0.03183\n",
            "Loss: 3.504913e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 3.501431e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 3.499650e-06, l1: 0.99876, l2: 0.03183\n",
            "Loss: 3.495397e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 3.492958e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 3.490209e-06, l1: 0.99872, l2: 0.03183\n",
            "Loss: 3.487528e-06, l1: 0.99874, l2: 0.03183\n",
            "Loss: 3.485455e-06, l1: 0.99874, l2: 0.03184\n",
            "Loss: 3.481037e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 3.475921e-06, l1: 0.99869, l2: 0.03184\n",
            "Loss: 3.472736e-06, l1: 0.99864, l2: 0.03183\n",
            "Loss: 3.465511e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 3.461246e-06, l1: 0.99860, l2: 0.03183\n",
            "Loss: 3.453297e-06, l1: 0.99864, l2: 0.03183\n",
            "Loss: 3.448460e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.439165e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.430389e-06, l1: 0.99863, l2: 0.03183\n",
            "Loss: 3.416972e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.400716e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 3.398950e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 3.378628e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 3.375311e-06, l1: 0.99879, l2: 0.03183\n",
            "Loss: 3.371081e-06, l1: 0.99879, l2: 0.03183\n",
            "Loss: 3.368531e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 3.365320e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 3.363622e-06, l1: 0.99893, l2: 0.03183\n",
            "Loss: 3.367957e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.361615e-06, l1: 0.99891, l2: 0.03184\n",
            "Loss: 3.357445e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 3.353897e-06, l1: 0.99889, l2: 0.03183\n",
            "Loss: 3.348543e-06, l1: 0.99891, l2: 0.03183\n",
            "Loss: 3.345891e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 3.343820e-06, l1: 0.99890, l2: 0.03183\n",
            "Loss: 3.338425e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 3.336291e-06, l1: 0.99881, l2: 0.03184\n",
            "Loss: 3.333853e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 3.331406e-06, l1: 0.99881, l2: 0.03184\n",
            "Loss: 3.328113e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 3.340125e-06, l1: 0.99893, l2: 0.03184\n",
            "Loss: 3.325936e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 3.322058e-06, l1: 0.99887, l2: 0.03183\n",
            "Loss: 3.318951e-06, l1: 0.99886, l2: 0.03183\n",
            "Loss: 3.316892e-06, l1: 0.99883, l2: 0.03183\n",
            "Loss: 3.313023e-06, l1: 0.99879, l2: 0.03183\n",
            "Loss: 3.308342e-06, l1: 0.99873, l2: 0.03183\n",
            "Loss: 3.304700e-06, l1: 0.99861, l2: 0.03183\n",
            "Loss: 3.301527e-06, l1: 0.99848, l2: 0.03183\n",
            "Loss: 3.292975e-06, l1: 0.99858, l2: 0.03183\n",
            "Loss: 3.291465e-06, l1: 0.99860, l2: 0.03183\n",
            "Loss: 3.289967e-06, l1: 0.99860, l2: 0.03183\n",
            "Loss: 3.289036e-06, l1: 0.99859, l2: 0.03183\n",
            "Loss: 3.285832e-06, l1: 0.99857, l2: 0.03183\n",
            "Loss: 3.281083e-06, l1: 0.99856, l2: 0.03183\n",
            "Loss: 3.276569e-06, l1: 0.99855, l2: 0.03183\n",
            "Loss: 3.272307e-06, l1: 0.99853, l2: 0.03183\n",
            "Loss: 3.269407e-06, l1: 0.99852, l2: 0.03183\n",
            "Loss: 3.268000e-06, l1: 0.99851, l2: 0.03183\n",
            "Loss: 3.273504e-06, l1: 0.99857, l2: 0.03182\n",
            "Loss: 3.266952e-06, l1: 0.99853, l2: 0.03182\n",
            "Loss: 3.264944e-06, l1: 0.99853, l2: 0.03183\n",
            "Loss: 3.263196e-06, l1: 0.99854, l2: 0.03183\n",
            "Loss: 3.260153e-06, l1: 0.99859, l2: 0.03183\n",
            "Loss: 3.257972e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.254896e-06, l1: 0.99867, l2: 0.03183\n",
            "Loss: 3.252062e-06, l1: 0.99869, l2: 0.03183\n",
            "Loss: 3.249878e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 3.248685e-06, l1: 0.99866, l2: 0.03183\n",
            "Loss: 3.246667e-06, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.244515e-06, l1: 0.99863, l2: 0.03183\n",
            "Loss: 3.242374e-06, l1: 0.99862, l2: 0.03183\n",
            "Loss: 3.239228e-06, l1: 0.99860, l2: 0.03183\n",
            "Loss: 3.235737e-06, l1: 0.99858, l2: 0.03184\n",
            "Loss: 3.230003e-06, l1: 0.99857, l2: 0.03184\n",
            "Loss: 3.222367e-06, l1: 0.99853, l2: 0.03184\n",
            "Loss: 3.218807e-06, l1: 0.99850, l2: 0.03184\n",
            "Loss: 3.215197e-06, l1: 0.99852, l2: 0.03184\n",
            "Loss: 3.213783e-06, l1: 0.99848, l2: 0.03184\n",
            "Loss: 3.211040e-06, l1: 0.99850, l2: 0.03184\n",
            "Loss: 3.208808e-06, l1: 0.99848, l2: 0.03184\n",
            "Loss: 3.206887e-06, l1: 0.99844, l2: 0.03184\n",
            "Loss: 3.206957e-06, l1: 0.99835, l2: 0.03185\n",
            "Loss: 3.205867e-06, l1: 0.99840, l2: 0.03184\n",
            "Loss: 3.204122e-06, l1: 0.99838, l2: 0.03184\n",
            "Loss: 3.202290e-06, l1: 0.99837, l2: 0.03184\n",
            "Loss: 3.200453e-06, l1: 0.99838, l2: 0.03184\n",
            "Loss: 3.197707e-06, l1: 0.99838, l2: 0.03184\n",
            "Loss: 3.192458e-06, l1: 0.99836, l2: 0.03184\n",
            "Loss: 3.187329e-06, l1: 0.99841, l2: 0.03184\n",
            "Loss: 3.183034e-06, l1: 0.99840, l2: 0.03184\n",
            "Loss: 3.173762e-06, l1: 0.99842, l2: 0.03184\n",
            "Loss: 3.168310e-06, l1: 0.99842, l2: 0.03184\n",
            "Loss: 3.164007e-06, l1: 0.99846, l2: 0.03184\n",
            "Loss: 3.159220e-06, l1: 0.99847, l2: 0.03184\n",
            "Loss: 3.153295e-06, l1: 0.99850, l2: 0.03184\n",
            "Loss: 3.149114e-06, l1: 0.99855, l2: 0.03185\n",
            "Loss: 3.146899e-06, l1: 0.99858, l2: 0.03185\n",
            "Loss: 3.143701e-06, l1: 0.99859, l2: 0.03185\n",
            "Loss: 3.141758e-06, l1: 0.99859, l2: 0.03185\n",
            "Loss: 3.140471e-06, l1: 0.99858, l2: 0.03185\n",
            "Loss: 3.138919e-06, l1: 0.99858, l2: 0.03185\n",
            "Loss: 3.137218e-06, l1: 0.99858, l2: 0.03185\n",
            "Loss: 3.133416e-06, l1: 0.99862, l2: 0.03185\n",
            "Loss: 3.129365e-06, l1: 0.99864, l2: 0.03185\n",
            "Loss: 3.124470e-06, l1: 0.99868, l2: 0.03185\n",
            "Loss: 3.124185e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 3.121838e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 3.114859e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 3.106945e-06, l1: 0.99875, l2: 0.03184\n",
            "Loss: 3.103832e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 3.101568e-06, l1: 0.99877, l2: 0.03184\n",
            "Loss: 3.100165e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 3.098565e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 3.095982e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 3.117029e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 3.095057e-06, l1: 0.99875, l2: 0.03184\n",
            "Loss: 3.091852e-06, l1: 0.99879, l2: 0.03184\n",
            "Loss: 3.089403e-06, l1: 0.99880, l2: 0.03184\n",
            "Loss: 3.086291e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 3.083846e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 3.081565e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 3.078216e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 3.090463e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.076294e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 3.074881e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 3.073593e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 3.072770e-06, l1: 0.99885, l2: 0.03184\n",
            "Loss: 3.071164e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 3.069735e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 3.067678e-06, l1: 0.99892, l2: 0.03184\n",
            "Loss: 3.071073e-06, l1: 0.99899, l2: 0.03184\n",
            "Loss: 3.066455e-06, l1: 0.99894, l2: 0.03184\n",
            "Loss: 3.064797e-06, l1: 0.99897, l2: 0.03184\n",
            "Loss: 3.062778e-06, l1: 0.99896, l2: 0.03184\n",
            "Loss: 3.059679e-06, l1: 0.99897, l2: 0.03184\n",
            "Loss: 3.080252e-06, l1: 0.99902, l2: 0.03185\n",
            "Loss: 3.057722e-06, l1: 0.99898, l2: 0.03184\n",
            "Loss: 3.055807e-06, l1: 0.99899, l2: 0.03184\n",
            "Loss: 3.049207e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.051415e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 3.047536e-06, l1: 0.99907, l2: 0.03184\n",
            "Loss: 3.044461e-06, l1: 0.99909, l2: 0.03184\n",
            "Loss: 3.052173e-06, l1: 0.99916, l2: 0.03184\n",
            "Loss: 3.041876e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 3.035973e-06, l1: 0.99920, l2: 0.03184\n",
            "Loss: 3.032963e-06, l1: 0.99919, l2: 0.03184\n",
            "Loss: 3.029463e-06, l1: 0.99917, l2: 0.03184\n",
            "Loss: 3.026226e-06, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.034717e-06, l1: 0.99910, l2: 0.03184\n",
            "Loss: 3.023366e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 3.014943e-06, l1: 0.99903, l2: 0.03185\n",
            "Loss: 3.011013e-06, l1: 0.99904, l2: 0.03185\n",
            "Loss: 3.007327e-06, l1: 0.99906, l2: 0.03185\n",
            "Loss: 3.005272e-06, l1: 0.99907, l2: 0.03185\n",
            "Loss: 2.999676e-06, l1: 0.99911, l2: 0.03185\n",
            "Loss: 2.995745e-06, l1: 0.99915, l2: 0.03185\n",
            "Loss: 2.991480e-06, l1: 0.99921, l2: 0.03185\n",
            "Loss: 2.990376e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 2.987098e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 2.984210e-06, l1: 0.99927, l2: 0.03185\n",
            "Loss: 2.982593e-06, l1: 0.99929, l2: 0.03185\n",
            "Loss: 2.980890e-06, l1: 0.99930, l2: 0.03185\n",
            "Loss: 2.977170e-06, l1: 0.99932, l2: 0.03185\n",
            "Loss: 2.977632e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 2.974564e-06, l1: 0.99936, l2: 0.03185\n",
            "Loss: 2.969959e-06, l1: 0.99936, l2: 0.03185\n",
            "Loss: 2.965863e-06, l1: 0.99937, l2: 0.03185\n",
            "Loss: 2.964135e-06, l1: 0.99937, l2: 0.03185\n",
            "Loss: 2.960473e-06, l1: 0.99938, l2: 0.03185\n",
            "Loss: 2.957853e-06, l1: 0.99939, l2: 0.03185\n",
            "Loss: 2.954260e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 2.951082e-06, l1: 0.99942, l2: 0.03185\n",
            "Loss: 2.948561e-06, l1: 0.99943, l2: 0.03186\n",
            "Loss: 2.946018e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 2.942021e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 2.939124e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 2.940456e-06, l1: 0.99953, l2: 0.03187\n",
            "Loss: 2.936853e-06, l1: 0.99946, l2: 0.03186\n",
            "Loss: 2.934288e-06, l1: 0.99946, l2: 0.03186\n",
            "Loss: 2.928206e-06, l1: 0.99950, l2: 0.03186\n",
            "Loss: 2.922592e-06, l1: 0.99950, l2: 0.03185\n",
            "Loss: 2.921093e-06, l1: 0.99951, l2: 0.03186\n",
            "Loss: 2.919094e-06, l1: 0.99949, l2: 0.03185\n",
            "Loss: 2.916968e-06, l1: 0.99948, l2: 0.03186\n",
            "Loss: 2.915927e-06, l1: 0.99947, l2: 0.03186\n",
            "Loss: 2.914013e-06, l1: 0.99945, l2: 0.03186\n",
            "Loss: 2.911641e-06, l1: 0.99943, l2: 0.03186\n",
            "Loss: 2.913794e-06, l1: 0.99941, l2: 0.03186\n",
            "Loss: 2.910026e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 2.906610e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 2.902679e-06, l1: 0.99946, l2: 0.03186\n",
            "Loss: 2.899681e-06, l1: 0.99951, l2: 0.03186\n",
            "Loss: 2.895606e-06, l1: 0.99956, l2: 0.03187\n",
            "Loss: 2.896678e-06, l1: 0.99965, l2: 0.03187\n",
            "Loss: 2.892507e-06, l1: 0.99960, l2: 0.03187\n",
            "Loss: 2.886680e-06, l1: 0.99962, l2: 0.03187\n",
            "Loss: 2.883311e-06, l1: 0.99962, l2: 0.03187\n",
            "Loss: 2.880525e-06, l1: 0.99958, l2: 0.03187\n",
            "Loss: 2.880879e-06, l1: 0.99953, l2: 0.03187\n",
            "Loss: 2.878451e-06, l1: 0.99955, l2: 0.03187\n",
            "Loss: 2.875036e-06, l1: 0.99956, l2: 0.03187\n",
            "Loss: 2.872178e-06, l1: 0.99959, l2: 0.03187\n",
            "Loss: 2.870226e-06, l1: 0.99962, l2: 0.03187\n",
            "Loss: 2.867835e-06, l1: 0.99965, l2: 0.03187\n",
            "Loss: 2.865738e-06, l1: 0.99966, l2: 0.03187\n",
            "Loss: 2.862975e-06, l1: 0.99966, l2: 0.03187\n",
            "Loss: 2.863120e-06, l1: 0.99973, l2: 0.03187\n",
            "Loss: 2.861359e-06, l1: 0.99969, l2: 0.03187\n",
            "Loss: 2.859446e-06, l1: 0.99966, l2: 0.03187\n",
            "Loss: 2.858468e-06, l1: 0.99963, l2: 0.03187\n",
            "Loss: 2.857226e-06, l1: 0.99961, l2: 0.03187\n",
            "Loss: 2.853752e-06, l1: 0.99958, l2: 0.03187\n",
            "Loss: 2.848530e-06, l1: 0.99958, l2: 0.03187\n",
            "Loss: 2.843359e-06, l1: 0.99956, l2: 0.03187\n",
            "Loss: 2.837968e-06, l1: 0.99955, l2: 0.03187\n",
            "Loss: 2.833031e-06, l1: 0.99948, l2: 0.03187\n",
            "Loss: 2.828207e-06, l1: 0.99946, l2: 0.03187\n",
            "Loss: 2.819761e-06, l1: 0.99936, l2: 0.03188\n",
            "Loss: 2.814646e-06, l1: 0.99932, l2: 0.03188\n",
            "Loss: 2.809783e-06, l1: 0.99932, l2: 0.03188\n",
            "Loss: 2.803797e-06, l1: 0.99929, l2: 0.03188\n",
            "Loss: 2.798336e-06, l1: 0.99929, l2: 0.03188\n",
            "Loss: 2.790779e-06, l1: 0.99927, l2: 0.03188\n",
            "Loss: 2.784665e-06, l1: 0.99927, l2: 0.03188\n",
            "Loss: 2.778592e-06, l1: 0.99928, l2: 0.03188\n",
            "Loss: 2.772010e-06, l1: 0.99922, l2: 0.03187\n",
            "Loss: 2.764745e-06, l1: 0.99920, l2: 0.03187\n",
            "Loss: 2.759780e-06, l1: 0.99916, l2: 0.03187\n",
            "Loss: 2.756502e-06, l1: 0.99910, l2: 0.03187\n",
            "Loss: 2.752795e-06, l1: 0.99902, l2: 0.03187\n",
            "Loss: 2.750669e-06, l1: 0.99893, l2: 0.03187\n",
            "Loss: 2.745438e-06, l1: 0.99893, l2: 0.03187\n",
            "Loss: 2.743339e-06, l1: 0.99894, l2: 0.03187\n",
            "Loss: 2.741526e-06, l1: 0.99896, l2: 0.03187\n",
            "Loss: 2.739452e-06, l1: 0.99896, l2: 0.03187\n",
            "Loss: 2.735228e-06, l1: 0.99894, l2: 0.03188\n",
            "Loss: 2.772608e-06, l1: 0.99865, l2: 0.03187\n",
            "Loss: 2.734462e-06, l1: 0.99890, l2: 0.03188\n",
            "Loss: 2.731212e-06, l1: 0.99886, l2: 0.03188\n",
            "Loss: 2.728088e-06, l1: 0.99886, l2: 0.03188\n",
            "Loss: 2.728884e-06, l1: 0.99874, l2: 0.03187\n",
            "Loss: 2.726267e-06, l1: 0.99881, l2: 0.03187\n",
            "Loss: 2.725492e-06, l1: 0.99881, l2: 0.03187\n",
            "Loss: 2.721457e-06, l1: 0.99878, l2: 0.03187\n",
            "Loss: 2.720020e-06, l1: 0.99877, l2: 0.03187\n",
            "Loss: 2.717070e-06, l1: 0.99873, l2: 0.03187\n",
            "Loss: 2.726107e-06, l1: 0.99855, l2: 0.03188\n",
            "Loss: 2.715479e-06, l1: 0.99868, l2: 0.03187\n",
            "Loss: 2.712069e-06, l1: 0.99866, l2: 0.03187\n",
            "Loss: 2.708186e-06, l1: 0.99862, l2: 0.03187\n",
            "Loss: 2.706375e-06, l1: 0.99861, l2: 0.03187\n",
            "Loss: 2.702583e-06, l1: 0.99859, l2: 0.03187\n",
            "Loss: 2.706569e-06, l1: 0.99861, l2: 0.03187\n",
            "Loss: 2.700132e-06, l1: 0.99859, l2: 0.03187\n",
            "Loss: 2.696966e-06, l1: 0.99860, l2: 0.03187\n",
            "Loss: 2.693263e-06, l1: 0.99860, l2: 0.03187\n",
            "Loss: 2.690216e-06, l1: 0.99861, l2: 0.03187\n",
            "Loss: 2.688430e-06, l1: 0.99866, l2: 0.03187\n",
            "Loss: 2.682844e-06, l1: 0.99864, l2: 0.03186\n",
            "Loss: 2.678786e-06, l1: 0.99868, l2: 0.03187\n",
            "Loss: 2.672663e-06, l1: 0.99870, l2: 0.03186\n",
            "Loss: 2.664543e-06, l1: 0.99874, l2: 0.03186\n",
            "Loss: 2.659568e-06, l1: 0.99873, l2: 0.03186\n",
            "Loss: 2.935252e-06, l1: 0.99932, l2: 0.03183\n",
            "Loss: 2.655826e-06, l1: 0.99879, l2: 0.03185\n",
            "Loss: 2.651779e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 2.648665e-06, l1: 0.99882, l2: 0.03185\n",
            "Loss: 2.644606e-06, l1: 0.99881, l2: 0.03185\n",
            "Loss: 2.638487e-06, l1: 0.99884, l2: 0.03185\n",
            "Loss: 2.632330e-06, l1: 0.99880, l2: 0.03185\n",
            "Loss: 2.626078e-06, l1: 0.99882, l2: 0.03185\n",
            "Loss: 2.623422e-06, l1: 0.99893, l2: 0.03185\n",
            "Loss: 2.618343e-06, l1: 0.99886, l2: 0.03185\n",
            "Loss: 2.616841e-06, l1: 0.99886, l2: 0.03185\n",
            "Loss: 2.613739e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 2.610890e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 2.604654e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 2.601737e-06, l1: 0.99884, l2: 0.03184\n",
            "Loss: 2.595195e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 2.590643e-06, l1: 0.99881, l2: 0.03184\n",
            "Loss: 2.588190e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.585624e-06, l1: 0.99878, l2: 0.03184\n",
            "Loss: 2.581212e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 2.662249e-06, l1: 0.99883, l2: 0.03182\n",
            "Loss: 2.580381e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 2.577570e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 2.574768e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 2.572187e-06, l1: 0.99882, l2: 0.03183\n",
            "Loss: 2.569591e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 2.566981e-06, l1: 0.99878, l2: 0.03183\n",
            "Loss: 2.565018e-06, l1: 0.99876, l2: 0.03183\n",
            "Loss: 2.562317e-06, l1: 0.99872, l2: 0.03183\n",
            "Loss: 2.560264e-06, l1: 0.99871, l2: 0.03183\n",
            "Loss: 2.556420e-06, l1: 0.99875, l2: 0.03183\n",
            "Loss: 3.160013e-06, l1: 0.99798, l2: 0.03181\n",
            "Loss: 2.556160e-06, l1: 0.99872, l2: 0.03183\n",
            "Loss: 2.554330e-06, l1: 0.99877, l2: 0.03183\n",
            "Loss: 2.552906e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 2.551806e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 2.550729e-06, l1: 0.99885, l2: 0.03183\n",
            "Loss: 2.549117e-06, l1: 0.99886, l2: 0.03183\n",
            "Loss: 2.546968e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 2.564545e-06, l1: 0.99856, l2: 0.03182\n",
            "Loss: 2.546585e-06, l1: 0.99881, l2: 0.03183\n",
            "Loss: 2.544978e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 2.542728e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 2.539430e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 2.537561e-06, l1: 0.99882, l2: 0.03182\n",
            "Loss: 2.536331e-06, l1: 0.99885, l2: 0.03182\n",
            "Loss: 2.534527e-06, l1: 0.99887, l2: 0.03182\n",
            "Loss: 2.532671e-06, l1: 0.99887, l2: 0.03182\n",
            "Loss: 2.527655e-06, l1: 0.99885, l2: 0.03182\n",
            "Loss: 2.524417e-06, l1: 0.99882, l2: 0.03182\n",
            "Loss: 2.522004e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 2.531988e-06, l1: 0.99893, l2: 0.03182\n",
            "Loss: 2.521203e-06, l1: 0.99882, l2: 0.03182\n",
            "Loss: 2.519847e-06, l1: 0.99881, l2: 0.03182\n",
            "Loss: 2.518095e-06, l1: 0.99881, l2: 0.03181\n",
            "Loss: 2.516334e-06, l1: 0.99882, l2: 0.03181\n",
            "Loss: 2.514156e-06, l1: 0.99884, l2: 0.03181\n",
            "Loss: 2.544341e-06, l1: 0.99865, l2: 0.03182\n",
            "Loss: 2.513292e-06, l1: 0.99881, l2: 0.03181\n",
            "Loss: 2.510484e-06, l1: 0.99881, l2: 0.03181\n",
            "Loss: 2.507909e-06, l1: 0.99880, l2: 0.03181\n",
            "Loss: 2.505629e-06, l1: 0.99876, l2: 0.03181\n",
            "Loss: 2.503803e-06, l1: 0.99870, l2: 0.03181\n",
            "Loss: 2.501451e-06, l1: 0.99866, l2: 0.03181\n",
            "Loss: 2.500541e-06, l1: 0.99863, l2: 0.03181\n",
            "Loss: 2.499436e-06, l1: 0.99866, l2: 0.03181\n",
            "Loss: 2.498911e-06, l1: 0.99867, l2: 0.03181\n",
            "Loss: 2.497672e-06, l1: 0.99870, l2: 0.03181\n",
            "Loss: 2.495026e-06, l1: 0.99872, l2: 0.03181\n",
            "Loss: 2.489691e-06, l1: 0.99876, l2: 0.03181\n",
            "Loss: 2.482805e-06, l1: 0.99873, l2: 0.03181\n",
            "Loss: 2.475395e-06, l1: 0.99875, l2: 0.03181\n",
            "Loss: 2.470200e-06, l1: 0.99870, l2: 0.03180\n",
            "Loss: 2.466596e-06, l1: 0.99864, l2: 0.03181\n",
            "Loss: 2.464043e-06, l1: 0.99863, l2: 0.03181\n",
            "Loss: 2.461669e-06, l1: 0.99863, l2: 0.03181\n",
            "Loss: 2.458940e-06, l1: 0.99866, l2: 0.03181\n",
            "Loss: 2.458079e-06, l1: 0.99867, l2: 0.03181\n",
            "Loss: 2.456928e-06, l1: 0.99867, l2: 0.03181\n",
            "Loss: 2.455363e-06, l1: 0.99865, l2: 0.03181\n",
            "Loss: 2.453359e-06, l1: 0.99862, l2: 0.03180\n",
            "Loss: 2.450753e-06, l1: 0.99861, l2: 0.03180\n",
            "Loss: 2.446763e-06, l1: 0.99861, l2: 0.03180\n",
            "Loss: 2.444905e-06, l1: 0.99868, l2: 0.03180\n",
            "Loss: 2.441110e-06, l1: 0.99864, l2: 0.03180\n",
            "Loss: 2.439331e-06, l1: 0.99865, l2: 0.03180\n",
            "Loss: 2.436063e-06, l1: 0.99866, l2: 0.03180\n",
            "Loss: 2.439276e-06, l1: 0.99860, l2: 0.03180\n",
            "Loss: 2.435553e-06, l1: 0.99865, l2: 0.03180\n",
            "Loss: 2.434084e-06, l1: 0.99865, l2: 0.03180\n",
            "Loss: 2.431609e-06, l1: 0.99864, l2: 0.03179\n",
            "Loss: 2.435904e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.430010e-06, l1: 0.99867, l2: 0.03179\n",
            "Loss: 2.428329e-06, l1: 0.99868, l2: 0.03179\n",
            "Loss: 2.426330e-06, l1: 0.99869, l2: 0.03179\n",
            "Loss: 2.423986e-06, l1: 0.99868, l2: 0.03179\n",
            "Loss: 2.422642e-06, l1: 0.99878, l2: 0.03180\n",
            "Loss: 2.420603e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.419580e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.417986e-06, l1: 0.99876, l2: 0.03179\n",
            "Loss: 2.419594e-06, l1: 0.99874, l2: 0.03179\n",
            "Loss: 2.416617e-06, l1: 0.99875, l2: 0.03179\n",
            "Loss: 2.414808e-06, l1: 0.99875, l2: 0.03179\n",
            "Loss: 2.409217e-06, l1: 0.99871, l2: 0.03179\n",
            "Loss: 2.407930e-06, l1: 0.99871, l2: 0.03179\n",
            "Loss: 2.407162e-06, l1: 0.99871, l2: 0.03179\n",
            "Loss: 2.404981e-06, l1: 0.99875, l2: 0.03179\n",
            "Loss: 2.401890e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.400518e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.446245e-06, l1: 0.99848, l2: 0.03178\n",
            "Loss: 2.400352e-06, l1: 0.99872, l2: 0.03179\n",
            "Loss: 2.398998e-06, l1: 0.99872, l2: 0.03179\n",
            "Loss: 2.397881e-06, l1: 0.99872, l2: 0.03179\n",
            "Loss: 2.396465e-06, l1: 0.99870, l2: 0.03179\n",
            "Loss: 2.395083e-06, l1: 0.99869, l2: 0.03179\n",
            "Loss: 2.392045e-06, l1: 0.99867, l2: 0.03179\n",
            "Loss: 2.388608e-06, l1: 0.99867, l2: 0.03179\n",
            "Loss: 2.388022e-06, l1: 0.99865, l2: 0.03179\n",
            "Loss: 2.385332e-06, l1: 0.99868, l2: 0.03179\n",
            "Loss: 2.384570e-06, l1: 0.99869, l2: 0.03179\n",
            "Loss: 2.383505e-06, l1: 0.99870, l2: 0.03179\n",
            "Loss: 2.381728e-06, l1: 0.99871, l2: 0.03179\n",
            "Loss: 2.379347e-06, l1: 0.99870, l2: 0.03179\n",
            "Loss: 2.377230e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.375075e-06, l1: 0.99872, l2: 0.03179\n",
            "Loss: 2.373952e-06, l1: 0.99871, l2: 0.03179\n",
            "Loss: 2.373149e-06, l1: 0.99871, l2: 0.03179\n",
            "Loss: 2.371160e-06, l1: 0.99874, l2: 0.03179\n",
            "Loss: 2.375964e-06, l1: 0.99872, l2: 0.03179\n",
            "Loss: 2.370195e-06, l1: 0.99873, l2: 0.03179\n",
            "Loss: 2.367875e-06, l1: 0.99876, l2: 0.03179\n",
            "Loss: 2.383150e-06, l1: 0.99883, l2: 0.03179\n",
            "Loss: 2.366461e-06, l1: 0.99878, l2: 0.03179\n",
            "Loss: 2.362383e-06, l1: 0.99882, l2: 0.03179\n",
            "Loss: 2.360515e-06, l1: 0.99878, l2: 0.03179\n",
            "Loss: 2.357199e-06, l1: 0.99878, l2: 0.03179\n",
            "Loss: 2.355221e-06, l1: 0.99876, l2: 0.03179\n",
            "Loss: 2.353368e-06, l1: 0.99877, l2: 0.03179\n",
            "Loss: 2.350493e-06, l1: 0.99878, l2: 0.03179\n",
            "Loss: 2.349482e-06, l1: 0.99884, l2: 0.03179\n",
            "Loss: 2.346707e-06, l1: 0.99884, l2: 0.03179\n",
            "Loss: 2.345565e-06, l1: 0.99883, l2: 0.03179\n",
            "Loss: 2.344286e-06, l1: 0.99880, l2: 0.03179\n",
            "Loss: 2.343089e-06, l1: 0.99880, l2: 0.03179\n",
            "Loss: 2.342286e-06, l1: 0.99879, l2: 0.03179\n",
            "Loss: 2.341092e-06, l1: 0.99878, l2: 0.03179\n",
            "Loss: 2.341146e-06, l1: 0.99875, l2: 0.03180\n",
            "Loss: 2.340628e-06, l1: 0.99876, l2: 0.03180\n",
            "Loss: 2.339941e-06, l1: 0.99875, l2: 0.03180\n",
            "Loss: 2.338972e-06, l1: 0.99875, l2: 0.03180\n",
            "Loss: 2.338459e-06, l1: 0.99876, l2: 0.03180\n",
            "Loss: 2.337873e-06, l1: 0.99877, l2: 0.03180\n",
            "Loss: 2.338364e-06, l1: 0.99870, l2: 0.03180\n",
            "Loss: 2.337403e-06, l1: 0.99874, l2: 0.03180\n",
            "Loss: 2.336798e-06, l1: 0.99881, l2: 0.03180\n",
            "Loss: 2.336106e-06, l1: 0.99880, l2: 0.03180\n",
            "Loss: 2.335403e-06, l1: 0.99880, l2: 0.03180\n",
            "Loss: 2.335093e-06, l1: 0.99880, l2: 0.03180\n",
            "Loss: 2.334674e-06, l1: 0.99881, l2: 0.03180\n",
            "Loss: 2.333916e-06, l1: 0.99881, l2: 0.03180\n",
            "Loss: 2.332539e-06, l1: 0.99880, l2: 0.03180\n",
            "Loss: 2.365242e-06, l1: 0.99872, l2: 0.03180\n",
            "Loss: 2.332228e-06, l1: 0.99879, l2: 0.03180\n",
            "Loss: 2.331026e-06, l1: 0.99877, l2: 0.03180\n",
            "Loss: 2.329088e-06, l1: 0.99874, l2: 0.03180\n",
            "Loss: 2.327362e-06, l1: 0.99871, l2: 0.03180\n",
            "Loss: 2.325988e-06, l1: 0.99872, l2: 0.03180\n",
            "Loss: 2.324527e-06, l1: 0.99865, l2: 0.03180\n",
            "Loss: 2.323039e-06, l1: 0.99869, l2: 0.03180\n",
            "Loss: 2.321567e-06, l1: 0.99873, l2: 0.03180\n",
            "Loss: 2.321007e-06, l1: 0.99874, l2: 0.03181\n",
            "Loss: 2.318821e-06, l1: 0.99876, l2: 0.03181\n",
            "Loss: 2.315839e-06, l1: 0.99878, l2: 0.03181\n",
            "Loss: 2.312363e-06, l1: 0.99878, l2: 0.03181\n",
            "Loss: 2.308834e-06, l1: 0.99875, l2: 0.03182\n",
            "Loss: 2.306520e-06, l1: 0.99876, l2: 0.03182\n",
            "Loss: 2.303915e-06, l1: 0.99875, l2: 0.03182\n",
            "Loss: 2.302574e-06, l1: 0.99874, l2: 0.03182\n",
            "Loss: 2.301409e-06, l1: 0.99875, l2: 0.03182\n",
            "Loss: 2.299734e-06, l1: 0.99877, l2: 0.03182\n",
            "Loss: 2.297120e-06, l1: 0.99880, l2: 0.03183\n",
            "Loss: 2.292933e-06, l1: 0.99883, l2: 0.03183\n",
            "Loss: 2.291363e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 2.289621e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 2.288775e-06, l1: 0.99886, l2: 0.03183\n",
            "Loss: 2.287859e-06, l1: 0.99887, l2: 0.03183\n",
            "Loss: 2.286396e-06, l1: 0.99890, l2: 0.03184\n",
            "Loss: 2.286992e-06, l1: 0.99897, l2: 0.03184\n",
            "Loss: 2.285513e-06, l1: 0.99893, l2: 0.03184\n",
            "Loss: 2.284222e-06, l1: 0.99897, l2: 0.03184\n",
            "Loss: 2.283547e-06, l1: 0.99898, l2: 0.03184\n",
            "Loss: 2.282304e-06, l1: 0.99899, l2: 0.03184\n",
            "Loss: 2.280699e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 2.278720e-06, l1: 0.99904, l2: 0.03185\n",
            "Loss: 2.276467e-06, l1: 0.99904, l2: 0.03185\n",
            "Loss: 2.273099e-06, l1: 0.99903, l2: 0.03185\n",
            "Loss: 2.272085e-06, l1: 0.99902, l2: 0.03185\n",
            "Loss: 2.270168e-06, l1: 0.99905, l2: 0.03185\n",
            "Loss: 2.268944e-06, l1: 0.99907, l2: 0.03185\n",
            "Loss: 2.267967e-06, l1: 0.99909, l2: 0.03185\n",
            "Loss: 2.269096e-06, l1: 0.99907, l2: 0.03186\n",
            "Loss: 2.267640e-06, l1: 0.99908, l2: 0.03185\n",
            "Loss: 2.266808e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 2.265487e-06, l1: 0.99911, l2: 0.03186\n",
            "Loss: 2.264507e-06, l1: 0.99914, l2: 0.03186\n",
            "Loss: 2.263495e-06, l1: 0.99914, l2: 0.03186\n",
            "Loss: 2.263065e-06, l1: 0.99914, l2: 0.03186\n",
            "Loss: 2.262260e-06, l1: 0.99915, l2: 0.03186\n",
            "Loss: 2.261477e-06, l1: 0.99915, l2: 0.03186\n",
            "Loss: 2.260254e-06, l1: 0.99918, l2: 0.03186\n",
            "Loss: 2.259919e-06, l1: 0.99910, l2: 0.03186\n",
            "Loss: 2.257633e-06, l1: 0.99914, l2: 0.03186\n",
            "Loss: 2.255756e-06, l1: 0.99916, l2: 0.03186\n",
            "Loss: 2.253308e-06, l1: 0.99916, l2: 0.03186\n",
            "Loss: 2.252051e-06, l1: 0.99920, l2: 0.03186\n",
            "Loss: 2.248920e-06, l1: 0.99919, l2: 0.03186\n",
            "Loss: 2.247827e-06, l1: 0.99918, l2: 0.03186\n",
            "Loss: 2.246538e-06, l1: 0.99917, l2: 0.03186\n",
            "Loss: 2.244912e-06, l1: 0.99916, l2: 0.03186\n",
            "Loss: 2.243373e-06, l1: 0.99912, l2: 0.03186\n",
            "Loss: 2.243829e-06, l1: 0.99919, l2: 0.03187\n",
            "Loss: 2.241895e-06, l1: 0.99915, l2: 0.03187\n",
            "Loss: 2.240567e-06, l1: 0.99915, l2: 0.03187\n",
            "Loss: 2.239407e-06, l1: 0.99913, l2: 0.03187\n",
            "Loss: 2.239130e-06, l1: 0.99912, l2: 0.03187\n",
            "Loss: 2.236731e-06, l1: 0.99909, l2: 0.03187\n",
            "Loss: 2.238178e-06, l1: 0.99916, l2: 0.03187\n",
            "Loss: 2.235527e-06, l1: 0.99911, l2: 0.03187\n",
            "Loss: 2.233595e-06, l1: 0.99911, l2: 0.03187\n",
            "Loss: 2.232014e-06, l1: 0.99913, l2: 0.03187\n",
            "Loss: 2.229651e-06, l1: 0.99914, l2: 0.03187\n",
            "Loss: 2.227318e-06, l1: 0.99916, l2: 0.03187\n",
            "Loss: 2.224679e-06, l1: 0.99917, l2: 0.03187\n",
            "Loss: 2.223304e-06, l1: 0.99917, l2: 0.03187\n",
            "Loss: 2.221876e-06, l1: 0.99917, l2: 0.03187\n",
            "Loss: 2.219317e-06, l1: 0.99919, l2: 0.03187\n",
            "Loss: 2.218180e-06, l1: 0.99924, l2: 0.03187\n",
            "Loss: 2.215796e-06, l1: 0.99929, l2: 0.03188\n",
            "Loss: 2.213281e-06, l1: 0.99925, l2: 0.03188\n",
            "Loss: 2.211713e-06, l1: 0.99923, l2: 0.03188\n",
            "Loss: 2.209722e-06, l1: 0.99922, l2: 0.03187\n",
            "Loss: 2.208221e-06, l1: 0.99923, l2: 0.03187\n",
            "Loss: 2.206557e-06, l1: 0.99925, l2: 0.03188\n",
            "Loss: 2.207013e-06, l1: 0.99925, l2: 0.03187\n",
            "Loss: 2.205726e-06, l1: 0.99925, l2: 0.03188\n",
            "Loss: 2.205280e-06, l1: 0.99929, l2: 0.03188\n",
            "Loss: 2.203303e-06, l1: 0.99928, l2: 0.03188\n",
            "Loss: 2.200635e-06, l1: 0.99927, l2: 0.03187\n",
            "Loss: 2.196632e-06, l1: 0.99927, l2: 0.03187\n",
            "Loss: 2.192526e-06, l1: 0.99927, l2: 0.03187\n",
            "Loss: 2.188197e-06, l1: 0.99933, l2: 0.03187\n",
            "Loss: 2.183972e-06, l1: 0.99934, l2: 0.03187\n",
            "Loss: 2.180057e-06, l1: 0.99935, l2: 0.03187\n",
            "Loss: 2.177171e-06, l1: 0.99935, l2: 0.03187\n",
            "Loss: 2.176083e-06, l1: 0.99935, l2: 0.03186\n",
            "Loss: 2.175159e-06, l1: 0.99933, l2: 0.03186\n",
            "Loss: 2.174524e-06, l1: 0.99932, l2: 0.03186\n",
            "Loss: 2.172840e-06, l1: 0.99931, l2: 0.03186\n",
            "Loss: 2.171128e-06, l1: 0.99931, l2: 0.03186\n",
            "Loss: 2.169664e-06, l1: 0.99933, l2: 0.03186\n",
            "Loss: 2.167963e-06, l1: 0.99935, l2: 0.03186\n",
            "Loss: 2.166204e-06, l1: 0.99937, l2: 0.03187\n",
            "Loss: 2.163127e-06, l1: 0.99937, l2: 0.03186\n",
            "Loss: 2.161416e-06, l1: 0.99943, l2: 0.03187\n",
            "Loss: 2.159320e-06, l1: 0.99940, l2: 0.03187\n",
            "Loss: 2.157899e-06, l1: 0.99938, l2: 0.03186\n",
            "Loss: 2.156311e-06, l1: 0.99937, l2: 0.03186\n",
            "Loss: 2.153914e-06, l1: 0.99936, l2: 0.03186\n",
            "Loss: 2.151056e-06, l1: 0.99938, l2: 0.03186\n",
            "Loss: 2.149917e-06, l1: 0.99931, l2: 0.03187\n",
            "Loss: 2.146632e-06, l1: 0.99937, l2: 0.03187\n",
            "Loss: 2.144905e-06, l1: 0.99940, l2: 0.03187\n",
            "Loss: 2.141990e-06, l1: 0.99944, l2: 0.03187\n",
            "Loss: 2.141026e-06, l1: 0.99945, l2: 0.03187\n",
            "Loss: 2.138968e-06, l1: 0.99946, l2: 0.03187\n",
            "Loss: 2.137954e-06, l1: 0.99943, l2: 0.03187\n",
            "Loss: 2.136906e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 2.135025e-06, l1: 0.99935, l2: 0.03186\n",
            "Loss: 2.132753e-06, l1: 0.99937, l2: 0.03186\n",
            "Loss: 2.129351e-06, l1: 0.99939, l2: 0.03186\n",
            "Loss: 2.127342e-06, l1: 0.99938, l2: 0.03186\n",
            "Loss: 2.123717e-06, l1: 0.99935, l2: 0.03186\n",
            "Loss: 2.124403e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 2.121882e-06, l1: 0.99932, l2: 0.03186\n",
            "Loss: 2.120111e-06, l1: 0.99931, l2: 0.03186\n",
            "Loss: 2.117170e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 2.115981e-06, l1: 0.99927, l2: 0.03186\n",
            "Loss: 2.114871e-06, l1: 0.99927, l2: 0.03186\n",
            "Loss: 2.113599e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 2.112575e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 2.111655e-06, l1: 0.99929, l2: 0.03186\n",
            "Loss: 2.110861e-06, l1: 0.99929, l2: 0.03186\n",
            "Loss: 2.109840e-06, l1: 0.99930, l2: 0.03186\n",
            "Loss: 2.108429e-06, l1: 0.99932, l2: 0.03186\n",
            "Loss: 2.106534e-06, l1: 0.99935, l2: 0.03187\n",
            "Loss: 2.104832e-06, l1: 0.99938, l2: 0.03187\n",
            "Loss: 2.103018e-06, l1: 0.99940, l2: 0.03187\n",
            "Loss: 2.100951e-06, l1: 0.99941, l2: 0.03187\n",
            "Loss: 2.108647e-06, l1: 0.99933, l2: 0.03186\n",
            "Loss: 2.100237e-06, l1: 0.99939, l2: 0.03186\n",
            "Loss: 2.098465e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 2.096904e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 2.095496e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 2.093721e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 2.090280e-06, l1: 0.99943, l2: 0.03186\n",
            "Loss: 2.084622e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 2.125861e-06, l1: 0.99952, l2: 0.03185\n",
            "Loss: 2.083912e-06, l1: 0.99946, l2: 0.03185\n",
            "Loss: 2.096446e-06, l1: 0.99940, l2: 0.03184\n",
            "Loss: 2.082602e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 2.079861e-06, l1: 0.99943, l2: 0.03185\n",
            "Loss: 2.077792e-06, l1: 0.99943, l2: 0.03185\n",
            "Loss: 2.075190e-06, l1: 0.99944, l2: 0.03185\n",
            "Loss: 2.071610e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 2.068086e-06, l1: 0.99948, l2: 0.03185\n",
            "Loss: 2.064442e-06, l1: 0.99948, l2: 0.03185\n",
            "Loss: 2.063257e-06, l1: 0.99948, l2: 0.03185\n",
            "Loss: 2.062559e-06, l1: 0.99949, l2: 0.03185\n",
            "Loss: 2.061947e-06, l1: 0.99949, l2: 0.03185\n",
            "Loss: 2.061511e-06, l1: 0.99949, l2: 0.03185\n",
            "Loss: 2.060123e-06, l1: 0.99947, l2: 0.03185\n",
            "Loss: 2.058674e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 2.057406e-06, l1: 0.99941, l2: 0.03184\n",
            "Loss: 2.056744e-06, l1: 0.99940, l2: 0.03184\n",
            "Loss: 2.055423e-06, l1: 0.99937, l2: 0.03185\n",
            "Loss: 2.053316e-06, l1: 0.99933, l2: 0.03185\n",
            "Loss: 2.051182e-06, l1: 0.99932, l2: 0.03185\n",
            "Loss: 2.047717e-06, l1: 0.99931, l2: 0.03185\n",
            "Loss: 2.045243e-06, l1: 0.99930, l2: 0.03185\n",
            "Loss: 2.042954e-06, l1: 0.99931, l2: 0.03185\n",
            "Loss: 2.041435e-06, l1: 0.99930, l2: 0.03185\n",
            "Loss: 2.040113e-06, l1: 0.99929, l2: 0.03185\n",
            "Loss: 2.039052e-06, l1: 0.99927, l2: 0.03185\n",
            "Loss: 2.037086e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 2.035534e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.034480e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.033708e-06, l1: 0.99923, l2: 0.03184\n",
            "Loss: 2.032538e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.031126e-06, l1: 0.99922, l2: 0.03184\n",
            "Loss: 2.029982e-06, l1: 0.99923, l2: 0.03184\n",
            "Loss: 2.029185e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.028677e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.028169e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.027778e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.026601e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.025872e-06, l1: 0.99923, l2: 0.03184\n",
            "Loss: 2.025150e-06, l1: 0.99924, l2: 0.03184\n",
            "Loss: 2.022859e-06, l1: 0.99923, l2: 0.03184\n",
            "Loss: 2.020755e-06, l1: 0.99920, l2: 0.03184\n",
            "Loss: 2.019243e-06, l1: 0.99917, l2: 0.03184\n",
            "Loss: 2.017713e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 2.016971e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 2.013024e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 2.021513e-06, l1: 0.99908, l2: 0.03184\n",
            "Loss: 2.011153e-06, l1: 0.99910, l2: 0.03184\n",
            "Loss: 2.009229e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 2.005964e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 2.003412e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 2.005438e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 2.002786e-06, l1: 0.99914, l2: 0.03184\n",
            "Loss: 2.001681e-06, l1: 0.99912, l2: 0.03184\n",
            "Loss: 2.001091e-06, l1: 0.99911, l2: 0.03184\n",
            "Loss: 2.000408e-06, l1: 0.99909, l2: 0.03184\n",
            "Loss: 2.001159e-06, l1: 0.99909, l2: 0.03184\n",
            "Loss: 1.999899e-06, l1: 0.99909, l2: 0.03184\n",
            "Loss: 1.999203e-06, l1: 0.99909, l2: 0.03184\n",
            "Loss: 1.997188e-06, l1: 0.99909, l2: 0.03183\n",
            "Loss: 1.996288e-06, l1: 0.99907, l2: 0.03183\n",
            "Loss: 1.995328e-06, l1: 0.99907, l2: 0.03183\n",
            "Loss: 1.994379e-06, l1: 0.99907, l2: 0.03184\n",
            "Loss: 1.993516e-06, l1: 0.99907, l2: 0.03184\n",
            "Loss: 1.992212e-06, l1: 0.99906, l2: 0.03184\n",
            "Loss: 1.998977e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 1.991716e-06, l1: 0.99905, l2: 0.03184\n",
            "Loss: 1.990652e-06, l1: 0.99906, l2: 0.03184\n",
            "Loss: 1.988226e-06, l1: 0.99904, l2: 0.03184\n",
            "Loss: 1.989666e-06, l1: 0.99901, l2: 0.03184\n",
            "Loss: 1.987325e-06, l1: 0.99903, l2: 0.03184\n",
            "Loss: 1.986412e-06, l1: 0.99901, l2: 0.03184\n",
            "Loss: 1.985537e-06, l1: 0.99900, l2: 0.03184\n",
            "Loss: 1.983801e-06, l1: 0.99897, l2: 0.03184\n",
            "Loss: 1.981615e-06, l1: 0.99894, l2: 0.03184\n",
            "Loss: 1.979796e-06, l1: 0.99891, l2: 0.03183\n",
            "Loss: 1.976683e-06, l1: 0.99887, l2: 0.03183\n",
            "Loss: 1.975201e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 1.974265e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 1.973475e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 1.972278e-06, l1: 0.99888, l2: 0.03183\n",
            "Loss: 1.970389e-06, l1: 0.99886, l2: 0.03183\n",
            "Loss: 1.967858e-06, l1: 0.99885, l2: 0.03183\n",
            "Loss: 1.965534e-06, l1: 0.99883, l2: 0.03183\n",
            "Loss: 1.965371e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 1.964630e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 1.962328e-06, l1: 0.99884, l2: 0.03183\n",
            "Loss: 1.959296e-06, l1: 0.99887, l2: 0.03183\n",
            "Loss: 1.954175e-06, l1: 0.99890, l2: 0.03183\n",
            "Loss: 1.951561e-06, l1: 0.99893, l2: 0.03183\n",
            "Loss: 1.949697e-06, l1: 0.99891, l2: 0.03183\n",
            "Loss: 1.947663e-06, l1: 0.99889, l2: 0.03183\n",
            "Loss: 1.946444e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 1.944718e-06, l1: 0.99888, l2: 0.03184\n",
            "Loss: 1.942721e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 1.941314e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 1.940386e-06, l1: 0.99889, l2: 0.03184\n",
            "Loss: 1.938386e-06, l1: 0.99888, l2: 0.03184\n",
            "Loss: 1.936565e-06, l1: 0.99887, l2: 0.03184\n",
            "Loss: 1.935188e-06, l1: 0.99886, l2: 0.03184\n",
            "Loss: 1.933900e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 1.933398e-06, l1: 0.99883, l2: 0.03184\n",
            "Loss: 1.928907e-06, l1: 0.99876, l2: 0.03184\n",
            "Loss: 1.926313e-06, l1: 0.99873, l2: 0.03184\n",
            "Loss: 1.923957e-06, l1: 0.99870, l2: 0.03184\n",
            "Loss: 1.928680e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 1.923038e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 1.921936e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 1.920398e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 1.919424e-06, l1: 0.99871, l2: 0.03184\n",
            "Loss: 1.917241e-06, l1: 0.99869, l2: 0.03184\n",
            "Loss: 1.923056e-06, l1: 0.99863, l2: 0.03184\n",
            "Loss: 1.916487e-06, l1: 0.99867, l2: 0.03184\n",
            "Loss: 1.915146e-06, l1: 0.99865, l2: 0.03184\n",
            "Loss: 1.913482e-06, l1: 0.99860, l2: 0.03184\n",
            "Loss: 1.916124e-06, l1: 0.99856, l2: 0.03184\n",
            "Loss: 1.912803e-06, l1: 0.99859, l2: 0.03184\n",
            "Loss: 1.911834e-06, l1: 0.99858, l2: 0.03184\n",
            "Loss: 1.907594e-06, l1: 0.99853, l2: 0.03183\n",
            "Loss: 1.906152e-06, l1: 0.99851, l2: 0.03183\n",
            "Loss: 1.904212e-06, l1: 0.99849, l2: 0.03183\n",
            "Loss: 1.902322e-06, l1: 0.99850, l2: 0.03183\n",
            "Loss: 1.899065e-06, l1: 0.99853, l2: 0.03183\n",
            "Loss: 1.905727e-06, l1: 0.99858, l2: 0.03182\n",
            "Loss: 1.898014e-06, l1: 0.99854, l2: 0.03183\n",
            "Loss: 1.896404e-06, l1: 0.99855, l2: 0.03183\n",
            "Loss: 1.894253e-06, l1: 0.99855, l2: 0.03183\n",
            "Loss: 1.893404e-06, l1: 0.99855, l2: 0.03183\n",
            "Loss: 1.891333e-06, l1: 0.99854, l2: 0.03183\n",
            "Loss: 1.890295e-06, l1: 0.99853, l2: 0.03183\n",
            "Loss: 1.888420e-06, l1: 0.99852, l2: 0.03183\n",
            "Loss: 1.886826e-06, l1: 0.99851, l2: 0.03183\n",
            "Loss: 1.885954e-06, l1: 0.99850, l2: 0.03183\n",
            "Loss: 1.884184e-06, l1: 0.99850, l2: 0.03183\n",
            "Loss: 1.881687e-06, l1: 0.99852, l2: 0.03183\n",
            "Loss: 1.882576e-06, l1: 0.99856, l2: 0.03183\n",
            "Loss: 1.880188e-06, l1: 0.99854, l2: 0.03183\n",
            "Loss: 1.877633e-06, l1: 0.99858, l2: 0.03184\n",
            "Loss: 1.874511e-06, l1: 0.99864, l2: 0.03184\n",
            "Loss: 1.870878e-06, l1: 0.99870, l2: 0.03184\n",
            "Loss: 1.866854e-06, l1: 0.99872, l2: 0.03184\n",
            "Loss: 1.863877e-06, l1: 0.99873, l2: 0.03183\n",
            "Loss: 1.861325e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 1.861018e-06, l1: 0.99867, l2: 0.03182\n",
            "Loss: 1.858738e-06, l1: 0.99867, l2: 0.03183\n",
            "Loss: 1.857971e-06, l1: 0.99868, l2: 0.03183\n",
            "Loss: 1.856197e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 1.854053e-06, l1: 0.99872, l2: 0.03183\n",
            "Loss: 1.851232e-06, l1: 0.99873, l2: 0.03183\n",
            "Loss: 1.848625e-06, l1: 0.99872, l2: 0.03182\n",
            "Loss: 1.846718e-06, l1: 0.99873, l2: 0.03182\n",
            "Loss: 1.844631e-06, l1: 0.99873, l2: 0.03182\n",
            "Loss: 1.842473e-06, l1: 0.99873, l2: 0.03182\n",
            "Loss: 1.839761e-06, l1: 0.99873, l2: 0.03182\n",
            "Loss: 1.837655e-06, l1: 0.99874, l2: 0.03182\n",
            "Loss: 1.835283e-06, l1: 0.99876, l2: 0.03182\n",
            "Loss: 1.833560e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 1.831626e-06, l1: 0.99879, l2: 0.03182\n",
            "Loss: 1.828521e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 1.825437e-06, l1: 0.99878, l2: 0.03182\n",
            "Loss: 1.821891e-06, l1: 0.99876, l2: 0.03182\n",
            "Loss: 1.816313e-06, l1: 0.99874, l2: 0.03182\n",
            "Loss: 1.806456e-06, l1: 0.99871, l2: 0.03182\n",
            "Loss: 2.312310e-06, l1: 0.99867, l2: 0.03185\n",
            "Loss: 1.804962e-06, l1: 0.99871, l2: 0.03182\n",
            "Loss: 1.797967e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 1.792131e-06, l1: 0.99869, l2: 0.03183\n",
            "Loss: 1.788975e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 1.785731e-06, l1: 0.99870, l2: 0.03183\n",
            "Loss: 1.782305e-06, l1: 0.99871, l2: 0.03182\n",
            "Loss: 1.859290e-06, l1: 0.99875, l2: 0.03182\n",
            "Loss: 1.781652e-06, l1: 0.99871, l2: 0.03182\n",
            "Loss: 1.777132e-06, l1: 0.99872, l2: 0.03182\n",
            "Loss: 1.772885e-06, l1: 0.99874, l2: 0.03182\n",
            "Loss: 1.767502e-06, l1: 0.99879, l2: 0.03181\n",
            "Loss: 1.769540e-06, l1: 0.99885, l2: 0.03182\n",
            "Loss: 1.764232e-06, l1: 0.99882, l2: 0.03182\n",
            "Loss: 1.758552e-06, l1: 0.99889, l2: 0.03181\n",
            "Loss: 1.754694e-06, l1: 0.99894, l2: 0.03181\n",
            "Loss: 1.752222e-06, l1: 0.99896, l2: 0.03181\n",
            "Loss: 1.781625e-06, l1: 0.99907, l2: 0.03181\n",
            "Loss: 1.751777e-06, l1: 0.99897, l2: 0.03181\n",
            "Loss: 1.749687e-06, l1: 0.99898, l2: 0.03182\n",
            "Loss: 1.747079e-06, l1: 0.99899, l2: 0.03182\n",
            "Loss: 1.745487e-06, l1: 0.99899, l2: 0.03181\n",
            "Loss: 1.740855e-06, l1: 0.99902, l2: 0.03181\n",
            "Loss: 1.738902e-06, l1: 0.99902, l2: 0.03181\n",
            "Loss: 1.736824e-06, l1: 0.99903, l2: 0.03181\n",
            "Loss: 1.735739e-06, l1: 0.99904, l2: 0.03181\n",
            "Loss: 1.734334e-06, l1: 0.99904, l2: 0.03181\n",
            "Loss: 1.732032e-06, l1: 0.99905, l2: 0.03181\n",
            "Loss: 1.729739e-06, l1: 0.99908, l2: 0.03181\n",
            "Loss: 1.728476e-06, l1: 0.99915, l2: 0.03182\n",
            "Loss: 1.725232e-06, l1: 0.99918, l2: 0.03182\n",
            "Loss: 1.723748e-06, l1: 0.99917, l2: 0.03182\n",
            "Loss: 1.722348e-06, l1: 0.99918, l2: 0.03182\n",
            "Loss: 1.721627e-06, l1: 0.99919, l2: 0.03182\n",
            "Loss: 1.720678e-06, l1: 0.99921, l2: 0.03183\n",
            "Loss: 1.719447e-06, l1: 0.99922, l2: 0.03183\n",
            "Loss: 1.718659e-06, l1: 0.99922, l2: 0.03183\n",
            "Loss: 1.717728e-06, l1: 0.99924, l2: 0.03182\n",
            "Loss: 1.716614e-06, l1: 0.99925, l2: 0.03183\n",
            "Loss: 1.725410e-06, l1: 0.99942, l2: 0.03183\n",
            "Loss: 1.715706e-06, l1: 0.99929, l2: 0.03183\n",
            "Loss: 1.725722e-06, l1: 0.99943, l2: 0.03184\n",
            "Loss: 1.714134e-06, l1: 0.99933, l2: 0.03183\n",
            "Loss: 1.711787e-06, l1: 0.99935, l2: 0.03183\n",
            "Loss: 1.709521e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 1.708034e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 1.706315e-06, l1: 0.99945, l2: 0.03183\n",
            "Loss: 1.705827e-06, l1: 0.99945, l2: 0.03183\n",
            "Loss: 1.703605e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.703605e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.702791e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.701462e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.700490e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.700195e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.697846e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.697133e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.695883e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.694824e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.693746e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.692249e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.690827e-06, l1: 0.99946, l2: 0.03183\n",
            "Loss: 1.689780e-06, l1: 0.99945, l2: 0.03183\n",
            "Loss: 1.688618e-06, l1: 0.99944, l2: 0.03183\n",
            "Loss: 1.687389e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 1.686246e-06, l1: 0.99942, l2: 0.03183\n",
            "Loss: 1.685440e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 1.684979e-06, l1: 0.99944, l2: 0.03183\n",
            "Loss: 1.684462e-06, l1: 0.99944, l2: 0.03183\n",
            "Loss: 1.684175e-06, l1: 0.99944, l2: 0.03183\n",
            "Loss: 1.683438e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 1.682587e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 1.681522e-06, l1: 0.99941, l2: 0.03183\n",
            "Loss: 1.680721e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 1.680101e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.679077e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.677605e-06, l1: 0.99937, l2: 0.03183\n",
            "Loss: 1.679606e-06, l1: 0.99941, l2: 0.03182\n",
            "Loss: 1.676604e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.675089e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.673312e-06, l1: 0.99938, l2: 0.03183\n",
            "Loss: 1.672492e-06, l1: 0.99938, l2: 0.03183\n",
            "Loss: 1.671139e-06, l1: 0.99937, l2: 0.03183\n",
            "Loss: 1.669526e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.666720e-06, l1: 0.99934, l2: 0.03182\n",
            "Loss: 1.665197e-06, l1: 0.99931, l2: 0.03182\n",
            "Loss: 1.663666e-06, l1: 0.99933, l2: 0.03182\n",
            "Loss: 1.662453e-06, l1: 0.99934, l2: 0.03183\n",
            "Loss: 1.661674e-06, l1: 0.99935, l2: 0.03183\n",
            "Loss: 1.660581e-06, l1: 0.99934, l2: 0.03183\n",
            "Loss: 1.657944e-06, l1: 0.99932, l2: 0.03183\n",
            "Loss: 1.656854e-06, l1: 0.99926, l2: 0.03182\n",
            "Loss: 1.653341e-06, l1: 0.99927, l2: 0.03182\n",
            "Loss: 1.651536e-06, l1: 0.99926, l2: 0.03182\n",
            "Loss: 1.650859e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.647908e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.646522e-06, l1: 0.99926, l2: 0.03182\n",
            "Loss: 1.644056e-06, l1: 0.99926, l2: 0.03182\n",
            "Loss: 1.641439e-06, l1: 0.99927, l2: 0.03182\n",
            "Loss: 1.638870e-06, l1: 0.99927, l2: 0.03183\n",
            "Loss: 1.647052e-06, l1: 0.99921, l2: 0.03182\n",
            "Loss: 1.638254e-06, l1: 0.99925, l2: 0.03183\n",
            "Loss: 1.636751e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.634743e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.633452e-06, l1: 0.99923, l2: 0.03182\n",
            "Loss: 1.631870e-06, l1: 0.99922, l2: 0.03182\n",
            "Loss: 1.633052e-06, l1: 0.99916, l2: 0.03182\n",
            "Loss: 1.629806e-06, l1: 0.99919, l2: 0.03182\n",
            "Loss: 1.627274e-06, l1: 0.99920, l2: 0.03182\n",
            "Loss: 1.624236e-06, l1: 0.99922, l2: 0.03183\n",
            "Loss: 1.621276e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 1.616317e-06, l1: 0.99931, l2: 0.03184\n",
            "Loss: 1.612996e-06, l1: 0.99932, l2: 0.03184\n",
            "Loss: 1.610815e-06, l1: 0.99933, l2: 0.03183\n",
            "Loss: 1.608015e-06, l1: 0.99934, l2: 0.03183\n",
            "Loss: 1.605196e-06, l1: 0.99937, l2: 0.03184\n",
            "Loss: 1.602642e-06, l1: 0.99935, l2: 0.03184\n",
            "Loss: 1.601197e-06, l1: 0.99933, l2: 0.03184\n",
            "Loss: 1.598186e-06, l1: 0.99929, l2: 0.03184\n",
            "Loss: 1.596419e-06, l1: 0.99928, l2: 0.03184\n",
            "Loss: 1.595187e-06, l1: 0.99925, l2: 0.03184\n",
            "Loss: 1.597181e-06, l1: 0.99930, l2: 0.03184\n",
            "Loss: 1.594332e-06, l1: 0.99926, l2: 0.03184\n",
            "Loss: 1.593454e-06, l1: 0.99927, l2: 0.03184\n",
            "Loss: 1.592055e-06, l1: 0.99926, l2: 0.03184\n",
            "Loss: 1.591137e-06, l1: 0.99926, l2: 0.03184\n",
            "Loss: 1.589548e-06, l1: 0.99926, l2: 0.03184\n",
            "Loss: 1.588666e-06, l1: 0.99926, l2: 0.03184\n",
            "Loss: 1.587215e-06, l1: 0.99928, l2: 0.03184\n",
            "Loss: 1.586346e-06, l1: 0.99931, l2: 0.03184\n",
            "Loss: 1.584742e-06, l1: 0.99932, l2: 0.03185\n",
            "Loss: 1.583020e-06, l1: 0.99931, l2: 0.03185\n",
            "Loss: 1.581332e-06, l1: 0.99933, l2: 0.03185\n",
            "Loss: 1.580127e-06, l1: 0.99935, l2: 0.03185\n",
            "Loss: 1.579035e-06, l1: 0.99935, l2: 0.03185\n",
            "Loss: 1.577858e-06, l1: 0.99938, l2: 0.03185\n",
            "Loss: 1.576742e-06, l1: 0.99938, l2: 0.03185\n",
            "Loss: 1.575920e-06, l1: 0.99940, l2: 0.03185\n",
            "Loss: 1.575239e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 1.574966e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 1.572786e-06, l1: 0.99947, l2: 0.03185\n",
            "Loss: 1.571322e-06, l1: 0.99949, l2: 0.03185\n",
            "Loss: 1.569118e-06, l1: 0.99953, l2: 0.03185\n",
            "Loss: 1.567500e-06, l1: 0.99955, l2: 0.03185\n",
            "Loss: 1.564388e-06, l1: 0.99957, l2: 0.03185\n",
            "Loss: 1.561990e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.560197e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.558031e-06, l1: 0.99959, l2: 0.03185\n",
            "Loss: 1.556989e-06, l1: 0.99967, l2: 0.03185\n",
            "Loss: 1.554224e-06, l1: 0.99964, l2: 0.03185\n",
            "Loss: 1.551769e-06, l1: 0.99964, l2: 0.03186\n",
            "Loss: 1.550038e-06, l1: 0.99966, l2: 0.03186\n",
            "Loss: 1.548674e-06, l1: 0.99968, l2: 0.03186\n",
            "Loss: 1.547859e-06, l1: 0.99969, l2: 0.03186\n",
            "Loss: 1.546784e-06, l1: 0.99969, l2: 0.03186\n",
            "Loss: 1.546035e-06, l1: 0.99968, l2: 0.03186\n",
            "Loss: 1.545255e-06, l1: 0.99966, l2: 0.03186\n",
            "Loss: 1.544345e-06, l1: 0.99964, l2: 0.03186\n",
            "Loss: 1.543223e-06, l1: 0.99963, l2: 0.03186\n",
            "Loss: 1.542225e-06, l1: 0.99964, l2: 0.03186\n",
            "Loss: 1.540959e-06, l1: 0.99966, l2: 0.03186\n",
            "Loss: 1.584155e-06, l1: 0.99956, l2: 0.03186\n",
            "Loss: 1.540526e-06, l1: 0.99965, l2: 0.03186\n",
            "Loss: 1.539628e-06, l1: 0.99967, l2: 0.03186\n",
            "Loss: 1.538573e-06, l1: 0.99970, l2: 0.03186\n",
            "Loss: 1.537734e-06, l1: 0.99969, l2: 0.03186\n",
            "Loss: 1.536986e-06, l1: 0.99968, l2: 0.03186\n",
            "Loss: 1.535929e-06, l1: 0.99967, l2: 0.03186\n",
            "Loss: 1.535017e-06, l1: 0.99966, l2: 0.03186\n",
            "Loss: 1.533957e-06, l1: 0.99965, l2: 0.03186\n",
            "Loss: 1.533264e-06, l1: 0.99965, l2: 0.03186\n",
            "Loss: 1.532623e-06, l1: 0.99964, l2: 0.03186\n",
            "Loss: 1.532210e-06, l1: 0.99964, l2: 0.03186\n",
            "Loss: 1.531483e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.531010e-06, l1: 0.99961, l2: 0.03186\n",
            "Loss: 1.530583e-06, l1: 0.99961, l2: 0.03186\n",
            "Loss: 1.530202e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.529938e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.529125e-06, l1: 0.99961, l2: 0.03186\n",
            "Loss: 1.567845e-06, l1: 0.99978, l2: 0.03185\n",
            "Loss: 1.528513e-06, l1: 0.99963, l2: 0.03186\n",
            "Loss: 1.527742e-06, l1: 0.99963, l2: 0.03186\n",
            "Loss: 1.526917e-06, l1: 0.99963, l2: 0.03186\n",
            "Loss: 1.526019e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.524449e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.522398e-06, l1: 0.99962, l2: 0.03186\n",
            "Loss: 1.529159e-06, l1: 0.99967, l2: 0.03186\n",
            "Loss: 1.521641e-06, l1: 0.99964, l2: 0.03186\n",
            "Loss: 1.518829e-06, l1: 0.99965, l2: 0.03187\n",
            "Loss: 1.531216e-06, l1: 0.99971, l2: 0.03187\n",
            "Loss: 1.518172e-06, l1: 0.99966, l2: 0.03187\n",
            "Loss: 1.516842e-06, l1: 0.99967, l2: 0.03187\n",
            "Loss: 1.515993e-06, l1: 0.99968, l2: 0.03187\n",
            "Loss: 1.515434e-06, l1: 0.99969, l2: 0.03187\n",
            "Loss: 1.513914e-06, l1: 0.99968, l2: 0.03187\n",
            "Loss: 1.511965e-06, l1: 0.99966, l2: 0.03187\n",
            "Loss: 1.509279e-06, l1: 0.99960, l2: 0.03186\n",
            "Loss: 1.512552e-06, l1: 0.99966, l2: 0.03187\n",
            "Loss: 1.508377e-06, l1: 0.99962, l2: 0.03187\n",
            "Loss: 1.506258e-06, l1: 0.99956, l2: 0.03186\n",
            "Loss: 1.504379e-06, l1: 0.99953, l2: 0.03186\n",
            "Loss: 1.502916e-06, l1: 0.99949, l2: 0.03186\n",
            "Loss: 1.501414e-06, l1: 0.99949, l2: 0.03186\n",
            "Loss: 1.498894e-06, l1: 0.99948, l2: 0.03186\n",
            "Loss: 1.496730e-06, l1: 0.99946, l2: 0.03187\n",
            "Loss: 1.494489e-06, l1: 0.99944, l2: 0.03187\n",
            "Loss: 1.492726e-06, l1: 0.99942, l2: 0.03186\n",
            "Loss: 1.491681e-06, l1: 0.99941, l2: 0.03186\n",
            "Loss: 1.490777e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 1.489601e-06, l1: 0.99939, l2: 0.03186\n",
            "Loss: 1.488038e-06, l1: 0.99940, l2: 0.03186\n",
            "Loss: 1.484411e-06, l1: 0.99939, l2: 0.03187\n",
            "Loss: 1.482621e-06, l1: 0.99936, l2: 0.03187\n",
            "Loss: 1.480498e-06, l1: 0.99934, l2: 0.03187\n",
            "Loss: 1.478264e-06, l1: 0.99930, l2: 0.03186\n",
            "Loss: 1.476312e-06, l1: 0.99925, l2: 0.03186\n",
            "Loss: 1.475676e-06, l1: 0.99925, l2: 0.03186\n",
            "Loss: 1.474545e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.473503e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.472936e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.472319e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.471844e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.471860e-06, l1: 0.99928, l2: 0.03187\n",
            "Loss: 1.471662e-06, l1: 0.99928, l2: 0.03187\n",
            "Loss: 1.471435e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.470689e-06, l1: 0.99928, l2: 0.03186\n",
            "Loss: 1.471129e-06, l1: 0.99926, l2: 0.03186\n",
            "Loss: 1.470094e-06, l1: 0.99927, l2: 0.03186\n",
            "Loss: 1.469077e-06, l1: 0.99927, l2: 0.03186\n",
            "Loss: 1.467903e-06, l1: 0.99926, l2: 0.03186\n",
            "Loss: 1.467131e-06, l1: 0.99925, l2: 0.03186\n",
            "Loss: 1.466434e-06, l1: 0.99924, l2: 0.03186\n",
            "Loss: 1.466131e-06, l1: 0.99923, l2: 0.03186\n",
            "Loss: 1.465210e-06, l1: 0.99920, l2: 0.03186\n",
            "Loss: 1.464749e-06, l1: 0.99919, l2: 0.03186\n",
            "Loss: 1.463706e-06, l1: 0.99919, l2: 0.03186\n",
            "Loss: 1.462434e-06, l1: 0.99919, l2: 0.03186\n",
            "Loss: 1.461313e-06, l1: 0.99920, l2: 0.03186\n",
            "Loss: 1.462474e-06, l1: 0.99922, l2: 0.03186\n",
            "Loss: 1.460636e-06, l1: 0.99920, l2: 0.03186\n",
            "Loss: 1.459293e-06, l1: 0.99920, l2: 0.03185\n",
            "Loss: 1.457083e-06, l1: 0.99922, l2: 0.03185\n",
            "Loss: 1.454441e-06, l1: 0.99921, l2: 0.03185\n",
            "Loss: 1.452597e-06, l1: 0.99923, l2: 0.03185\n",
            "Loss: 1.450842e-06, l1: 0.99924, l2: 0.03185\n",
            "Loss: 1.449570e-06, l1: 0.99924, l2: 0.03185\n",
            "Loss: 1.447380e-06, l1: 0.99924, l2: 0.03185\n",
            "Loss: 1.446451e-06, l1: 0.99924, l2: 0.03185\n",
            "Loss: 1.445424e-06, l1: 0.99924, l2: 0.03185\n",
            "Loss: 1.444538e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 1.443885e-06, l1: 0.99925, l2: 0.03185\n",
            "Loss: 1.442444e-06, l1: 0.99927, l2: 0.03184\n",
            "Loss: 1.450131e-06, l1: 0.99939, l2: 0.03185\n",
            "Loss: 1.441738e-06, l1: 0.99930, l2: 0.03185\n",
            "Loss: 1.439115e-06, l1: 0.99932, l2: 0.03184\n",
            "Loss: 1.436477e-06, l1: 0.99937, l2: 0.03185\n",
            "Loss: 1.434206e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 1.432573e-06, l1: 0.99944, l2: 0.03185\n",
            "Loss: 1.429972e-06, l1: 0.99946, l2: 0.03185\n",
            "Loss: 1.427466e-06, l1: 0.99947, l2: 0.03185\n",
            "Loss: 1.425527e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 1.428334e-06, l1: 0.99944, l2: 0.03185\n",
            "Loss: 1.424956e-06, l1: 0.99945, l2: 0.03185\n",
            "Loss: 1.423646e-06, l1: 0.99942, l2: 0.03185\n",
            "Loss: 1.422780e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 1.421539e-06, l1: 0.99940, l2: 0.03185\n",
            "Loss: 1.420115e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 1.419103e-06, l1: 0.99941, l2: 0.03185\n",
            "Loss: 1.417795e-06, l1: 0.99944, l2: 0.03185\n",
            "Loss: 1.417113e-06, l1: 0.99946, l2: 0.03185\n",
            "Loss: 1.416669e-06, l1: 0.99947, l2: 0.03185\n",
            "Loss: 1.416158e-06, l1: 0.99948, l2: 0.03185\n",
            "Loss: 1.416773e-06, l1: 0.99945, l2: 0.03184\n",
            "Loss: 1.415980e-06, l1: 0.99947, l2: 0.03185\n",
            "Loss: 1.415504e-06, l1: 0.99947, l2: 0.03185\n",
            "Loss: 1.415341e-06, l1: 0.99947, l2: 0.03184\n",
            "Loss: 1.414868e-06, l1: 0.99947, l2: 0.03184\n",
            "Loss: 1.414543e-06, l1: 0.99947, l2: 0.03184\n",
            "Loss: 1.414137e-06, l1: 0.99947, l2: 0.03184\n",
            "Loss: 1.413590e-06, l1: 0.99946, l2: 0.03184\n",
            "Loss: 1.429120e-06, l1: 0.99946, l2: 0.03184\n",
            "Loss: 1.413405e-06, l1: 0.99946, l2: 0.03184\n",
            "Loss: 1.411547e-06, l1: 0.99947, l2: 0.03184\n",
            "Loss: 1.410460e-06, l1: 0.99950, l2: 0.03184\n",
            "Loss: 1.409556e-06, l1: 0.99949, l2: 0.03184\n",
            "Loss: 1.409005e-06, l1: 0.99948, l2: 0.03184\n",
            "Loss: 1.408418e-06, l1: 0.99947, l2: 0.03184\n",
            "Loss: 1.407644e-06, l1: 0.99943, l2: 0.03183\n",
            "Loss: 1.406523e-06, l1: 0.99942, l2: 0.03183\n",
            "Loss: 1.405772e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 1.404790e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.403281e-06, l1: 0.99938, l2: 0.03183\n",
            "Loss: 1.400844e-06, l1: 0.99938, l2: 0.03183\n",
            "Loss: 1.399273e-06, l1: 0.99938, l2: 0.03183\n",
            "Loss: 1.398522e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 1.398016e-06, l1: 0.99941, l2: 0.03183\n",
            "Loss: 1.397554e-06, l1: 0.99941, l2: 0.03183\n",
            "Loss: 1.397122e-06, l1: 0.99942, l2: 0.03183\n",
            "Loss: 1.396491e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 1.396208e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.395560e-06, l1: 0.99938, l2: 0.03183\n",
            "Loss: 1.394879e-06, l1: 0.99936, l2: 0.03183\n",
            "Loss: 1.394375e-06, l1: 0.99934, l2: 0.03183\n",
            "Loss: 1.393432e-06, l1: 0.99927, l2: 0.03182\n",
            "Loss: 1.391663e-06, l1: 0.99931, l2: 0.03183\n",
            "Loss: 1.391200e-06, l1: 0.99932, l2: 0.03183\n",
            "Loss: 1.390359e-06, l1: 0.99931, l2: 0.03183\n",
            "Loss: 1.389320e-06, l1: 0.99930, l2: 0.03183\n",
            "Loss: 1.387863e-06, l1: 0.99928, l2: 0.03182\n",
            "Loss: 1.386511e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.384290e-06, l1: 0.99926, l2: 0.03182\n",
            "Loss: 1.382552e-06, l1: 0.99929, l2: 0.03183\n",
            "Loss: 1.381212e-06, l1: 0.99930, l2: 0.03183\n",
            "Loss: 1.378385e-06, l1: 0.99933, l2: 0.03183\n",
            "Loss: 1.374850e-06, l1: 0.99931, l2: 0.03183\n",
            "Loss: 1.376626e-06, l1: 0.99932, l2: 0.03183\n",
            "Loss: 1.373813e-06, l1: 0.99932, l2: 0.03183\n",
            "Loss: 1.371009e-06, l1: 0.99927, l2: 0.03182\n",
            "Loss: 1.369542e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.367486e-06, l1: 0.99924, l2: 0.03182\n",
            "Loss: 1.365798e-06, l1: 0.99925, l2: 0.03182\n",
            "Loss: 1.362818e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 1.362174e-06, l1: 0.99927, l2: 0.03183\n",
            "Loss: 1.360309e-06, l1: 0.99932, l2: 0.03183\n",
            "Loss: 1.358757e-06, l1: 0.99929, l2: 0.03183\n",
            "Loss: 1.358234e-06, l1: 0.99927, l2: 0.03183\n",
            "Loss: 1.357485e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 1.356639e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 1.356675e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 1.356043e-06, l1: 0.99926, l2: 0.03183\n",
            "Loss: 1.354729e-06, l1: 0.99927, l2: 0.03182\n",
            "Loss: 1.352592e-06, l1: 0.99929, l2: 0.03182\n",
            "Loss: 1.351228e-06, l1: 0.99933, l2: 0.03182\n",
            "Loss: 1.350003e-06, l1: 0.99934, l2: 0.03182\n",
            "Loss: 1.348955e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.347460e-06, l1: 0.99936, l2: 0.03182\n",
            "Loss: 1.346028e-06, l1: 0.99936, l2: 0.03182\n",
            "Loss: 1.343747e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.340289e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.336546e-06, l1: 0.99931, l2: 0.03182\n",
            "Loss: 1.332585e-06, l1: 0.99929, l2: 0.03182\n",
            "Loss: 1.328337e-06, l1: 0.99930, l2: 0.03182\n",
            "Loss: 1.327509e-06, l1: 0.99930, l2: 0.03182\n",
            "Loss: 1.325562e-06, l1: 0.99931, l2: 0.03182\n",
            "Loss: 1.324794e-06, l1: 0.99932, l2: 0.03182\n",
            "Loss: 1.323822e-06, l1: 0.99932, l2: 0.03182\n",
            "Loss: 1.322305e-06, l1: 0.99934, l2: 0.03182\n",
            "Loss: 1.322064e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.321385e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.320304e-06, l1: 0.99935, l2: 0.03182\n",
            "Loss: 1.318807e-06, l1: 0.99937, l2: 0.03182\n",
            "Loss: 1.317842e-06, l1: 0.99938, l2: 0.03182\n",
            "Loss: 1.316368e-06, l1: 0.99940, l2: 0.03182\n",
            "Loss: 1.314987e-06, l1: 0.99943, l2: 0.03182\n",
            "Loss: 1.313546e-06, l1: 0.99943, l2: 0.03182\n",
            "Loss: 1.311750e-06, l1: 0.99943, l2: 0.03182\n",
            "Loss: 1.310125e-06, l1: 0.99941, l2: 0.03182\n",
            "Loss: 1.308988e-06, l1: 0.99939, l2: 0.03182\n",
            "Loss: 1.308029e-06, l1: 0.99938, l2: 0.03182\n",
            "Loss: 1.308886e-06, l1: 0.99941, l2: 0.03183\n",
            "Loss: 1.307096e-06, l1: 0.99939, l2: 0.03183\n",
            "Loss: 1.305641e-06, l1: 0.99940, l2: 0.03183\n",
            "Loss: 1.302973e-06, l1: 0.99942, l2: 0.03183\n",
            "Loss: 1.301431e-06, l1: 0.99944, l2: 0.03183\n",
            "Loss: 1.300463e-06, l1: 0.99946, l2: 0.03183\n",
            "Loss: 1.299877e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.299054e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.298094e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.296643e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.295651e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.294163e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.293472e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.292370e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.292623e-06, l1: 0.99945, l2: 0.03183\n",
            "Loss: 1.291641e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.290482e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.289905e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.287749e-06, l1: 0.99952, l2: 0.03183\n",
            "Loss: 1.286417e-06, l1: 0.99952, l2: 0.03183\n",
            "Loss: 1.285513e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.286735e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.284315e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.282872e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.281410e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.279562e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.277865e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.276065e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.274918e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.274084e-06, l1: 0.99952, l2: 0.03183\n",
            "Loss: 1.273055e-06, l1: 0.99953, l2: 0.03183\n",
            "Loss: 1.271835e-06, l1: 0.99953, l2: 0.03183\n",
            "Loss: 1.271131e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.272092e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.270673e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.270037e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.269222e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.268855e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.268646e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.268197e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.267611e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.267107e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.266065e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.265389e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.264529e-06, l1: 0.99949, l2: 0.03183\n",
            "Loss: 1.263590e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.262829e-06, l1: 0.99955, l2: 0.03183\n",
            "Loss: 1.261453e-06, l1: 0.99953, l2: 0.03183\n",
            "Loss: 1.261160e-06, l1: 0.99953, l2: 0.03183\n",
            "Loss: 1.260373e-06, l1: 0.99954, l2: 0.03183\n",
            "Loss: 1.258728e-06, l1: 0.99954, l2: 0.03183\n",
            "Loss: 1.257859e-06, l1: 0.99957, l2: 0.03182\n",
            "Loss: 1.255366e-06, l1: 0.99957, l2: 0.03182\n",
            "Loss: 1.253055e-06, l1: 0.99958, l2: 0.03183\n",
            "Loss: 1.251351e-06, l1: 0.99959, l2: 0.03182\n",
            "Loss: 1.249492e-06, l1: 0.99962, l2: 0.03182\n",
            "Loss: 1.256513e-06, l1: 0.99972, l2: 0.03183\n",
            "Loss: 1.248429e-06, l1: 0.99965, l2: 0.03182\n",
            "Loss: 1.247485e-06, l1: 0.99965, l2: 0.03182\n",
            "Loss: 1.245835e-06, l1: 0.99965, l2: 0.03182\n",
            "Loss: 1.244252e-06, l1: 0.99965, l2: 0.03182\n",
            "Loss: 1.242589e-06, l1: 0.99966, l2: 0.03182\n",
            "Loss: 1.241078e-06, l1: 0.99966, l2: 0.03182\n",
            "Loss: 1.239764e-06, l1: 0.99967, l2: 0.03182\n",
            "Loss: 1.237741e-06, l1: 0.99966, l2: 0.03182\n",
            "Loss: 1.235323e-06, l1: 0.99968, l2: 0.03181\n",
            "Loss: 1.233110e-06, l1: 0.99970, l2: 0.03182\n",
            "Loss: 1.231483e-06, l1: 0.99971, l2: 0.03182\n",
            "Loss: 1.230001e-06, l1: 0.99974, l2: 0.03182\n",
            "Loss: 1.228390e-06, l1: 0.99971, l2: 0.03182\n",
            "Loss: 1.238032e-06, l1: 0.99986, l2: 0.03183\n",
            "Loss: 1.228015e-06, l1: 0.99974, l2: 0.03182\n",
            "Loss: 1.226974e-06, l1: 0.99973, l2: 0.03182\n",
            "Loss: 1.225065e-06, l1: 0.99975, l2: 0.03182\n",
            "Loss: 1.223709e-06, l1: 0.99976, l2: 0.03182\n",
            "Loss: 1.227549e-06, l1: 0.99985, l2: 0.03182\n",
            "Loss: 1.222888e-06, l1: 0.99979, l2: 0.03182\n",
            "Loss: 1.221296e-06, l1: 0.99982, l2: 0.03182\n",
            "Loss: 1.221447e-06, l1: 0.99984, l2: 0.03182\n",
            "Loss: 1.220811e-06, l1: 0.99983, l2: 0.03182\n",
            "Loss: 1.220143e-06, l1: 0.99984, l2: 0.03182\n",
            "Loss: 1.219489e-06, l1: 0.99986, l2: 0.03182\n",
            "Loss: 1.218784e-06, l1: 0.99987, l2: 0.03182\n",
            "Loss: 1.217721e-06, l1: 0.99990, l2: 0.03182\n",
            "Loss: 1.222031e-06, l1: 0.99992, l2: 0.03182\n",
            "Loss: 1.216988e-06, l1: 0.99991, l2: 0.03182\n",
            "Loss: 1.216060e-06, l1: 0.99991, l2: 0.03182\n",
            "Loss: 1.214710e-06, l1: 0.99991, l2: 0.03182\n",
            "Loss: 1.214103e-06, l1: 0.99991, l2: 0.03182\n",
            "Loss: 1.213147e-06, l1: 0.99993, l2: 0.03182\n",
            "Loss: 1.212592e-06, l1: 0.99993, l2: 0.03182\n",
            "Loss: 1.220937e-06, l1: 1.00002, l2: 0.03182\n",
            "Loss: 1.212203e-06, l1: 0.99995, l2: 0.03182\n",
            "Loss: 1.210920e-06, l1: 0.99994, l2: 0.03182\n",
            "Loss: 1.210028e-06, l1: 0.99992, l2: 0.03182\n",
            "Loss: 1.209143e-06, l1: 0.99990, l2: 0.03182\n",
            "Loss: 1.208256e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.205787e-06, l1: 0.99987, l2: 0.03182\n",
            "Loss: 1.204351e-06, l1: 0.99988, l2: 0.03182\n",
            "Loss: 1.203873e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.203749e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.203610e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.203247e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.206335e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.203092e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.202666e-06, l1: 0.99989, l2: 0.03182\n",
            "Loss: 1.202520e-06, l1: 0.99987, l2: 0.03182\n",
            "Loss: 1.201988e-06, l1: 0.99987, l2: 0.03182\n",
            "Loss: 1.201715e-06, l1: 0.99987, l2: 0.03182\n",
            "Loss: 1.201499e-06, l1: 0.99986, l2: 0.03182\n",
            "Loss: 1.201163e-06, l1: 0.99986, l2: 0.03182\n",
            "Loss: 1.200499e-06, l1: 0.99985, l2: 0.03182\n",
            "Loss: 1.199730e-06, l1: 0.99983, l2: 0.03182\n",
            "Loss: 1.198076e-06, l1: 0.99982, l2: 0.03182\n",
            "Loss: 1.225784e-06, l1: 0.99987, l2: 0.03182\n",
            "Loss: 1.197749e-06, l1: 0.99982, l2: 0.03182\n",
            "Loss: 1.196696e-06, l1: 0.99981, l2: 0.03182\n",
            "Loss: 1.195732e-06, l1: 0.99980, l2: 0.03181\n",
            "Loss: 1.194895e-06, l1: 0.99979, l2: 0.03182\n",
            "Loss: 1.193357e-06, l1: 0.99977, l2: 0.03182\n",
            "Loss: 1.191289e-06, l1: 0.99974, l2: 0.03182\n",
            "Loss: 1.188573e-06, l1: 0.99970, l2: 0.03182\n",
            "Loss: 1.189973e-06, l1: 0.99961, l2: 0.03181\n",
            "Loss: 1.187408e-06, l1: 0.99967, l2: 0.03182\n",
            "Loss: 1.186496e-06, l1: 0.99967, l2: 0.03182\n",
            "Loss: 1.185670e-06, l1: 0.99968, l2: 0.03182\n",
            "Loss: 1.184266e-06, l1: 0.99967, l2: 0.03182\n",
            "Loss: 1.182951e-06, l1: 0.99961, l2: 0.03182\n",
            "Loss: 1.184303e-06, l1: 0.99959, l2: 0.03182\n",
            "Loss: 1.180989e-06, l1: 0.99960, l2: 0.03182\n",
            "Loss: 1.179467e-06, l1: 0.99960, l2: 0.03182\n",
            "Loss: 1.186763e-06, l1: 0.99947, l2: 0.03182\n",
            "Loss: 1.179149e-06, l1: 0.99957, l2: 0.03182\n",
            "Loss: 1.178043e-06, l1: 0.99956, l2: 0.03182\n",
            "Loss: 1.177097e-06, l1: 0.99954, l2: 0.03182\n",
            "Loss: 1.176057e-06, l1: 0.99953, l2: 0.03182\n",
            "Loss: 1.175308e-06, l1: 0.99951, l2: 0.03182\n",
            "Loss: 1.174306e-06, l1: 0.99951, l2: 0.03182\n",
            "Loss: 1.173080e-06, l1: 0.99952, l2: 0.03183\n",
            "Loss: 1.172437e-06, l1: 0.99952, l2: 0.03183\n",
            "Loss: 1.171969e-06, l1: 0.99952, l2: 0.03183\n",
            "Loss: 1.171746e-06, l1: 0.99951, l2: 0.03183\n",
            "Loss: 1.171375e-06, l1: 0.99950, l2: 0.03183\n",
            "Loss: 1.170914e-06, l1: 0.99948, l2: 0.03183\n",
            "Loss: 1.170779e-06, l1: 0.99947, l2: 0.03183\n",
            "Loss: 1.172331e-06, l1: 0.99942, l2: 0.03183\n",
            "Loss: 1.170519e-06, l1: 0.99946, l2: 0.03183\n",
            "Loss: 1.170354e-06, l1: 0.99946, l2: 0.03183\n",
            "Loss: 1.170089e-06, l1: 0.99946, l2: 0.03183\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
            "  Objective function value: 0.000001\n",
            "  Number of iterations: 2964\n",
            "  Number of functions evaluations: 3226\n",
            "Error u: 8.298877e-04\n",
            "Error l1: 0.05437%\n",
            "Error l2: 0.01798%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "########################### Noisy Data ###############################\n",
        "######################################################################\n",
        "noise = 0.01        \n",
        "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "        \n",
        "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "model.train(2000)\n",
        "    \n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "        \n",
        "lambda_1_value_noisy = model.sess.run(model.lambda_1)\n",
        "lambda_2_value_noisy = model.sess.run(model.lambda_2)\n",
        "lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n",
        "            \n",
        "error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
        "error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu)/nu * 100\n",
        "    \n",
        "print('Error lambda_1: %f%%' % (error_lambda_1_noisy))\n",
        "print('Error lambda_2: %f%%' % (error_lambda_2_noisy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WSYpFgNogBy",
        "outputId": "6c928ad9-f9c5-48fe-bb6e-fbb715664529"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n",
            "It: 0, Loss: 3.148e-01, Lambda_1: -0.001, Lambda_2: 0.002480, Time: 1.48\n",
            "It: 10, Loss: 2.198e-01, Lambda_1: -0.004, Lambda_2: 0.002487, Time: 0.27\n",
            "It: 20, Loss: 1.849e-01, Lambda_1: -0.012, Lambda_2: 0.002509, Time: 0.26\n",
            "It: 30, Loss: 1.744e-01, Lambda_1: -0.024, Lambda_2: 0.002539, Time: 0.28\n",
            "It: 40, Loss: 1.710e-01, Lambda_1: -0.036, Lambda_2: 0.002572, Time: 0.27\n",
            "It: 50, Loss: 1.646e-01, Lambda_1: -0.047, Lambda_2: 0.002606, Time: 0.26\n",
            "It: 60, Loss: 1.562e-01, Lambda_1: -0.059, Lambda_2: 0.002642, Time: 0.26\n",
            "It: 70, Loss: 1.405e-01, Lambda_1: -0.068, Lambda_2: 0.002677, Time: 0.28\n",
            "It: 80, Loss: 1.104e-01, Lambda_1: -0.074, Lambda_2: 0.002701, Time: 0.27\n",
            "It: 90, Loss: 7.321e-02, Lambda_1: -0.072, Lambda_2: 0.002670, Time: 0.26\n",
            "It: 100, Loss: 4.994e-02, Lambda_1: -0.058, Lambda_2: 0.002615, Time: 0.29\n",
            "It: 110, Loss: 4.058e-02, Lambda_1: -0.036, Lambda_2: 0.002556, Time: 0.28\n",
            "It: 120, Loss: 3.431e-02, Lambda_1: -0.013, Lambda_2: 0.002498, Time: 0.28\n",
            "It: 130, Loss: 2.858e-02, Lambda_1: 0.008, Lambda_2: 0.002448, Time: 0.29\n",
            "It: 140, Loss: 2.458e-02, Lambda_1: 0.025, Lambda_2: 0.002414, Time: 0.27\n",
            "It: 150, Loss: 2.213e-02, Lambda_1: 0.037, Lambda_2: 0.002396, Time: 0.27\n",
            "It: 160, Loss: 2.072e-02, Lambda_1: 0.044, Lambda_2: 0.002391, Time: 0.26\n",
            "It: 170, Loss: 1.998e-02, Lambda_1: 0.048, Lambda_2: 0.002393, Time: 0.27\n",
            "It: 180, Loss: 1.954e-02, Lambda_1: 0.050, Lambda_2: 0.002401, Time: 0.27\n",
            "It: 190, Loss: 1.924e-02, Lambda_1: 0.050, Lambda_2: 0.002410, Time: 0.27\n",
            "It: 200, Loss: 1.901e-02, Lambda_1: 0.049, Lambda_2: 0.002421, Time: 0.27\n",
            "It: 210, Loss: 1.883e-02, Lambda_1: 0.049, Lambda_2: 0.002432, Time: 0.27\n",
            "It: 220, Loss: 1.867e-02, Lambda_1: 0.049, Lambda_2: 0.002442, Time: 0.27\n",
            "It: 230, Loss: 1.854e-02, Lambda_1: 0.050, Lambda_2: 0.002453, Time: 0.27\n",
            "It: 240, Loss: 1.843e-02, Lambda_1: 0.050, Lambda_2: 0.002464, Time: 0.28\n",
            "It: 250, Loss: 1.834e-02, Lambda_1: 0.051, Lambda_2: 0.002476, Time: 0.28\n",
            "It: 260, Loss: 1.825e-02, Lambda_1: 0.052, Lambda_2: 0.002487, Time: 0.27\n",
            "It: 270, Loss: 1.817e-02, Lambda_1: 0.053, Lambda_2: 0.002499, Time: 0.27\n",
            "It: 280, Loss: 1.809e-02, Lambda_1: 0.054, Lambda_2: 0.002512, Time: 0.28\n",
            "It: 290, Loss: 1.802e-02, Lambda_1: 0.055, Lambda_2: 0.002525, Time: 0.28\n",
            "It: 300, Loss: 1.794e-02, Lambda_1: 0.056, Lambda_2: 0.002538, Time: 0.27\n",
            "It: 310, Loss: 1.787e-02, Lambda_1: 0.057, Lambda_2: 0.002552, Time: 0.26\n",
            "It: 320, Loss: 1.779e-02, Lambda_1: 0.058, Lambda_2: 0.002566, Time: 0.28\n",
            "It: 330, Loss: 1.772e-02, Lambda_1: 0.059, Lambda_2: 0.002581, Time: 0.27\n",
            "It: 340, Loss: 1.764e-02, Lambda_1: 0.060, Lambda_2: 0.002596, Time: 0.26\n",
            "It: 350, Loss: 1.756e-02, Lambda_1: 0.062, Lambda_2: 0.002611, Time: 0.27\n",
            "It: 360, Loss: 1.748e-02, Lambda_1: 0.063, Lambda_2: 0.002627, Time: 0.29\n",
            "It: 370, Loss: 1.740e-02, Lambda_1: 0.065, Lambda_2: 0.002644, Time: 0.26\n",
            "It: 380, Loss: 1.731e-02, Lambda_1: 0.066, Lambda_2: 0.002661, Time: 0.27\n",
            "It: 390, Loss: 1.722e-02, Lambda_1: 0.068, Lambda_2: 0.002678, Time: 0.29\n",
            "It: 400, Loss: 1.713e-02, Lambda_1: 0.069, Lambda_2: 0.002696, Time: 0.27\n",
            "It: 410, Loss: 1.703e-02, Lambda_1: 0.071, Lambda_2: 0.002715, Time: 0.28\n",
            "It: 420, Loss: 1.694e-02, Lambda_1: 0.073, Lambda_2: 0.002735, Time: 0.28\n",
            "It: 430, Loss: 1.684e-02, Lambda_1: 0.075, Lambda_2: 0.002755, Time: 0.28\n",
            "It: 440, Loss: 1.673e-02, Lambda_1: 0.077, Lambda_2: 0.002776, Time: 0.28\n",
            "It: 450, Loss: 1.663e-02, Lambda_1: 0.079, Lambda_2: 0.002798, Time: 0.28\n",
            "It: 460, Loss: 1.652e-02, Lambda_1: 0.082, Lambda_2: 0.002820, Time: 0.28\n",
            "It: 470, Loss: 1.641e-02, Lambda_1: 0.084, Lambda_2: 0.002843, Time: 0.27\n",
            "It: 480, Loss: 1.630e-02, Lambda_1: 0.087, Lambda_2: 0.002867, Time: 0.27\n",
            "It: 490, Loss: 1.618e-02, Lambda_1: 0.090, Lambda_2: 0.002892, Time: 0.27\n",
            "It: 500, Loss: 1.607e-02, Lambda_1: 0.092, Lambda_2: 0.002918, Time: 0.28\n",
            "It: 510, Loss: 1.595e-02, Lambda_1: 0.095, Lambda_2: 0.002945, Time: 0.27\n",
            "It: 520, Loss: 1.582e-02, Lambda_1: 0.098, Lambda_2: 0.002973, Time: 0.26\n",
            "It: 530, Loss: 1.570e-02, Lambda_1: 0.102, Lambda_2: 0.003001, Time: 0.26\n",
            "It: 540, Loss: 1.557e-02, Lambda_1: 0.105, Lambda_2: 0.003031, Time: 0.28\n",
            "It: 550, Loss: 1.544e-02, Lambda_1: 0.109, Lambda_2: 0.003062, Time: 0.26\n",
            "It: 560, Loss: 1.530e-02, Lambda_1: 0.112, Lambda_2: 0.003093, Time: 0.26\n",
            "It: 570, Loss: 1.517e-02, Lambda_1: 0.116, Lambda_2: 0.003126, Time: 0.28\n",
            "It: 580, Loss: 1.503e-02, Lambda_1: 0.120, Lambda_2: 0.003160, Time: 0.27\n",
            "It: 590, Loss: 1.489e-02, Lambda_1: 0.124, Lambda_2: 0.003195, Time: 0.27\n",
            "It: 600, Loss: 1.475e-02, Lambda_1: 0.128, Lambda_2: 0.003232, Time: 0.26\n",
            "It: 610, Loss: 1.460e-02, Lambda_1: 0.133, Lambda_2: 0.003269, Time: 0.29\n",
            "It: 620, Loss: 1.446e-02, Lambda_1: 0.137, Lambda_2: 0.003308, Time: 0.26\n",
            "It: 630, Loss: 1.447e-02, Lambda_1: 0.142, Lambda_2: 0.003348, Time: 0.26\n",
            "It: 640, Loss: 1.420e-02, Lambda_1: 0.147, Lambda_2: 0.003389, Time: 0.27\n",
            "It: 650, Loss: 1.404e-02, Lambda_1: 0.152, Lambda_2: 0.003432, Time: 0.29\n",
            "It: 660, Loss: 1.390e-02, Lambda_1: 0.156, Lambda_2: 0.003476, Time: 0.27\n",
            "It: 670, Loss: 1.376e-02, Lambda_1: 0.161, Lambda_2: 0.003522, Time: 0.27\n",
            "It: 680, Loss: 1.363e-02, Lambda_1: 0.167, Lambda_2: 0.003569, Time: 0.30\n",
            "It: 690, Loss: 1.349e-02, Lambda_1: 0.172, Lambda_2: 0.003617, Time: 0.27\n",
            "It: 700, Loss: 1.335e-02, Lambda_1: 0.177, Lambda_2: 0.003667, Time: 0.29\n",
            "It: 710, Loss: 1.322e-02, Lambda_1: 0.183, Lambda_2: 0.003719, Time: 0.29\n",
            "It: 720, Loss: 1.308e-02, Lambda_1: 0.188, Lambda_2: 0.003771, Time: 0.29\n",
            "It: 730, Loss: 1.315e-02, Lambda_1: 0.194, Lambda_2: 0.003825, Time: 0.29\n",
            "It: 740, Loss: 1.283e-02, Lambda_1: 0.200, Lambda_2: 0.003881, Time: 0.29\n",
            "It: 750, Loss: 1.269e-02, Lambda_1: 0.205, Lambda_2: 0.003938, Time: 0.30\n",
            "It: 760, Loss: 1.255e-02, Lambda_1: 0.211, Lambda_2: 0.003996, Time: 0.28\n",
            "It: 770, Loss: 1.242e-02, Lambda_1: 0.217, Lambda_2: 0.004056, Time: 0.28\n",
            "It: 780, Loss: 1.228e-02, Lambda_1: 0.223, Lambda_2: 0.004118, Time: 0.27\n",
            "It: 790, Loss: 1.214e-02, Lambda_1: 0.229, Lambda_2: 0.004180, Time: 0.29\n",
            "It: 800, Loss: 1.200e-02, Lambda_1: 0.235, Lambda_2: 0.004245, Time: 0.28\n",
            "It: 810, Loss: 1.186e-02, Lambda_1: 0.242, Lambda_2: 0.004310, Time: 0.27\n",
            "It: 820, Loss: 1.172e-02, Lambda_1: 0.248, Lambda_2: 0.004377, Time: 0.29\n",
            "It: 830, Loss: 1.192e-02, Lambda_1: 0.254, Lambda_2: 0.004444, Time: 0.29\n",
            "It: 840, Loss: 1.153e-02, Lambda_1: 0.261, Lambda_2: 0.004513, Time: 0.28\n",
            "It: 850, Loss: 1.131e-02, Lambda_1: 0.267, Lambda_2: 0.004583, Time: 0.28\n",
            "It: 860, Loss: 1.114e-02, Lambda_1: 0.274, Lambda_2: 0.004654, Time: 0.28\n",
            "It: 870, Loss: 1.100e-02, Lambda_1: 0.281, Lambda_2: 0.004727, Time: 0.26\n",
            "It: 880, Loss: 1.085e-02, Lambda_1: 0.288, Lambda_2: 0.004800, Time: 0.26\n",
            "It: 890, Loss: 1.070e-02, Lambda_1: 0.295, Lambda_2: 0.004874, Time: 0.29\n",
            "It: 900, Loss: 1.060e-02, Lambda_1: 0.302, Lambda_2: 0.004950, Time: 0.28\n",
            "It: 910, Loss: 1.041e-02, Lambda_1: 0.309, Lambda_2: 0.005026, Time: 0.26\n",
            "It: 920, Loss: 1.037e-02, Lambda_1: 0.316, Lambda_2: 0.005103, Time: 0.27\n",
            "It: 930, Loss: 1.016e-02, Lambda_1: 0.323, Lambda_2: 0.005182, Time: 0.29\n",
            "It: 940, Loss: 1.002e-02, Lambda_1: 0.331, Lambda_2: 0.005262, Time: 0.27\n",
            "It: 950, Loss: 9.873e-03, Lambda_1: 0.338, Lambda_2: 0.005343, Time: 0.28\n",
            "It: 960, Loss: 9.727e-03, Lambda_1: 0.345, Lambda_2: 0.005425, Time: 0.28\n",
            "It: 970, Loss: 9.662e-03, Lambda_1: 0.352, Lambda_2: 0.005508, Time: 0.29\n",
            "It: 980, Loss: 1.011e-02, Lambda_1: 0.359, Lambda_2: 0.005591, Time: 0.27\n",
            "It: 990, Loss: 9.465e-03, Lambda_1: 0.365, Lambda_2: 0.005676, Time: 0.26\n",
            "It: 1000, Loss: 9.278e-03, Lambda_1: 0.372, Lambda_2: 0.005763, Time: 0.28\n",
            "It: 1010, Loss: 9.154e-03, Lambda_1: 0.378, Lambda_2: 0.005849, Time: 0.27\n",
            "It: 1020, Loss: 9.011e-03, Lambda_1: 0.385, Lambda_2: 0.005937, Time: 0.26\n",
            "It: 1030, Loss: 8.890e-03, Lambda_1: 0.392, Lambda_2: 0.006026, Time: 0.27\n",
            "It: 1040, Loss: 8.771e-03, Lambda_1: 0.399, Lambda_2: 0.006117, Time: 0.27\n",
            "It: 1050, Loss: 8.660e-03, Lambda_1: 0.405, Lambda_2: 0.006209, Time: 0.26\n",
            "It: 1060, Loss: 8.553e-03, Lambda_1: 0.412, Lambda_2: 0.006302, Time: 0.26\n",
            "It: 1070, Loss: 8.560e-03, Lambda_1: 0.418, Lambda_2: 0.006396, Time: 0.27\n",
            "It: 1080, Loss: 8.971e-03, Lambda_1: 0.425, Lambda_2: 0.006490, Time: 0.28\n",
            "It: 1090, Loss: 8.536e-03, Lambda_1: 0.430, Lambda_2: 0.006586, Time: 0.27\n",
            "It: 1100, Loss: 8.198e-03, Lambda_1: 0.435, Lambda_2: 0.006684, Time: 0.27\n",
            "It: 1110, Loss: 8.066e-03, Lambda_1: 0.441, Lambda_2: 0.006781, Time: 0.27\n",
            "It: 1120, Loss: 7.970e-03, Lambda_1: 0.447, Lambda_2: 0.006880, Time: 0.27\n",
            "It: 1130, Loss: 7.868e-03, Lambda_1: 0.453, Lambda_2: 0.006982, Time: 0.26\n",
            "It: 1140, Loss: 7.769e-03, Lambda_1: 0.459, Lambda_2: 0.007085, Time: 0.27\n",
            "It: 1150, Loss: 7.670e-03, Lambda_1: 0.465, Lambda_2: 0.007190, Time: 0.28\n",
            "It: 1160, Loss: 7.624e-03, Lambda_1: 0.471, Lambda_2: 0.007296, Time: 0.27\n",
            "It: 1170, Loss: 7.682e-03, Lambda_1: 0.477, Lambda_2: 0.007403, Time: 0.27\n",
            "It: 1180, Loss: 7.472e-03, Lambda_1: 0.482, Lambda_2: 0.007511, Time: 0.28\n",
            "It: 1190, Loss: 7.319e-03, Lambda_1: 0.486, Lambda_2: 0.007624, Time: 0.31\n",
            "It: 1200, Loss: 7.236e-03, Lambda_1: 0.491, Lambda_2: 0.007736, Time: 0.27\n",
            "It: 1210, Loss: 7.140e-03, Lambda_1: 0.497, Lambda_2: 0.007849, Time: 0.28\n",
            "It: 1220, Loss: 7.025e-03, Lambda_1: 0.502, Lambda_2: 0.007966, Time: 0.28\n",
            "It: 1230, Loss: 6.929e-03, Lambda_1: 0.508, Lambda_2: 0.008085, Time: 0.27\n",
            "It: 1240, Loss: 6.840e-03, Lambda_1: 0.513, Lambda_2: 0.008205, Time: 0.28\n",
            "It: 1250, Loss: 7.119e-03, Lambda_1: 0.519, Lambda_2: 0.008327, Time: 0.27\n",
            "It: 1260, Loss: 7.305e-03, Lambda_1: 0.524, Lambda_2: 0.008446, Time: 0.30\n",
            "It: 1270, Loss: 6.700e-03, Lambda_1: 0.528, Lambda_2: 0.008572, Time: 0.28\n",
            "It: 1280, Loss: 6.473e-03, Lambda_1: 0.532, Lambda_2: 0.008701, Time: 0.29\n",
            "It: 1290, Loss: 6.420e-03, Lambda_1: 0.537, Lambda_2: 0.008830, Time: 0.28\n",
            "It: 1300, Loss: 6.289e-03, Lambda_1: 0.543, Lambda_2: 0.008959, Time: 0.27\n",
            "It: 1310, Loss: 6.196e-03, Lambda_1: 0.548, Lambda_2: 0.009091, Time: 0.27\n",
            "It: 1320, Loss: 6.115e-03, Lambda_1: 0.553, Lambda_2: 0.009226, Time: 0.26\n",
            "It: 1330, Loss: 6.378e-03, Lambda_1: 0.558, Lambda_2: 0.009361, Time: 0.28\n",
            "It: 1340, Loss: 6.286e-03, Lambda_1: 0.563, Lambda_2: 0.009494, Time: 0.26\n",
            "It: 1350, Loss: 5.864e-03, Lambda_1: 0.567, Lambda_2: 0.009634, Time: 0.27\n",
            "It: 1360, Loss: 5.841e-03, Lambda_1: 0.571, Lambda_2: 0.009777, Time: 0.27\n",
            "It: 1370, Loss: 5.686e-03, Lambda_1: 0.576, Lambda_2: 0.009919, Time: 0.27\n",
            "It: 1380, Loss: 5.580e-03, Lambda_1: 0.581, Lambda_2: 0.010062, Time: 0.26\n",
            "It: 1390, Loss: 5.496e-03, Lambda_1: 0.586, Lambda_2: 0.010208, Time: 0.27\n",
            "It: 1400, Loss: 5.541e-03, Lambda_1: 0.590, Lambda_2: 0.010356, Time: 0.28\n",
            "It: 1410, Loss: 5.500e-03, Lambda_1: 0.595, Lambda_2: 0.010502, Time: 0.29\n",
            "It: 1420, Loss: 5.338e-03, Lambda_1: 0.599, Lambda_2: 0.010652, Time: 0.28\n",
            "It: 1430, Loss: 5.247e-03, Lambda_1: 0.603, Lambda_2: 0.010807, Time: 0.28\n",
            "It: 1440, Loss: 5.093e-03, Lambda_1: 0.608, Lambda_2: 0.010961, Time: 0.28\n",
            "It: 1450, Loss: 4.996e-03, Lambda_1: 0.612, Lambda_2: 0.011116, Time: 0.27\n",
            "It: 1460, Loss: 4.892e-03, Lambda_1: 0.617, Lambda_2: 0.011273, Time: 0.27\n",
            "It: 1470, Loss: 5.031e-03, Lambda_1: 0.622, Lambda_2: 0.011432, Time: 0.29\n",
            "It: 1480, Loss: 4.784e-03, Lambda_1: 0.626, Lambda_2: 0.011586, Time: 0.28\n",
            "It: 1490, Loss: 4.755e-03, Lambda_1: 0.630, Lambda_2: 0.011749, Time: 0.27\n",
            "It: 1500, Loss: 4.685e-03, Lambda_1: 0.633, Lambda_2: 0.011917, Time: 0.28\n",
            "It: 1510, Loss: 4.473e-03, Lambda_1: 0.637, Lambda_2: 0.012082, Time: 0.28\n",
            "It: 1520, Loss: 4.407e-03, Lambda_1: 0.642, Lambda_2: 0.012247, Time: 0.27\n",
            "It: 1530, Loss: 4.318e-03, Lambda_1: 0.646, Lambda_2: 0.012415, Time: 0.27\n",
            "It: 1540, Loss: 4.229e-03, Lambda_1: 0.651, Lambda_2: 0.012584, Time: 0.25\n",
            "It: 1550, Loss: 4.167e-03, Lambda_1: 0.655, Lambda_2: 0.012755, Time: 0.27\n",
            "It: 1560, Loss: 4.985e-03, Lambda_1: 0.660, Lambda_2: 0.012927, Time: 0.26\n",
            "It: 1570, Loss: 4.631e-03, Lambda_1: 0.664, Lambda_2: 0.013094, Time: 0.27\n",
            "It: 1580, Loss: 3.983e-03, Lambda_1: 0.667, Lambda_2: 0.013274, Time: 0.28\n",
            "It: 1590, Loss: 3.908e-03, Lambda_1: 0.670, Lambda_2: 0.013455, Time: 0.27\n",
            "It: 1600, Loss: 3.768e-03, Lambda_1: 0.674, Lambda_2: 0.013632, Time: 0.27\n",
            "It: 1610, Loss: 3.698e-03, Lambda_1: 0.679, Lambda_2: 0.013809, Time: 0.26\n",
            "It: 1620, Loss: 3.617e-03, Lambda_1: 0.683, Lambda_2: 0.013990, Time: 0.28\n",
            "It: 1630, Loss: 3.549e-03, Lambda_1: 0.687, Lambda_2: 0.014172, Time: 0.28\n",
            "It: 1640, Loss: 3.546e-03, Lambda_1: 0.691, Lambda_2: 0.014354, Time: 0.27\n",
            "It: 1650, Loss: 4.538e-03, Lambda_1: 0.695, Lambda_2: 0.014535, Time: 0.27\n",
            "It: 1660, Loss: 3.365e-03, Lambda_1: 0.699, Lambda_2: 0.014719, Time: 0.27\n",
            "It: 1670, Loss: 3.394e-03, Lambda_1: 0.701, Lambda_2: 0.014911, Time: 0.28\n",
            "It: 1680, Loss: 3.190e-03, Lambda_1: 0.705, Lambda_2: 0.015100, Time: 0.27\n",
            "It: 1690, Loss: 3.136e-03, Lambda_1: 0.709, Lambda_2: 0.015286, Time: 0.28\n",
            "It: 1700, Loss: 3.059e-03, Lambda_1: 0.713, Lambda_2: 0.015474, Time: 0.27\n",
            "It: 1710, Loss: 2.983e-03, Lambda_1: 0.717, Lambda_2: 0.015663, Time: 0.27\n",
            "It: 1720, Loss: 2.926e-03, Lambda_1: 0.721, Lambda_2: 0.015853, Time: 0.27\n",
            "It: 1730, Loss: 3.134e-03, Lambda_1: 0.725, Lambda_2: 0.016044, Time: 0.29\n",
            "It: 1740, Loss: 2.800e-03, Lambda_1: 0.729, Lambda_2: 0.016230, Time: 0.28\n",
            "It: 1750, Loss: 3.020e-03, Lambda_1: 0.732, Lambda_2: 0.016427, Time: 0.28\n",
            "It: 1760, Loss: 2.670e-03, Lambda_1: 0.734, Lambda_2: 0.016625, Time: 0.28\n",
            "It: 1770, Loss: 2.628e-03, Lambda_1: 0.738, Lambda_2: 0.016818, Time: 0.27\n",
            "It: 1780, Loss: 2.559e-03, Lambda_1: 0.742, Lambda_2: 0.017010, Time: 0.27\n",
            "It: 1790, Loss: 2.487e-03, Lambda_1: 0.746, Lambda_2: 0.017203, Time: 0.26\n",
            "It: 1800, Loss: 2.421e-03, Lambda_1: 0.749, Lambda_2: 0.017397, Time: 0.28\n",
            "It: 1810, Loss: 2.364e-03, Lambda_1: 0.753, Lambda_2: 0.017592, Time: 0.27\n",
            "It: 1820, Loss: 2.442e-03, Lambda_1: 0.757, Lambda_2: 0.017786, Time: 0.26\n",
            "It: 1830, Loss: 2.581e-03, Lambda_1: 0.760, Lambda_2: 0.017975, Time: 0.26\n",
            "It: 1840, Loss: 2.633e-03, Lambda_1: 0.763, Lambda_2: 0.018175, Time: 0.29\n",
            "It: 1850, Loss: 2.283e-03, Lambda_1: 0.765, Lambda_2: 0.018377, Time: 0.27\n",
            "It: 1860, Loss: 2.100e-03, Lambda_1: 0.768, Lambda_2: 0.018572, Time: 0.27\n",
            "It: 1870, Loss: 2.063e-03, Lambda_1: 0.771, Lambda_2: 0.018762, Time: 0.29\n",
            "It: 1880, Loss: 1.993e-03, Lambda_1: 0.775, Lambda_2: 0.018953, Time: 0.26\n",
            "It: 1890, Loss: 1.943e-03, Lambda_1: 0.779, Lambda_2: 0.019146, Time: 0.30\n",
            "It: 1900, Loss: 1.892e-03, Lambda_1: 0.782, Lambda_2: 0.019338, Time: 0.28\n",
            "It: 1910, Loss: 1.842e-03, Lambda_1: 0.786, Lambda_2: 0.019530, Time: 0.28\n",
            "It: 1920, Loss: 1.794e-03, Lambda_1: 0.789, Lambda_2: 0.019722, Time: 0.28\n",
            "It: 1930, Loss: 1.749e-03, Lambda_1: 0.792, Lambda_2: 0.019914, Time: 0.29\n",
            "It: 1940, Loss: 1.847e-03, Lambda_1: 0.796, Lambda_2: 0.020105, Time: 0.29\n",
            "It: 1950, Loss: 1.935e-03, Lambda_1: 0.799, Lambda_2: 0.020289, Time: 0.26\n",
            "It: 1960, Loss: 2.028e-03, Lambda_1: 0.800, Lambda_2: 0.020487, Time: 0.27\n",
            "It: 1970, Loss: 1.773e-03, Lambda_1: 0.802, Lambda_2: 0.020687, Time: 0.26\n",
            "It: 1980, Loss: 1.596e-03, Lambda_1: 0.805, Lambda_2: 0.020876, Time: 0.27\n",
            "It: 1990, Loss: 1.499e-03, Lambda_1: 0.808, Lambda_2: 0.021059, Time: 0.27\n",
            "Loss: 1.473080e-03, l1: 0.81084, l2: 0.02123\n",
            "Loss: 1.127890e+02, l1: 0.83103, l2: 0.02200\n",
            "Loss: 1.463838e-03, l1: 0.81084, l2: 0.02123\n",
            "Loss: 1.461781e-03, l1: 0.81085, l2: 0.02123\n",
            "Loss: 1.461705e-03, l1: 0.81086, l2: 0.02123\n",
            "Loss: 1.461011e-03, l1: 0.81097, l2: 0.02123\n",
            "Loss: 1.460247e-03, l1: 0.81113, l2: 0.02123\n",
            "Loss: 1.457396e-03, l1: 0.81184, l2: 0.02125\n",
            "Loss: 1.451894e-03, l1: 0.81331, l2: 0.02130\n",
            "Loss: 1.437578e-03, l1: 0.81732, l2: 0.02142\n",
            "Loss: 1.407370e-03, l1: 0.82622, l2: 0.02169\n",
            "Loss: 1.353129e-03, l1: 0.84344, l2: 0.02223\n",
            "Loss: 1.302844e-03, l1: 0.87197, l2: 0.02316\n",
            "Loss: 1.240223e-03, l1: 0.87753, l2: 0.02336\n",
            "Loss: 1.205815e-03, l1: 0.88275, l2: 0.02355\n",
            "Loss: 1.193300e-03, l1: 0.88505, l2: 0.02364\n",
            "Loss: 1.152738e-03, l1: 0.89451, l2: 0.02400\n",
            "Loss: 1.113670e-03, l1: 0.90786, l2: 0.02453\n",
            "Loss: 1.095115e-03, l1: 0.91293, l2: 0.02473\n",
            "Loss: 1.085331e-03, l1: 0.91515, l2: 0.02483\n",
            "Loss: 1.074830e-03, l1: 0.91683, l2: 0.02491\n",
            "Loss: 1.035434e-03, l1: 0.92393, l2: 0.02529\n",
            "Loss: 9.915193e-04, l1: 0.93182, l2: 0.02579\n",
            "Loss: 9.805859e-04, l1: 0.94666, l2: 0.02684\n",
            "Loss: 9.141486e-04, l1: 0.94112, l2: 0.02658\n",
            "Loss: 8.948890e-04, l1: 0.94202, l2: 0.02664\n",
            "Loss: 8.538358e-04, l1: 0.94395, l2: 0.02686\n",
            "Loss: 7.958452e-04, l1: 0.95593, l2: 0.02764\n",
            "Loss: 7.485989e-04, l1: 0.96292, l2: 0.02825\n",
            "Loss: 7.263752e-04, l1: 0.97660, l2: 0.02902\n",
            "Loss: 7.174097e-04, l1: 0.97653, l2: 0.02906\n",
            "Loss: 7.106172e-04, l1: 0.97907, l2: 0.02920\n",
            "Loss: 6.988546e-04, l1: 0.98103, l2: 0.02932\n",
            "Loss: 6.904398e-04, l1: 0.98216, l2: 0.02944\n",
            "Loss: 6.778083e-04, l1: 0.98232, l2: 0.02955\n",
            "Loss: 6.648157e-04, l1: 0.98296, l2: 0.02972\n",
            "Loss: 6.474590e-04, l1: 0.97979, l2: 0.02976\n",
            "Loss: 6.314240e-04, l1: 0.97775, l2: 0.02995\n",
            "Loss: 6.115498e-04, l1: 0.97468, l2: 0.03011\n",
            "Loss: 5.896559e-04, l1: 0.97215, l2: 0.03045\n",
            "Loss: 5.767656e-04, l1: 0.97055, l2: 0.03081\n",
            "Loss: 5.667272e-04, l1: 0.97075, l2: 0.03080\n",
            "Loss: 5.623207e-04, l1: 0.97198, l2: 0.03088\n",
            "Loss: 5.573402e-04, l1: 0.97263, l2: 0.03095\n",
            "Loss: 5.457370e-04, l1: 0.97234, l2: 0.03115\n",
            "Loss: 5.255116e-04, l1: 0.96929, l2: 0.03144\n",
            "Loss: 5.020174e-04, l1: 0.96218, l2: 0.03179\n",
            "Loss: 4.827458e-04, l1: 0.95450, l2: 0.03194\n",
            "Loss: 4.682827e-04, l1: 0.94783, l2: 0.03194\n",
            "Loss: 4.606895e-04, l1: 0.94644, l2: 0.03192\n",
            "Loss: 4.519599e-04, l1: 0.94641, l2: 0.03198\n",
            "Loss: 4.432691e-04, l1: 0.94783, l2: 0.03213\n",
            "Loss: 4.354916e-04, l1: 0.94834, l2: 0.03227\n",
            "Loss: 4.287860e-04, l1: 0.94884, l2: 0.03245\n",
            "Loss: 4.243273e-04, l1: 0.94833, l2: 0.03251\n",
            "Loss: 4.203486e-04, l1: 0.94750, l2: 0.03262\n",
            "Loss: 4.184009e-04, l1: 0.94639, l2: 0.03254\n",
            "Loss: 4.164984e-04, l1: 0.94586, l2: 0.03252\n",
            "Loss: 4.126831e-04, l1: 0.94447, l2: 0.03251\n",
            "Loss: 4.028202e-04, l1: 0.94443, l2: 0.03267\n",
            "Loss: 3.968818e-04, l1: 0.94070, l2: 0.03274\n",
            "Loss: 3.933289e-04, l1: 0.94046, l2: 0.03275\n",
            "Loss: 3.893159e-04, l1: 0.94111, l2: 0.03282\n",
            "Loss: 3.844181e-04, l1: 0.94097, l2: 0.03279\n",
            "Loss: 3.779145e-04, l1: 0.94197, l2: 0.03283\n",
            "Loss: 3.893638e-04, l1: 0.93076, l2: 0.03251\n",
            "Loss: 3.760949e-04, l1: 0.93896, l2: 0.03274\n",
            "Loss: 3.727682e-04, l1: 0.93952, l2: 0.03280\n",
            "Loss: 3.707864e-04, l1: 0.93996, l2: 0.03289\n",
            "Loss: 3.689951e-04, l1: 0.94035, l2: 0.03297\n",
            "Loss: 3.664286e-04, l1: 0.94138, l2: 0.03311\n",
            "Loss: 3.613177e-04, l1: 0.94234, l2: 0.03323\n",
            "Loss: 3.655059e-04, l1: 0.94436, l2: 0.03352\n",
            "Loss: 3.590427e-04, l1: 0.94310, l2: 0.03334\n",
            "Loss: 3.548439e-04, l1: 0.94386, l2: 0.03333\n",
            "Loss: 3.529091e-04, l1: 0.94370, l2: 0.03320\n",
            "Loss: 3.515657e-04, l1: 0.94340, l2: 0.03313\n",
            "Loss: 3.493741e-04, l1: 0.94293, l2: 0.03303\n",
            "Loss: 3.466696e-04, l1: 0.94273, l2: 0.03299\n",
            "Loss: 3.433644e-04, l1: 0.94216, l2: 0.03299\n",
            "Loss: 3.392754e-04, l1: 0.94299, l2: 0.03313\n",
            "Loss: 3.353455e-04, l1: 0.94406, l2: 0.03324\n",
            "Loss: 3.311904e-04, l1: 0.94706, l2: 0.03342\n",
            "Loss: 3.298823e-04, l1: 0.94466, l2: 0.03324\n",
            "Loss: 3.290536e-04, l1: 0.94557, l2: 0.03322\n",
            "Loss: 3.286061e-04, l1: 0.94563, l2: 0.03321\n",
            "Loss: 3.276328e-04, l1: 0.94566, l2: 0.03316\n",
            "Loss: 3.250778e-04, l1: 0.94560, l2: 0.03310\n",
            "Loss: 3.208593e-04, l1: 0.94571, l2: 0.03304\n",
            "Loss: 3.151432e-04, l1: 0.94599, l2: 0.03305\n",
            "Loss: 3.092750e-04, l1: 0.94681, l2: 0.03320\n",
            "Loss: 3.055960e-04, l1: 0.94725, l2: 0.03323\n",
            "Loss: 3.029078e-04, l1: 0.94689, l2: 0.03333\n",
            "Loss: 2.997383e-04, l1: 0.94723, l2: 0.03338\n",
            "Loss: 2.952883e-04, l1: 0.94806, l2: 0.03346\n",
            "Loss: 2.944037e-04, l1: 0.95124, l2: 0.03344\n",
            "Loss: 2.887331e-04, l1: 0.95020, l2: 0.03328\n",
            "Loss: 2.873874e-04, l1: 0.94956, l2: 0.03328\n",
            "Loss: 2.864837e-04, l1: 0.94952, l2: 0.03322\n",
            "Loss: 2.848944e-04, l1: 0.94956, l2: 0.03315\n",
            "Loss: 2.810768e-04, l1: 0.94988, l2: 0.03304\n",
            "Loss: 2.722726e-04, l1: 0.95120, l2: 0.03289\n",
            "Loss: 2.571735e-04, l1: 0.95375, l2: 0.03272\n",
            "Loss: 2.673490e-04, l1: 0.96447, l2: 0.03265\n",
            "Loss: 2.503957e-04, l1: 0.95796, l2: 0.03269\n",
            "Loss: 2.381989e-04, l1: 0.95999, l2: 0.03270\n",
            "Loss: 2.313123e-04, l1: 0.96199, l2: 0.03293\n",
            "Loss: 2.281844e-04, l1: 0.96279, l2: 0.03308\n",
            "Loss: 2.258112e-04, l1: 0.96313, l2: 0.03319\n",
            "Loss: 2.238024e-04, l1: 0.96383, l2: 0.03321\n",
            "Loss: 2.205165e-04, l1: 0.96447, l2: 0.03317\n",
            "Loss: 2.169840e-04, l1: 0.96445, l2: 0.03302\n",
            "Loss: 2.130665e-04, l1: 0.96648, l2: 0.03297\n",
            "Loss: 2.100871e-04, l1: 0.96511, l2: 0.03266\n",
            "Loss: 2.068083e-04, l1: 0.96624, l2: 0.03251\n",
            "Loss: 2.035054e-04, l1: 0.96242, l2: 0.03219\n",
            "Loss: 2.000248e-04, l1: 0.96573, l2: 0.03216\n",
            "Loss: 1.978924e-04, l1: 0.96551, l2: 0.03218\n",
            "Loss: 1.964428e-04, l1: 0.96566, l2: 0.03216\n",
            "Loss: 1.955069e-04, l1: 0.96601, l2: 0.03212\n",
            "Loss: 1.926719e-04, l1: 0.96764, l2: 0.03207\n",
            "Loss: 2.013318e-04, l1: 0.97621, l2: 0.03213\n",
            "Loss: 1.910394e-04, l1: 0.97008, l2: 0.03208\n",
            "Loss: 1.874116e-04, l1: 0.97150, l2: 0.03203\n",
            "Loss: 1.845046e-04, l1: 0.97209, l2: 0.03201\n",
            "Loss: 1.810665e-04, l1: 0.97260, l2: 0.03206\n",
            "Loss: 1.768296e-04, l1: 0.97253, l2: 0.03212\n",
            "Loss: 1.787062e-04, l1: 0.97698, l2: 0.03241\n",
            "Loss: 1.746326e-04, l1: 0.97442, l2: 0.03224\n",
            "Loss: 1.711049e-04, l1: 0.97547, l2: 0.03227\n",
            "Loss: 1.697174e-04, l1: 0.97383, l2: 0.03229\n",
            "Loss: 1.689540e-04, l1: 0.97508, l2: 0.03227\n",
            "Loss: 1.682917e-04, l1: 0.97597, l2: 0.03225\n",
            "Loss: 1.662873e-04, l1: 0.97832, l2: 0.03223\n",
            "Loss: 1.644827e-04, l1: 0.98014, l2: 0.03209\n",
            "Loss: 1.618271e-04, l1: 0.98020, l2: 0.03214\n",
            "Loss: 1.589226e-04, l1: 0.97921, l2: 0.03218\n",
            "Loss: 1.570176e-04, l1: 0.97852, l2: 0.03225\n",
            "Loss: 1.596560e-04, l1: 0.97303, l2: 0.03195\n",
            "Loss: 1.562439e-04, l1: 0.97674, l2: 0.03215\n",
            "Loss: 1.549031e-04, l1: 0.97642, l2: 0.03212\n",
            "Loss: 1.541128e-04, l1: 0.97668, l2: 0.03208\n",
            "Loss: 1.535127e-04, l1: 0.97726, l2: 0.03204\n",
            "Loss: 1.525717e-04, l1: 0.97843, l2: 0.03203\n",
            "Loss: 1.518853e-04, l1: 0.97975, l2: 0.03204\n",
            "Loss: 1.515112e-04, l1: 0.98025, l2: 0.03205\n",
            "Loss: 1.506957e-04, l1: 0.98100, l2: 0.03206\n",
            "Loss: 1.495221e-04, l1: 0.98188, l2: 0.03203\n",
            "Loss: 1.485500e-04, l1: 0.98204, l2: 0.03203\n",
            "Loss: 1.473647e-04, l1: 0.98183, l2: 0.03199\n",
            "Loss: 1.448975e-04, l1: 0.98101, l2: 0.03192\n",
            "Loss: 1.421936e-04, l1: 0.97973, l2: 0.03188\n",
            "Loss: 1.405444e-04, l1: 0.97787, l2: 0.03190\n",
            "Loss: 1.384767e-04, l1: 0.97914, l2: 0.03195\n",
            "Loss: 1.373033e-04, l1: 0.97938, l2: 0.03198\n",
            "Loss: 1.358325e-04, l1: 0.97982, l2: 0.03201\n",
            "Loss: 1.338388e-04, l1: 0.97993, l2: 0.03198\n",
            "Loss: 1.327646e-04, l1: 0.98028, l2: 0.03196\n",
            "Loss: 1.314184e-04, l1: 0.97988, l2: 0.03187\n",
            "Loss: 1.308080e-04, l1: 0.97934, l2: 0.03184\n",
            "Loss: 1.300157e-04, l1: 0.97955, l2: 0.03183\n",
            "Loss: 1.292760e-04, l1: 0.97995, l2: 0.03180\n",
            "Loss: 1.287397e-04, l1: 0.98033, l2: 0.03179\n",
            "Loss: 1.285569e-04, l1: 0.98187, l2: 0.03177\n",
            "Loss: 1.279223e-04, l1: 0.98109, l2: 0.03178\n",
            "Loss: 1.273259e-04, l1: 0.98073, l2: 0.03179\n",
            "Loss: 1.264211e-04, l1: 0.98073, l2: 0.03179\n",
            "Loss: 1.255544e-04, l1: 0.98142, l2: 0.03182\n",
            "Loss: 1.275803e-04, l1: 0.98248, l2: 0.03166\n",
            "Loss: 1.247677e-04, l1: 0.98179, l2: 0.03176\n",
            "Loss: 1.227952e-04, l1: 0.98266, l2: 0.03183\n",
            "Loss: 1.215245e-04, l1: 0.98416, l2: 0.03189\n",
            "Loss: 1.208437e-04, l1: 0.98455, l2: 0.03190\n",
            "Loss: 1.239553e-04, l1: 0.98945, l2: 0.03205\n",
            "Loss: 1.206691e-04, l1: 0.98547, l2: 0.03193\n",
            "Loss: 1.202680e-04, l1: 0.98546, l2: 0.03194\n",
            "Loss: 1.198261e-04, l1: 0.98514, l2: 0.03193\n",
            "Loss: 1.194767e-04, l1: 0.98526, l2: 0.03190\n",
            "Loss: 1.191787e-04, l1: 0.98513, l2: 0.03186\n",
            "Loss: 1.184009e-04, l1: 0.98498, l2: 0.03178\n",
            "Loss: 1.177046e-04, l1: 0.98382, l2: 0.03171\n",
            "Loss: 1.170956e-04, l1: 0.98397, l2: 0.03173\n",
            "Loss: 1.166911e-04, l1: 0.98409, l2: 0.03176\n",
            "Loss: 1.164047e-04, l1: 0.98390, l2: 0.03178\n",
            "Loss: 1.165286e-04, l1: 0.98413, l2: 0.03184\n",
            "Loss: 1.162521e-04, l1: 0.98400, l2: 0.03180\n",
            "Loss: 1.160735e-04, l1: 0.98390, l2: 0.03181\n",
            "Loss: 1.159645e-04, l1: 0.98406, l2: 0.03182\n",
            "Loss: 1.158329e-04, l1: 0.98448, l2: 0.03183\n",
            "Loss: 1.156759e-04, l1: 0.98487, l2: 0.03184\n",
            "Loss: 1.155111e-04, l1: 0.98538, l2: 0.03185\n",
            "Loss: 1.153268e-04, l1: 0.98564, l2: 0.03185\n",
            "Loss: 1.150269e-04, l1: 0.98603, l2: 0.03186\n",
            "Loss: 1.146789e-04, l1: 0.98546, l2: 0.03182\n",
            "Loss: 1.141645e-04, l1: 0.98526, l2: 0.03183\n",
            "Loss: 1.136717e-04, l1: 0.98448, l2: 0.03182\n",
            "Loss: 1.125775e-04, l1: 0.98341, l2: 0.03180\n",
            "Loss: 1.115237e-04, l1: 0.98361, l2: 0.03181\n",
            "Loss: 1.109907e-04, l1: 0.98391, l2: 0.03182\n",
            "Loss: 1.106382e-04, l1: 0.98474, l2: 0.03183\n",
            "Loss: 1.101021e-04, l1: 0.98591, l2: 0.03185\n",
            "Loss: 1.095202e-04, l1: 0.98680, l2: 0.03186\n",
            "Loss: 1.086514e-04, l1: 0.98757, l2: 0.03187\n",
            "Loss: 1.078291e-04, l1: 0.98817, l2: 0.03191\n",
            "Loss: 1.076393e-04, l1: 0.98831, l2: 0.03187\n",
            "Loss: 1.067258e-04, l1: 0.98726, l2: 0.03187\n",
            "Loss: 1.065437e-04, l1: 0.98690, l2: 0.03187\n",
            "Loss: 1.061470e-04, l1: 0.98646, l2: 0.03187\n",
            "Loss: 1.054598e-04, l1: 0.98593, l2: 0.03186\n",
            "Loss: 1.048097e-04, l1: 0.98590, l2: 0.03183\n",
            "Loss: 1.044412e-04, l1: 0.98552, l2: 0.03178\n",
            "Loss: 1.037471e-04, l1: 0.98557, l2: 0.03177\n",
            "Loss: 1.034040e-04, l1: 0.98555, l2: 0.03175\n",
            "Loss: 1.030603e-04, l1: 0.98521, l2: 0.03172\n",
            "Loss: 1.025002e-04, l1: 0.98425, l2: 0.03168\n",
            "Loss: 1.027793e-04, l1: 0.98369, l2: 0.03159\n",
            "Loss: 1.022563e-04, l1: 0.98402, l2: 0.03165\n",
            "Loss: 1.017737e-04, l1: 0.98229, l2: 0.03161\n",
            "Loss: 1.015172e-04, l1: 0.98195, l2: 0.03162\n",
            "Loss: 1.011010e-04, l1: 0.98173, l2: 0.03162\n",
            "Loss: 1.008054e-04, l1: 0.98119, l2: 0.03160\n",
            "Loss: 1.000917e-04, l1: 0.98201, l2: 0.03161\n",
            "Loss: 9.926066e-05, l1: 0.98281, l2: 0.03162\n",
            "Loss: 9.822695e-05, l1: 0.98353, l2: 0.03160\n",
            "Loss: 9.749430e-05, l1: 0.98318, l2: 0.03159\n",
            "Loss: 9.672137e-05, l1: 0.98343, l2: 0.03165\n",
            "Loss: 9.766017e-05, l1: 0.98190, l2: 0.03159\n",
            "Loss: 9.608937e-05, l1: 0.98284, l2: 0.03163\n",
            "Loss: 9.534090e-05, l1: 0.98327, l2: 0.03167\n",
            "Loss: 9.394932e-05, l1: 0.98512, l2: 0.03179\n",
            "Loss: 9.310232e-05, l1: 0.98672, l2: 0.03184\n",
            "Loss: 9.201408e-05, l1: 0.98975, l2: 0.03191\n",
            "Loss: 9.282980e-05, l1: 0.99472, l2: 0.03196\n",
            "Loss: 9.150827e-05, l1: 0.99165, l2: 0.03193\n",
            "Loss: 9.113614e-05, l1: 0.99126, l2: 0.03189\n",
            "Loss: 9.068032e-05, l1: 0.99054, l2: 0.03183\n",
            "Loss: 9.034828e-05, l1: 0.99014, l2: 0.03180\n",
            "Loss: 8.989741e-05, l1: 0.98962, l2: 0.03178\n",
            "Loss: 9.271680e-05, l1: 0.99181, l2: 0.03171\n",
            "Loss: 8.970644e-05, l1: 0.99006, l2: 0.03176\n",
            "Loss: 8.938443e-05, l1: 0.99002, l2: 0.03176\n",
            "Loss: 8.904855e-05, l1: 0.99030, l2: 0.03177\n",
            "Loss: 8.864138e-05, l1: 0.99054, l2: 0.03174\n",
            "Loss: 8.812905e-05, l1: 0.99088, l2: 0.03172\n",
            "Loss: 8.695473e-05, l1: 0.99137, l2: 0.03169\n",
            "Loss: 8.609795e-05, l1: 0.98979, l2: 0.03167\n",
            "Loss: 8.515885e-05, l1: 0.99023, l2: 0.03168\n",
            "Loss: 8.470337e-05, l1: 0.99080, l2: 0.03175\n",
            "Loss: 8.395666e-05, l1: 0.99017, l2: 0.03175\n",
            "Loss: 8.428869e-05, l1: 0.98879, l2: 0.03170\n",
            "Loss: 8.379472e-05, l1: 0.98967, l2: 0.03173\n",
            "Loss: 8.355911e-05, l1: 0.98956, l2: 0.03175\n",
            "Loss: 8.341648e-05, l1: 0.98945, l2: 0.03176\n",
            "Loss: 8.326709e-05, l1: 0.98933, l2: 0.03176\n",
            "Loss: 8.297167e-05, l1: 0.98885, l2: 0.03175\n",
            "Loss: 8.319483e-05, l1: 0.98844, l2: 0.03174\n",
            "Loss: 8.268976e-05, l1: 0.98867, l2: 0.03175\n",
            "Loss: 8.221882e-05, l1: 0.98836, l2: 0.03172\n",
            "Loss: 8.136316e-05, l1: 0.98796, l2: 0.03169\n",
            "Loss: 8.061448e-05, l1: 0.98835, l2: 0.03168\n",
            "Loss: 7.978067e-05, l1: 0.98874, l2: 0.03172\n",
            "Loss: 7.920185e-05, l1: 0.98915, l2: 0.03176\n",
            "Loss: 7.884458e-05, l1: 0.98928, l2: 0.03177\n",
            "Loss: 7.844569e-05, l1: 0.98915, l2: 0.03177\n",
            "Loss: 7.809969e-05, l1: 0.98880, l2: 0.03174\n",
            "Loss: 7.771915e-05, l1: 0.98841, l2: 0.03174\n",
            "Loss: 7.724234e-05, l1: 0.98814, l2: 0.03173\n",
            "Loss: 7.653934e-05, l1: 0.98805, l2: 0.03175\n",
            "Loss: 7.545656e-05, l1: 0.98861, l2: 0.03180\n",
            "Loss: 7.451919e-05, l1: 0.99023, l2: 0.03187\n",
            "Loss: 7.392112e-05, l1: 0.99110, l2: 0.03190\n",
            "Loss: 7.356175e-05, l1: 0.99213, l2: 0.03189\n",
            "Loss: 7.333091e-05, l1: 0.99198, l2: 0.03186\n",
            "Loss: 7.313737e-05, l1: 0.99180, l2: 0.03183\n",
            "Loss: 7.304922e-05, l1: 0.99188, l2: 0.03179\n",
            "Loss: 7.293181e-05, l1: 0.99171, l2: 0.03180\n",
            "Loss: 7.285271e-05, l1: 0.99167, l2: 0.03180\n",
            "Loss: 7.272852e-05, l1: 0.99171, l2: 0.03181\n",
            "Loss: 7.249898e-05, l1: 0.99172, l2: 0.03180\n",
            "Loss: 7.216645e-05, l1: 0.99227, l2: 0.03181\n",
            "Loss: 7.183520e-05, l1: 0.99250, l2: 0.03180\n",
            "Loss: 7.095844e-05, l1: 0.99309, l2: 0.03177\n",
            "Loss: 7.015328e-05, l1: 0.99333, l2: 0.03174\n",
            "Loss: 7.029697e-05, l1: 0.99387, l2: 0.03169\n",
            "Loss: 6.964956e-05, l1: 0.99359, l2: 0.03172\n",
            "Loss: 6.901474e-05, l1: 0.99336, l2: 0.03173\n",
            "Loss: 6.880307e-05, l1: 0.99307, l2: 0.03174\n",
            "Loss: 6.869373e-05, l1: 0.99278, l2: 0.03175\n",
            "Loss: 6.857622e-05, l1: 0.99214, l2: 0.03174\n",
            "Loss: 6.865439e-05, l1: 0.99193, l2: 0.03175\n",
            "Loss: 6.853008e-05, l1: 0.99206, l2: 0.03174\n",
            "Loss: 6.844365e-05, l1: 0.99157, l2: 0.03173\n",
            "Loss: 6.835701e-05, l1: 0.99127, l2: 0.03172\n",
            "Loss: 6.827109e-05, l1: 0.99113, l2: 0.03173\n",
            "Loss: 6.809646e-05, l1: 0.99145, l2: 0.03175\n",
            "Loss: 6.787106e-05, l1: 0.99158, l2: 0.03179\n",
            "Loss: 6.759835e-05, l1: 0.99210, l2: 0.03184\n",
            "Loss: 6.715811e-05, l1: 0.99341, l2: 0.03192\n",
            "Loss: 6.713796e-05, l1: 0.99364, l2: 0.03199\n",
            "Loss: 6.696065e-05, l1: 0.99360, l2: 0.03195\n",
            "Loss: 6.686399e-05, l1: 0.99367, l2: 0.03194\n",
            "Loss: 6.674843e-05, l1: 0.99366, l2: 0.03194\n",
            "Loss: 6.662733e-05, l1: 0.99337, l2: 0.03194\n",
            "Loss: 6.655765e-05, l1: 0.99345, l2: 0.03198\n",
            "Loss: 6.630195e-05, l1: 0.99306, l2: 0.03199\n",
            "Loss: 6.615266e-05, l1: 0.99281, l2: 0.03199\n",
            "Loss: 6.609350e-05, l1: 0.99256, l2: 0.03201\n",
            "Loss: 6.588741e-05, l1: 0.99238, l2: 0.03200\n",
            "Loss: 6.583230e-05, l1: 0.99237, l2: 0.03199\n",
            "Loss: 6.573540e-05, l1: 0.99225, l2: 0.03198\n",
            "Loss: 6.562102e-05, l1: 0.99217, l2: 0.03196\n",
            "Loss: 6.539270e-05, l1: 0.99167, l2: 0.03194\n",
            "Loss: 6.517333e-05, l1: 0.99126, l2: 0.03192\n",
            "Loss: 6.493262e-05, l1: 0.99101, l2: 0.03192\n",
            "Loss: 6.456974e-05, l1: 0.99078, l2: 0.03190\n",
            "Loss: 6.417625e-05, l1: 0.99072, l2: 0.03185\n",
            "Loss: 6.385070e-05, l1: 0.99092, l2: 0.03184\n",
            "Loss: 6.345681e-05, l1: 0.99171, l2: 0.03182\n",
            "Loss: 6.329267e-05, l1: 0.99191, l2: 0.03181\n",
            "Loss: 6.305064e-05, l1: 0.99217, l2: 0.03180\n",
            "Loss: 6.267144e-05, l1: 0.99309, l2: 0.03182\n",
            "Loss: 6.237223e-05, l1: 0.99371, l2: 0.03182\n",
            "Loss: 6.214456e-05, l1: 0.99415, l2: 0.03182\n",
            "Loss: 6.196946e-05, l1: 0.99435, l2: 0.03183\n",
            "Loss: 6.177287e-05, l1: 0.99468, l2: 0.03181\n",
            "Loss: 6.155169e-05, l1: 0.99476, l2: 0.03179\n",
            "Loss: 6.145109e-05, l1: 0.99530, l2: 0.03178\n",
            "Loss: 6.109836e-05, l1: 0.99503, l2: 0.03177\n",
            "Loss: 6.087180e-05, l1: 0.99479, l2: 0.03178\n",
            "Loss: 6.068484e-05, l1: 0.99461, l2: 0.03180\n",
            "Loss: 6.056971e-05, l1: 0.99372, l2: 0.03182\n",
            "Loss: 6.034105e-05, l1: 0.99411, l2: 0.03184\n",
            "Loss: 6.021944e-05, l1: 0.99430, l2: 0.03184\n",
            "Loss: 6.001939e-05, l1: 0.99461, l2: 0.03184\n",
            "Loss: 5.985941e-05, l1: 0.99472, l2: 0.03184\n",
            "Loss: 5.954090e-05, l1: 0.99533, l2: 0.03186\n",
            "Loss: 5.938721e-05, l1: 0.99544, l2: 0.03187\n",
            "Loss: 5.911075e-05, l1: 0.99538, l2: 0.03187\n",
            "Loss: 5.891686e-05, l1: 0.99537, l2: 0.03188\n",
            "Loss: 5.877358e-05, l1: 0.99517, l2: 0.03188\n",
            "Loss: 5.854803e-05, l1: 0.99519, l2: 0.03190\n",
            "Loss: 5.871072e-05, l1: 0.99433, l2: 0.03188\n",
            "Loss: 5.846389e-05, l1: 0.99488, l2: 0.03190\n",
            "Loss: 5.833480e-05, l1: 0.99483, l2: 0.03188\n",
            "Loss: 5.816720e-05, l1: 0.99481, l2: 0.03187\n",
            "Loss: 5.806191e-05, l1: 0.99494, l2: 0.03187\n",
            "Loss: 5.778114e-05, l1: 0.99529, l2: 0.03186\n",
            "Loss: 5.803557e-05, l1: 0.99529, l2: 0.03186\n",
            "Loss: 5.768344e-05, l1: 0.99529, l2: 0.03186\n",
            "Loss: 5.754075e-05, l1: 0.99539, l2: 0.03186\n",
            "Loss: 5.736857e-05, l1: 0.99533, l2: 0.03185\n",
            "Loss: 5.727939e-05, l1: 0.99527, l2: 0.03185\n",
            "Loss: 5.715262e-05, l1: 0.99526, l2: 0.03184\n",
            "Loss: 5.752942e-05, l1: 0.99479, l2: 0.03184\n",
            "Loss: 5.710842e-05, l1: 0.99514, l2: 0.03184\n",
            "Loss: 5.704633e-05, l1: 0.99528, l2: 0.03184\n",
            "Loss: 5.697720e-05, l1: 0.99546, l2: 0.03183\n",
            "Loss: 5.691579e-05, l1: 0.99562, l2: 0.03183\n",
            "Loss: 5.681928e-05, l1: 0.99596, l2: 0.03184\n",
            "Loss: 5.681994e-05, l1: 0.99559, l2: 0.03184\n",
            "Loss: 5.678205e-05, l1: 0.99578, l2: 0.03184\n",
            "Loss: 5.672065e-05, l1: 0.99583, l2: 0.03185\n",
            "Loss: 5.667908e-05, l1: 0.99621, l2: 0.03185\n",
            "Loss: 5.655456e-05, l1: 0.99583, l2: 0.03187\n",
            "Loss: 5.648321e-05, l1: 0.99561, l2: 0.03188\n",
            "Loss: 5.638000e-05, l1: 0.99534, l2: 0.03189\n",
            "Loss: 5.625414e-05, l1: 0.99515, l2: 0.03190\n",
            "Loss: 5.616565e-05, l1: 0.99470, l2: 0.03193\n",
            "Loss: 5.565009e-05, l1: 0.99450, l2: 0.03191\n",
            "Loss: 5.526733e-05, l1: 0.99455, l2: 0.03189\n",
            "Loss: 5.489001e-05, l1: 0.99445, l2: 0.03188\n",
            "Loss: 5.455195e-05, l1: 0.99470, l2: 0.03187\n",
            "Loss: 5.442760e-05, l1: 0.99474, l2: 0.03188\n",
            "Loss: 5.432153e-05, l1: 0.99525, l2: 0.03189\n",
            "Loss: 5.412659e-05, l1: 0.99557, l2: 0.03190\n",
            "Loss: 5.405149e-05, l1: 0.99565, l2: 0.03190\n",
            "Loss: 5.390403e-05, l1: 0.99603, l2: 0.03190\n",
            "Loss: 5.370128e-05, l1: 0.99648, l2: 0.03191\n",
            "Loss: 5.332785e-05, l1: 0.99738, l2: 0.03192\n",
            "Loss: 5.334844e-05, l1: 0.99824, l2: 0.03195\n",
            "Loss: 5.311531e-05, l1: 0.99780, l2: 0.03194\n",
            "Loss: 5.292167e-05, l1: 0.99766, l2: 0.03193\n",
            "Loss: 5.279441e-05, l1: 0.99746, l2: 0.03193\n",
            "Loss: 5.273755e-05, l1: 0.99744, l2: 0.03193\n",
            "Loss: 5.265978e-05, l1: 0.99730, l2: 0.03192\n",
            "Loss: 5.242472e-05, l1: 0.99722, l2: 0.03191\n",
            "Loss: 5.225115e-05, l1: 0.99743, l2: 0.03190\n",
            "Loss: 5.204204e-05, l1: 0.99767, l2: 0.03190\n",
            "Loss: 5.191404e-05, l1: 0.99807, l2: 0.03189\n",
            "Loss: 5.182813e-05, l1: 0.99821, l2: 0.03189\n",
            "Loss: 5.173642e-05, l1: 0.99819, l2: 0.03189\n",
            "Loss: 5.160331e-05, l1: 0.99799, l2: 0.03190\n",
            "Loss: 5.140880e-05, l1: 0.99790, l2: 0.03191\n",
            "Loss: 5.129625e-05, l1: 0.99736, l2: 0.03192\n",
            "Loss: 5.109746e-05, l1: 0.99791, l2: 0.03192\n",
            "Loss: 5.099280e-05, l1: 0.99796, l2: 0.03192\n",
            "Loss: 5.084895e-05, l1: 0.99801, l2: 0.03191\n",
            "Loss: 5.070386e-05, l1: 0.99807, l2: 0.03190\n",
            "Loss: 5.046948e-05, l1: 0.99819, l2: 0.03189\n",
            "Loss: 5.089574e-05, l1: 0.99749, l2: 0.03188\n",
            "Loss: 5.036237e-05, l1: 0.99797, l2: 0.03189\n",
            "Loss: 5.023330e-05, l1: 0.99795, l2: 0.03189\n",
            "Loss: 5.017068e-05, l1: 0.99779, l2: 0.03188\n",
            "Loss: 5.010073e-05, l1: 0.99756, l2: 0.03188\n",
            "Loss: 4.997543e-05, l1: 0.99684, l2: 0.03187\n",
            "Loss: 4.988174e-05, l1: 0.99628, l2: 0.03186\n",
            "Loss: 4.977878e-05, l1: 0.99619, l2: 0.03186\n",
            "Loss: 4.967830e-05, l1: 0.99598, l2: 0.03185\n",
            "Loss: 4.962930e-05, l1: 0.99583, l2: 0.03185\n",
            "Loss: 4.943972e-05, l1: 0.99528, l2: 0.03186\n",
            "Loss: 4.967650e-05, l1: 0.99570, l2: 0.03187\n",
            "Loss: 4.938315e-05, l1: 0.99541, l2: 0.03187\n",
            "Loss: 4.925712e-05, l1: 0.99517, l2: 0.03188\n",
            "Loss: 4.916875e-05, l1: 0.99526, l2: 0.03188\n",
            "Loss: 4.911365e-05, l1: 0.99549, l2: 0.03188\n",
            "Loss: 4.906368e-05, l1: 0.99576, l2: 0.03188\n",
            "Loss: 4.896063e-05, l1: 0.99607, l2: 0.03189\n",
            "Loss: 4.892738e-05, l1: 0.99605, l2: 0.03189\n",
            "Loss: 4.888266e-05, l1: 0.99618, l2: 0.03189\n",
            "Loss: 4.886126e-05, l1: 0.99634, l2: 0.03189\n",
            "Loss: 4.884397e-05, l1: 0.99650, l2: 0.03190\n",
            "Loss: 4.881476e-05, l1: 0.99673, l2: 0.03190\n",
            "Loss: 4.885310e-05, l1: 0.99741, l2: 0.03192\n",
            "Loss: 4.878966e-05, l1: 0.99699, l2: 0.03191\n",
            "Loss: 4.872749e-05, l1: 0.99722, l2: 0.03192\n",
            "Loss: 4.862848e-05, l1: 0.99743, l2: 0.03195\n",
            "Loss: 4.862397e-05, l1: 0.99741, l2: 0.03199\n",
            "Loss: 4.854965e-05, l1: 0.99742, l2: 0.03197\n",
            "Loss: 4.844223e-05, l1: 0.99736, l2: 0.03198\n",
            "Loss: 4.830912e-05, l1: 0.99718, l2: 0.03199\n",
            "Loss: 4.823173e-05, l1: 0.99709, l2: 0.03199\n",
            "Loss: 4.821327e-05, l1: 0.99729, l2: 0.03200\n",
            "Loss: 4.813855e-05, l1: 0.99718, l2: 0.03199\n",
            "Loss: 4.810206e-05, l1: 0.99722, l2: 0.03199\n",
            "Loss: 4.801057e-05, l1: 0.99734, l2: 0.03200\n",
            "Loss: 4.791347e-05, l1: 0.99733, l2: 0.03201\n",
            "Loss: 4.783171e-05, l1: 0.99721, l2: 0.03201\n",
            "Loss: 4.762096e-05, l1: 0.99706, l2: 0.03202\n",
            "Loss: 4.750826e-05, l1: 0.99692, l2: 0.03201\n",
            "Loss: 4.751010e-05, l1: 0.99668, l2: 0.03199\n",
            "Loss: 4.745238e-05, l1: 0.99680, l2: 0.03200\n",
            "Loss: 4.737465e-05, l1: 0.99665, l2: 0.03199\n",
            "Loss: 4.731719e-05, l1: 0.99660, l2: 0.03198\n",
            "Loss: 4.717270e-05, l1: 0.99650, l2: 0.03195\n",
            "Loss: 4.699608e-05, l1: 0.99646, l2: 0.03191\n",
            "Loss: 4.702422e-05, l1: 0.99624, l2: 0.03185\n",
            "Loss: 4.690161e-05, l1: 0.99636, l2: 0.03188\n",
            "Loss: 4.677062e-05, l1: 0.99634, l2: 0.03186\n",
            "Loss: 4.669849e-05, l1: 0.99635, l2: 0.03186\n",
            "Loss: 4.660456e-05, l1: 0.99628, l2: 0.03185\n",
            "Loss: 4.650722e-05, l1: 0.99608, l2: 0.03185\n",
            "Loss: 4.651837e-05, l1: 0.99621, l2: 0.03181\n",
            "Loss: 4.644712e-05, l1: 0.99614, l2: 0.03183\n",
            "Loss: 4.634142e-05, l1: 0.99600, l2: 0.03181\n",
            "Loss: 4.625820e-05, l1: 0.99589, l2: 0.03180\n",
            "Loss: 4.620439e-05, l1: 0.99587, l2: 0.03179\n",
            "Loss: 4.613445e-05, l1: 0.99590, l2: 0.03178\n",
            "Loss: 4.601937e-05, l1: 0.99603, l2: 0.03177\n",
            "Loss: 4.594809e-05, l1: 0.99647, l2: 0.03176\n",
            "Loss: 4.584955e-05, l1: 0.99625, l2: 0.03177\n",
            "Loss: 4.580563e-05, l1: 0.99630, l2: 0.03178\n",
            "Loss: 4.574432e-05, l1: 0.99632, l2: 0.03178\n",
            "Loss: 4.570171e-05, l1: 0.99633, l2: 0.03178\n",
            "Loss: 4.565736e-05, l1: 0.99621, l2: 0.03177\n",
            "Loss: 4.561297e-05, l1: 0.99611, l2: 0.03175\n",
            "Loss: 4.556100e-05, l1: 0.99610, l2: 0.03176\n",
            "Loss: 4.546883e-05, l1: 0.99616, l2: 0.03177\n",
            "Loss: 4.538861e-05, l1: 0.99632, l2: 0.03178\n",
            "Loss: 4.525804e-05, l1: 0.99655, l2: 0.03179\n",
            "Loss: 4.520273e-05, l1: 0.99731, l2: 0.03179\n",
            "Loss: 4.509882e-05, l1: 0.99711, l2: 0.03178\n",
            "Loss: 4.505676e-05, l1: 0.99708, l2: 0.03178\n",
            "Loss: 4.501387e-05, l1: 0.99716, l2: 0.03177\n",
            "Loss: 4.495368e-05, l1: 0.99750, l2: 0.03176\n",
            "Loss: 4.491771e-05, l1: 0.99761, l2: 0.03174\n",
            "Loss: 4.484579e-05, l1: 0.99778, l2: 0.03175\n",
            "Loss: 4.480392e-05, l1: 0.99786, l2: 0.03175\n",
            "Loss: 4.476300e-05, l1: 0.99791, l2: 0.03175\n",
            "Loss: 4.469126e-05, l1: 0.99797, l2: 0.03174\n",
            "Loss: 4.462108e-05, l1: 0.99759, l2: 0.03174\n",
            "Loss: 4.453846e-05, l1: 0.99794, l2: 0.03173\n",
            "Loss: 4.448895e-05, l1: 0.99802, l2: 0.03174\n",
            "Loss: 4.443349e-05, l1: 0.99803, l2: 0.03174\n",
            "Loss: 4.437906e-05, l1: 0.99798, l2: 0.03174\n",
            "Loss: 4.433564e-05, l1: 0.99795, l2: 0.03174\n",
            "Loss: 4.424016e-05, l1: 0.99783, l2: 0.03175\n",
            "Loss: 4.415458e-05, l1: 0.99781, l2: 0.03175\n",
            "Loss: 4.403068e-05, l1: 0.99785, l2: 0.03175\n",
            "Loss: 4.380632e-05, l1: 0.99770, l2: 0.03174\n",
            "Loss: 4.349332e-05, l1: 0.99839, l2: 0.03173\n",
            "Loss: 4.326407e-05, l1: 0.99868, l2: 0.03171\n",
            "Loss: 4.297436e-05, l1: 0.99919, l2: 0.03170\n",
            "Loss: 4.274280e-05, l1: 0.99908, l2: 0.03168\n",
            "Loss: 4.261480e-05, l1: 0.99910, l2: 0.03169\n",
            "Loss: 4.251872e-05, l1: 0.99911, l2: 0.03168\n",
            "Loss: 4.246347e-05, l1: 0.99915, l2: 0.03167\n",
            "Loss: 4.238507e-05, l1: 0.99916, l2: 0.03164\n",
            "Loss: 4.228225e-05, l1: 0.99911, l2: 0.03163\n",
            "Loss: 4.222618e-05, l1: 0.99892, l2: 0.03160\n",
            "Loss: 4.234098e-05, l1: 0.99841, l2: 0.03157\n",
            "Loss: 4.217666e-05, l1: 0.99874, l2: 0.03159\n",
            "Loss: 4.214837e-05, l1: 0.99861, l2: 0.03160\n",
            "Loss: 4.211988e-05, l1: 0.99850, l2: 0.03161\n",
            "Loss: 4.207863e-05, l1: 0.99839, l2: 0.03162\n",
            "Loss: 4.199557e-05, l1: 0.99818, l2: 0.03163\n",
            "Loss: 4.207185e-05, l1: 0.99821, l2: 0.03166\n",
            "Loss: 4.194446e-05, l1: 0.99819, l2: 0.03164\n",
            "Loss: 4.181162e-05, l1: 0.99766, l2: 0.03166\n",
            "Loss: 4.173554e-05, l1: 0.99772, l2: 0.03167\n",
            "Loss: 4.166890e-05, l1: 0.99773, l2: 0.03166\n",
            "Loss: 4.162059e-05, l1: 0.99775, l2: 0.03164\n",
            "Loss: 4.158651e-05, l1: 0.99769, l2: 0.03163\n",
            "Loss: 4.155977e-05, l1: 0.99777, l2: 0.03162\n",
            "Loss: 4.155198e-05, l1: 0.99730, l2: 0.03162\n",
            "Loss: 4.151960e-05, l1: 0.99752, l2: 0.03162\n",
            "Loss: 4.151054e-05, l1: 0.99758, l2: 0.03163\n",
            "Loss: 4.148659e-05, l1: 0.99770, l2: 0.03164\n",
            "Loss: 4.146478e-05, l1: 0.99780, l2: 0.03165\n",
            "Loss: 4.142823e-05, l1: 0.99793, l2: 0.03165\n",
            "Loss: 4.139324e-05, l1: 0.99827, l2: 0.03168\n",
            "Loss: 4.135444e-05, l1: 0.99827, l2: 0.03167\n",
            "Loss: 4.132279e-05, l1: 0.99829, l2: 0.03167\n",
            "Loss: 4.128975e-05, l1: 0.99840, l2: 0.03168\n",
            "Loss: 4.123065e-05, l1: 0.99859, l2: 0.03169\n",
            "Loss: 4.116710e-05, l1: 0.99880, l2: 0.03170\n",
            "Loss: 4.114070e-05, l1: 0.99897, l2: 0.03171\n",
            "Loss: 4.107760e-05, l1: 0.99898, l2: 0.03171\n",
            "Loss: 4.102766e-05, l1: 0.99893, l2: 0.03171\n",
            "Loss: 4.097035e-05, l1: 0.99888, l2: 0.03170\n",
            "Loss: 4.093225e-05, l1: 0.99877, l2: 0.03170\n",
            "Loss: 4.086981e-05, l1: 0.99877, l2: 0.03171\n",
            "Loss: 4.083000e-05, l1: 0.99878, l2: 0.03172\n",
            "Loss: 4.079099e-05, l1: 0.99873, l2: 0.03172\n",
            "Loss: 4.076793e-05, l1: 0.99873, l2: 0.03173\n",
            "Loss: 4.075909e-05, l1: 0.99877, l2: 0.03174\n",
            "Loss: 4.075130e-05, l1: 0.99881, l2: 0.03173\n",
            "Loss: 4.073586e-05, l1: 0.99888, l2: 0.03173\n",
            "Loss: 4.071817e-05, l1: 0.99888, l2: 0.03173\n",
            "Loss: 4.069471e-05, l1: 0.99889, l2: 0.03173\n",
            "Loss: 4.067022e-05, l1: 0.99870, l2: 0.03174\n",
            "Loss: 4.064546e-05, l1: 0.99869, l2: 0.03174\n",
            "Loss: 4.061118e-05, l1: 0.99862, l2: 0.03174\n",
            "Loss: 4.057035e-05, l1: 0.99859, l2: 0.03175\n",
            "Loss: 4.054281e-05, l1: 0.99862, l2: 0.03175\n",
            "Loss: 4.051353e-05, l1: 0.99863, l2: 0.03175\n",
            "Loss: 4.048417e-05, l1: 0.99863, l2: 0.03175\n",
            "Loss: 4.044506e-05, l1: 0.99861, l2: 0.03176\n",
            "Loss: 4.041001e-05, l1: 0.99860, l2: 0.03176\n",
            "Loss: 4.035651e-05, l1: 0.99864, l2: 0.03176\n",
            "Loss: 4.029443e-05, l1: 0.99855, l2: 0.03177\n",
            "Loss: 4.023597e-05, l1: 0.99849, l2: 0.03178\n",
            "Loss: 4.019095e-05, l1: 0.99850, l2: 0.03180\n",
            "Loss: 4.010273e-05, l1: 0.99862, l2: 0.03182\n",
            "Loss: 4.067770e-05, l1: 0.99936, l2: 0.03190\n",
            "Loss: 4.007249e-05, l1: 0.99875, l2: 0.03184\n",
            "Loss: 4.001081e-05, l1: 0.99890, l2: 0.03185\n",
            "Loss: 3.996748e-05, l1: 0.99903, l2: 0.03186\n",
            "Loss: 3.992434e-05, l1: 0.99913, l2: 0.03186\n",
            "Loss: 3.983987e-05, l1: 0.99936, l2: 0.03187\n",
            "Loss: 3.974231e-05, l1: 0.99951, l2: 0.03188\n",
            "Loss: 3.966369e-05, l1: 0.99984, l2: 0.03191\n",
            "Loss: 3.961368e-05, l1: 0.99975, l2: 0.03190\n",
            "Loss: 3.957882e-05, l1: 0.99972, l2: 0.03191\n",
            "Loss: 3.951749e-05, l1: 0.99977, l2: 0.03192\n",
            "Loss: 3.941120e-05, l1: 0.99969, l2: 0.03193\n",
            "Loss: 3.932180e-05, l1: 0.99980, l2: 0.03194\n",
            "Loss: 3.926965e-05, l1: 0.99982, l2: 0.03194\n",
            "Loss: 3.922672e-05, l1: 0.99965, l2: 0.03194\n",
            "Loss: 3.919843e-05, l1: 0.99927, l2: 0.03193\n",
            "Loss: 3.914841e-05, l1: 0.99904, l2: 0.03193\n",
            "Loss: 3.913082e-05, l1: 0.99900, l2: 0.03194\n",
            "Loss: 3.911434e-05, l1: 0.99894, l2: 0.03195\n",
            "Loss: 3.909625e-05, l1: 0.99890, l2: 0.03195\n",
            "Loss: 3.906487e-05, l1: 0.99891, l2: 0.03195\n",
            "Loss: 3.905258e-05, l1: 0.99897, l2: 0.03195\n",
            "Loss: 3.898944e-05, l1: 0.99904, l2: 0.03195\n",
            "Loss: 3.894310e-05, l1: 0.99914, l2: 0.03195\n",
            "Loss: 3.889493e-05, l1: 0.99924, l2: 0.03196\n",
            "Loss: 3.885319e-05, l1: 0.99923, l2: 0.03196\n",
            "Loss: 3.878638e-05, l1: 0.99907, l2: 0.03197\n",
            "Loss: 3.883679e-05, l1: 0.99892, l2: 0.03198\n",
            "Loss: 3.876702e-05, l1: 0.99902, l2: 0.03197\n",
            "Loss: 3.875057e-05, l1: 0.99891, l2: 0.03197\n",
            "Loss: 3.874261e-05, l1: 0.99884, l2: 0.03197\n",
            "Loss: 3.873242e-05, l1: 0.99876, l2: 0.03196\n",
            "Loss: 3.871597e-05, l1: 0.99870, l2: 0.03196\n",
            "Loss: 3.868837e-05, l1: 0.99867, l2: 0.03195\n",
            "Loss: 3.865952e-05, l1: 0.99865, l2: 0.03194\n",
            "Loss: 3.862305e-05, l1: 0.99876, l2: 0.03194\n",
            "Loss: 3.859666e-05, l1: 0.99885, l2: 0.03195\n",
            "Loss: 3.857004e-05, l1: 0.99888, l2: 0.03196\n",
            "Loss: 3.854152e-05, l1: 0.99893, l2: 0.03197\n",
            "Loss: 3.851279e-05, l1: 0.99887, l2: 0.03197\n",
            "Loss: 3.848694e-05, l1: 0.99880, l2: 0.03197\n",
            "Loss: 3.847323e-05, l1: 0.99871, l2: 0.03197\n",
            "Loss: 3.844888e-05, l1: 0.99851, l2: 0.03196\n",
            "Loss: 3.843026e-05, l1: 0.99846, l2: 0.03196\n",
            "Loss: 3.840755e-05, l1: 0.99839, l2: 0.03195\n",
            "Loss: 3.837980e-05, l1: 0.99843, l2: 0.03193\n",
            "Loss: 3.834890e-05, l1: 0.99838, l2: 0.03193\n",
            "Loss: 3.830579e-05, l1: 0.99831, l2: 0.03191\n",
            "Loss: 3.830980e-05, l1: 0.99819, l2: 0.03189\n",
            "Loss: 3.829078e-05, l1: 0.99825, l2: 0.03190\n",
            "Loss: 3.826435e-05, l1: 0.99821, l2: 0.03189\n",
            "Loss: 3.824169e-05, l1: 0.99815, l2: 0.03188\n",
            "Loss: 3.818196e-05, l1: 0.99801, l2: 0.03187\n",
            "Loss: 3.812978e-05, l1: 0.99786, l2: 0.03186\n",
            "Loss: 3.808091e-05, l1: 0.99791, l2: 0.03186\n",
            "Loss: 3.806538e-05, l1: 0.99801, l2: 0.03186\n",
            "Loss: 3.800031e-05, l1: 0.99814, l2: 0.03187\n",
            "Loss: 3.797981e-05, l1: 0.99819, l2: 0.03186\n",
            "Loss: 3.793985e-05, l1: 0.99830, l2: 0.03185\n",
            "Loss: 3.789490e-05, l1: 0.99839, l2: 0.03183\n",
            "Loss: 3.804951e-05, l1: 0.99905, l2: 0.03180\n",
            "Loss: 3.786340e-05, l1: 0.99858, l2: 0.03183\n",
            "Loss: 3.779001e-05, l1: 0.99868, l2: 0.03181\n",
            "Loss: 3.774607e-05, l1: 0.99864, l2: 0.03180\n",
            "Loss: 3.769402e-05, l1: 0.99867, l2: 0.03181\n",
            "Loss: 3.765565e-05, l1: 0.99858, l2: 0.03181\n",
            "Loss: 3.761605e-05, l1: 0.99849, l2: 0.03181\n",
            "Loss: 3.758989e-05, l1: 0.99847, l2: 0.03180\n",
            "Loss: 3.756754e-05, l1: 0.99845, l2: 0.03179\n",
            "Loss: 3.756193e-05, l1: 0.99847, l2: 0.03178\n",
            "Loss: 3.754319e-05, l1: 0.99847, l2: 0.03178\n",
            "Loss: 3.753546e-05, l1: 0.99847, l2: 0.03179\n",
            "Loss: 3.752788e-05, l1: 0.99846, l2: 0.03179\n",
            "Loss: 3.751910e-05, l1: 0.99842, l2: 0.03179\n",
            "Loss: 3.750487e-05, l1: 0.99833, l2: 0.03178\n",
            "Loss: 3.750253e-05, l1: 0.99826, l2: 0.03179\n",
            "Loss: 3.748311e-05, l1: 0.99818, l2: 0.03178\n",
            "Loss: 3.747639e-05, l1: 0.99816, l2: 0.03179\n",
            "Loss: 3.746797e-05, l1: 0.99810, l2: 0.03179\n",
            "Loss: 3.747024e-05, l1: 0.99806, l2: 0.03179\n",
            "Loss: 3.746389e-05, l1: 0.99808, l2: 0.03179\n",
            "Loss: 3.745680e-05, l1: 0.99803, l2: 0.03179\n",
            "Loss: 3.744509e-05, l1: 0.99800, l2: 0.03179\n",
            "Loss: 3.743059e-05, l1: 0.99794, l2: 0.03180\n",
            "Loss: 3.741371e-05, l1: 0.99795, l2: 0.03180\n",
            "Loss: 3.738282e-05, l1: 0.99800, l2: 0.03180\n",
            "Loss: 3.735974e-05, l1: 0.99800, l2: 0.03179\n",
            "Loss: 3.733241e-05, l1: 0.99791, l2: 0.03179\n",
            "Loss: 3.730956e-05, l1: 0.99778, l2: 0.03178\n",
            "Loss: 3.728728e-05, l1: 0.99767, l2: 0.03178\n",
            "Loss: 3.725341e-05, l1: 0.99764, l2: 0.03178\n",
            "Loss: 3.719844e-05, l1: 0.99771, l2: 0.03178\n",
            "Loss: 3.724008e-05, l1: 0.99819, l2: 0.03179\n",
            "Loss: 3.717744e-05, l1: 0.99789, l2: 0.03178\n",
            "Loss: 3.713230e-05, l1: 0.99811, l2: 0.03178\n",
            "Loss: 3.711140e-05, l1: 0.99852, l2: 0.03179\n",
            "Loss: 3.705581e-05, l1: 0.99861, l2: 0.03180\n",
            "Loss: 3.701928e-05, l1: 0.99872, l2: 0.03181\n",
            "Loss: 3.698052e-05, l1: 0.99888, l2: 0.03182\n",
            "Loss: 3.693576e-05, l1: 0.99903, l2: 0.03183\n",
            "Loss: 3.766962e-05, l1: 0.99961, l2: 0.03192\n",
            "Loss: 3.692332e-05, l1: 0.99909, l2: 0.03184\n",
            "Loss: 3.688319e-05, l1: 0.99916, l2: 0.03185\n",
            "Loss: 3.685030e-05, l1: 0.99916, l2: 0.03186\n",
            "Loss: 3.682200e-05, l1: 0.99913, l2: 0.03186\n",
            "Loss: 3.679745e-05, l1: 0.99907, l2: 0.03186\n",
            "Loss: 3.676840e-05, l1: 0.99898, l2: 0.03185\n",
            "Loss: 3.673948e-05, l1: 0.99894, l2: 0.03185\n",
            "Loss: 3.668778e-05, l1: 0.99890, l2: 0.03185\n",
            "Loss: 3.666487e-05, l1: 0.99890, l2: 0.03185\n",
            "Loss: 3.664166e-05, l1: 0.99889, l2: 0.03185\n",
            "Loss: 3.661578e-05, l1: 0.99895, l2: 0.03184\n",
            "Loss: 3.668375e-05, l1: 0.99886, l2: 0.03184\n",
            "Loss: 3.660470e-05, l1: 0.99893, l2: 0.03184\n",
            "Loss: 3.659554e-05, l1: 0.99893, l2: 0.03184\n",
            "Loss: 3.658699e-05, l1: 0.99896, l2: 0.03184\n",
            "Loss: 3.657955e-05, l1: 0.99894, l2: 0.03184\n",
            "Loss: 3.655476e-05, l1: 0.99890, l2: 0.03183\n",
            "Loss: 3.654054e-05, l1: 0.99882, l2: 0.03184\n",
            "Loss: 3.653022e-05, l1: 0.99878, l2: 0.03184\n",
            "Loss: 3.651355e-05, l1: 0.99878, l2: 0.03183\n",
            "Loss: 3.649192e-05, l1: 0.99859, l2: 0.03183\n",
            "Loss: 3.646966e-05, l1: 0.99865, l2: 0.03183\n",
            "Loss: 3.642407e-05, l1: 0.99871, l2: 0.03182\n",
            "Loss: 3.639407e-05, l1: 0.99868, l2: 0.03182\n",
            "Loss: 3.636871e-05, l1: 0.99851, l2: 0.03182\n",
            "Loss: 3.632260e-05, l1: 0.99845, l2: 0.03183\n",
            "Loss: 3.630386e-05, l1: 0.99845, l2: 0.03183\n",
            "Loss: 3.628770e-05, l1: 0.99838, l2: 0.03183\n",
            "Loss: 3.627497e-05, l1: 0.99834, l2: 0.03184\n",
            "Loss: 3.624977e-05, l1: 0.99825, l2: 0.03184\n",
            "Loss: 3.632580e-05, l1: 0.99811, l2: 0.03183\n",
            "Loss: 3.623685e-05, l1: 0.99821, l2: 0.03184\n",
            "Loss: 3.619614e-05, l1: 0.99806, l2: 0.03185\n",
            "Loss: 3.615180e-05, l1: 0.99797, l2: 0.03186\n",
            "Loss: 3.613601e-05, l1: 0.99751, l2: 0.03188\n",
            "Loss: 3.609567e-05, l1: 0.99768, l2: 0.03187\n",
            "Loss: 3.608037e-05, l1: 0.99774, l2: 0.03187\n",
            "Loss: 3.606260e-05, l1: 0.99772, l2: 0.03187\n",
            "Loss: 3.604463e-05, l1: 0.99772, l2: 0.03188\n",
            "Loss: 3.602657e-05, l1: 0.99764, l2: 0.03188\n",
            "Loss: 3.598341e-05, l1: 0.99749, l2: 0.03189\n",
            "Loss: 3.593537e-05, l1: 0.99745, l2: 0.03191\n",
            "Loss: 3.588082e-05, l1: 0.99738, l2: 0.03191\n",
            "Loss: 3.582872e-05, l1: 0.99747, l2: 0.03190\n",
            "Loss: 3.579373e-05, l1: 0.99759, l2: 0.03189\n",
            "Loss: 3.575453e-05, l1: 0.99762, l2: 0.03189\n",
            "Loss: 3.573430e-05, l1: 0.99764, l2: 0.03189\n",
            "Loss: 3.570155e-05, l1: 0.99774, l2: 0.03189\n",
            "Loss: 3.567600e-05, l1: 0.99786, l2: 0.03189\n",
            "Loss: 3.565706e-05, l1: 0.99792, l2: 0.03189\n",
            "Loss: 3.562630e-05, l1: 0.99795, l2: 0.03189\n",
            "Loss: 3.559341e-05, l1: 0.99791, l2: 0.03190\n",
            "Loss: 3.556894e-05, l1: 0.99787, l2: 0.03190\n",
            "Loss: 3.553394e-05, l1: 0.99784, l2: 0.03190\n",
            "Loss: 3.549419e-05, l1: 0.99776, l2: 0.03190\n",
            "Loss: 3.547315e-05, l1: 0.99778, l2: 0.03190\n",
            "Loss: 3.545724e-05, l1: 0.99778, l2: 0.03189\n",
            "Loss: 3.543235e-05, l1: 0.99781, l2: 0.03189\n",
            "Loss: 3.539212e-05, l1: 0.99781, l2: 0.03188\n",
            "Loss: 3.534204e-05, l1: 0.99784, l2: 0.03187\n",
            "Loss: 3.531265e-05, l1: 0.99784, l2: 0.03187\n",
            "Loss: 3.527409e-05, l1: 0.99789, l2: 0.03187\n",
            "Loss: 3.525006e-05, l1: 0.99792, l2: 0.03186\n",
            "Loss: 3.522951e-05, l1: 0.99796, l2: 0.03186\n",
            "Loss: 3.521052e-05, l1: 0.99813, l2: 0.03184\n",
            "Loss: 3.518666e-05, l1: 0.99826, l2: 0.03182\n",
            "Loss: 3.516851e-05, l1: 0.99818, l2: 0.03183\n",
            "Loss: 3.515968e-05, l1: 0.99817, l2: 0.03182\n",
            "Loss: 3.515292e-05, l1: 0.99819, l2: 0.03182\n",
            "Loss: 3.514445e-05, l1: 0.99821, l2: 0.03181\n",
            "Loss: 3.512138e-05, l1: 0.99825, l2: 0.03180\n",
            "Loss: 3.509470e-05, l1: 0.99831, l2: 0.03179\n",
            "Loss: 3.507339e-05, l1: 0.99832, l2: 0.03178\n",
            "Loss: 3.504421e-05, l1: 0.99826, l2: 0.03178\n",
            "Loss: 3.501192e-05, l1: 0.99815, l2: 0.03178\n",
            "Loss: 3.499448e-05, l1: 0.99804, l2: 0.03177\n",
            "Loss: 3.496346e-05, l1: 0.99796, l2: 0.03177\n",
            "Loss: 3.494729e-05, l1: 0.99795, l2: 0.03178\n",
            "Loss: 3.496037e-05, l1: 0.99799, l2: 0.03178\n",
            "Loss: 3.492843e-05, l1: 0.99797, l2: 0.03178\n",
            "Loss: 3.490897e-05, l1: 0.99800, l2: 0.03178\n",
            "Loss: 3.488973e-05, l1: 0.99808, l2: 0.03178\n",
            "Loss: 3.487762e-05, l1: 0.99813, l2: 0.03178\n",
            "Loss: 3.484476e-05, l1: 0.99829, l2: 0.03178\n",
            "Loss: 3.480606e-05, l1: 0.99856, l2: 0.03178\n",
            "Loss: 3.477776e-05, l1: 0.99869, l2: 0.03180\n",
            "Loss: 3.475414e-05, l1: 0.99868, l2: 0.03180\n",
            "Loss: 3.474252e-05, l1: 0.99863, l2: 0.03180\n",
            "Loss: 3.472237e-05, l1: 0.99862, l2: 0.03180\n",
            "Loss: 3.474703e-05, l1: 0.99845, l2: 0.03180\n",
            "Loss: 3.469811e-05, l1: 0.99855, l2: 0.03180\n",
            "Loss: 3.465752e-05, l1: 0.99852, l2: 0.03180\n",
            "Loss: 3.462914e-05, l1: 0.99856, l2: 0.03181\n",
            "Loss: 3.459097e-05, l1: 0.99856, l2: 0.03180\n",
            "Loss: 3.455949e-05, l1: 0.99860, l2: 0.03180\n",
            "Loss: 3.451989e-05, l1: 0.99870, l2: 0.03180\n",
            "Loss: 3.451386e-05, l1: 0.99877, l2: 0.03180\n",
            "Loss: 3.447150e-05, l1: 0.99886, l2: 0.03179\n",
            "Loss: 3.445281e-05, l1: 0.99894, l2: 0.03179\n",
            "Loss: 3.443679e-05, l1: 0.99912, l2: 0.03179\n",
            "Loss: 3.445054e-05, l1: 0.99927, l2: 0.03177\n",
            "Loss: 3.442781e-05, l1: 0.99917, l2: 0.03178\n",
            "Loss: 3.441758e-05, l1: 0.99920, l2: 0.03178\n",
            "Loss: 3.438426e-05, l1: 0.99932, l2: 0.03178\n",
            "Loss: 3.436188e-05, l1: 0.99936, l2: 0.03179\n",
            "Loss: 3.433899e-05, l1: 0.99947, l2: 0.03179\n",
            "Loss: 3.432289e-05, l1: 0.99947, l2: 0.03180\n",
            "Loss: 3.430679e-05, l1: 0.99947, l2: 0.03180\n",
            "Loss: 3.428716e-05, l1: 0.99954, l2: 0.03180\n",
            "Loss: 3.426987e-05, l1: 0.99954, l2: 0.03180\n",
            "Loss: 3.430283e-05, l1: 0.99977, l2: 0.03181\n",
            "Loss: 3.426149e-05, l1: 0.99961, l2: 0.03180\n",
            "Loss: 3.424812e-05, l1: 0.99956, l2: 0.03180\n",
            "Loss: 3.423795e-05, l1: 0.99954, l2: 0.03180\n",
            "Loss: 3.422606e-05, l1: 0.99953, l2: 0.03180\n",
            "Loss: 3.420561e-05, l1: 0.99959, l2: 0.03180\n",
            "Loss: 3.418831e-05, l1: 0.99960, l2: 0.03180\n",
            "Loss: 3.417331e-05, l1: 0.99959, l2: 0.03180\n",
            "Loss: 3.415601e-05, l1: 0.99958, l2: 0.03180\n",
            "Loss: 3.413926e-05, l1: 0.99960, l2: 0.03180\n",
            "Loss: 3.427017e-05, l1: 0.99927, l2: 0.03179\n",
            "Loss: 3.413397e-05, l1: 0.99955, l2: 0.03179\n",
            "Loss: 3.411470e-05, l1: 0.99955, l2: 0.03178\n",
            "Loss: 3.410826e-05, l1: 0.99961, l2: 0.03178\n",
            "Loss: 3.410113e-05, l1: 0.99958, l2: 0.03178\n",
            "Loss: 3.409571e-05, l1: 0.99956, l2: 0.03178\n",
            "Loss: 3.408953e-05, l1: 0.99955, l2: 0.03177\n",
            "Loss: 3.407892e-05, l1: 0.99953, l2: 0.03177\n",
            "Loss: 3.407997e-05, l1: 0.99943, l2: 0.03177\n",
            "Loss: 3.407214e-05, l1: 0.99948, l2: 0.03177\n",
            "Loss: 3.406251e-05, l1: 0.99943, l2: 0.03177\n",
            "Loss: 3.405340e-05, l1: 0.99936, l2: 0.03177\n",
            "Loss: 3.404066e-05, l1: 0.99927, l2: 0.03177\n",
            "Loss: 3.403855e-05, l1: 0.99898, l2: 0.03177\n",
            "Loss: 3.402830e-05, l1: 0.99912, l2: 0.03177\n",
            "Loss: 3.401292e-05, l1: 0.99908, l2: 0.03178\n",
            "Loss: 3.398777e-05, l1: 0.99898, l2: 0.03178\n",
            "Loss: 3.398185e-05, l1: 0.99895, l2: 0.03178\n",
            "Loss: 3.397102e-05, l1: 0.99896, l2: 0.03178\n",
            "Loss: 3.395871e-05, l1: 0.99896, l2: 0.03178\n",
            "Loss: 3.394964e-05, l1: 0.99896, l2: 0.03177\n",
            "Loss: 3.393152e-05, l1: 0.99894, l2: 0.03177\n",
            "Loss: 3.392473e-05, l1: 0.99903, l2: 0.03176\n",
            "Loss: 3.389777e-05, l1: 0.99897, l2: 0.03176\n",
            "Loss: 3.388526e-05, l1: 0.99900, l2: 0.03176\n",
            "Loss: 3.387194e-05, l1: 0.99904, l2: 0.03176\n",
            "Loss: 3.385554e-05, l1: 0.99906, l2: 0.03176\n",
            "Loss: 3.383403e-05, l1: 0.99894, l2: 0.03177\n",
            "Loss: 3.382533e-05, l1: 0.99897, l2: 0.03178\n",
            "Loss: 3.380843e-05, l1: 0.99892, l2: 0.03178\n",
            "Loss: 3.380329e-05, l1: 0.99886, l2: 0.03178\n",
            "Loss: 3.379721e-05, l1: 0.99883, l2: 0.03179\n",
            "Loss: 3.378644e-05, l1: 0.99879, l2: 0.03180\n",
            "Loss: 3.379591e-05, l1: 0.99868, l2: 0.03183\n",
            "Loss: 3.377875e-05, l1: 0.99875, l2: 0.03181\n",
            "Loss: 3.376278e-05, l1: 0.99877, l2: 0.03181\n",
            "Loss: 3.373599e-05, l1: 0.99885, l2: 0.03182\n",
            "Loss: 3.372314e-05, l1: 0.99897, l2: 0.03182\n",
            "Loss: 3.370953e-05, l1: 0.99901, l2: 0.03182\n",
            "Loss: 3.369641e-05, l1: 0.99902, l2: 0.03182\n",
            "Loss: 3.367649e-05, l1: 0.99905, l2: 0.03182\n",
            "Loss: 3.366196e-05, l1: 0.99901, l2: 0.03184\n",
            "Loss: 3.364544e-05, l1: 0.99901, l2: 0.03184\n",
            "Loss: 3.361954e-05, l1: 0.99904, l2: 0.03184\n",
            "Loss: 3.360536e-05, l1: 0.99909, l2: 0.03184\n",
            "Loss: 3.358677e-05, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.364313e-05, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.358187e-05, l1: 0.99915, l2: 0.03184\n",
            "Loss: 3.357307e-05, l1: 0.99918, l2: 0.03184\n",
            "Loss: 3.354946e-05, l1: 0.99931, l2: 0.03184\n",
            "Loss: 3.353437e-05, l1: 0.99939, l2: 0.03184\n",
            "Loss: 3.352166e-05, l1: 0.99954, l2: 0.03186\n",
            "Loss: 3.350564e-05, l1: 0.99952, l2: 0.03186\n",
            "Loss: 3.349588e-05, l1: 0.99946, l2: 0.03186\n",
            "Loss: 3.348668e-05, l1: 0.99939, l2: 0.03186\n",
            "Loss: 3.348158e-05, l1: 0.99937, l2: 0.03186\n",
            "Loss: 3.347402e-05, l1: 0.99938, l2: 0.03186\n",
            "Loss: 3.346832e-05, l1: 0.99942, l2: 0.03186\n",
            "Loss: 3.346645e-05, l1: 0.99947, l2: 0.03186\n",
            "Loss: 3.346306e-05, l1: 0.99951, l2: 0.03186\n",
            "Loss: 3.346083e-05, l1: 0.99952, l2: 0.03186\n",
            "Loss: 3.345438e-05, l1: 0.99955, l2: 0.03187\n",
            "Loss: 3.344601e-05, l1: 0.99959, l2: 0.03187\n",
            "Loss: 3.343773e-05, l1: 0.99965, l2: 0.03188\n",
            "Loss: 3.342723e-05, l1: 0.99969, l2: 0.03188\n",
            "Loss: 3.341881e-05, l1: 0.99972, l2: 0.03188\n",
            "Loss: 3.340677e-05, l1: 0.99977, l2: 0.03188\n",
            "Loss: 3.339030e-05, l1: 0.99983, l2: 0.03187\n",
            "Loss: 3.342265e-05, l1: 1.00001, l2: 0.03189\n",
            "Loss: 3.338124e-05, l1: 0.99989, l2: 0.03188\n",
            "Loss: 3.336152e-05, l1: 0.99987, l2: 0.03188\n",
            "Loss: 3.334566e-05, l1: 0.99991, l2: 0.03188\n",
            "Loss: 3.333086e-05, l1: 0.99994, l2: 0.03188\n",
            "Loss: 3.330398e-05, l1: 1.00001, l2: 0.03188\n",
            "Loss: 3.327518e-05, l1: 1.00011, l2: 0.03186\n",
            "Loss: 3.326463e-05, l1: 1.00022, l2: 0.03186\n",
            "Loss: 3.325909e-05, l1: 1.00030, l2: 0.03184\n",
            "Loss: 3.324071e-05, l1: 1.00025, l2: 0.03185\n",
            "Loss: 3.323572e-05, l1: 1.00025, l2: 0.03185\n",
            "Loss: 3.322555e-05, l1: 1.00028, l2: 0.03184\n",
            "Loss: 3.322320e-05, l1: 1.00034, l2: 0.03184\n",
            "Loss: 3.321422e-05, l1: 1.00033, l2: 0.03184\n",
            "Loss: 3.320568e-05, l1: 1.00032, l2: 0.03184\n",
            "Loss: 3.319836e-05, l1: 1.00032, l2: 0.03183\n",
            "Loss: 3.319315e-05, l1: 1.00025, l2: 0.03183\n",
            "Loss: 3.319014e-05, l1: 1.00023, l2: 0.03182\n",
            "Loss: 3.318707e-05, l1: 1.00023, l2: 0.03183\n",
            "Loss: 3.318502e-05, l1: 1.00019, l2: 0.03183\n",
            "Loss: 3.318209e-05, l1: 1.00016, l2: 0.03183\n",
            "Loss: 3.317822e-05, l1: 1.00010, l2: 0.03183\n",
            "Loss: 3.318804e-05, l1: 0.99996, l2: 0.03183\n",
            "Loss: 3.317681e-05, l1: 1.00007, l2: 0.03183\n",
            "Loss: 3.317322e-05, l1: 1.00005, l2: 0.03182\n",
            "Loss: 3.316837e-05, l1: 1.00005, l2: 0.03182\n",
            "Loss: 3.316471e-05, l1: 1.00007, l2: 0.03182\n",
            "Loss: 3.315929e-05, l1: 1.00008, l2: 0.03181\n",
            "Loss: 3.315483e-05, l1: 1.00010, l2: 0.03181\n",
            "Loss: 3.314758e-05, l1: 1.00011, l2: 0.03180\n",
            "Loss: 3.314303e-05, l1: 1.00006, l2: 0.03180\n",
            "Loss: 3.313720e-05, l1: 1.00002, l2: 0.03180\n",
            "Loss: 3.313133e-05, l1: 0.99997, l2: 0.03180\n",
            "Loss: 3.314060e-05, l1: 0.99982, l2: 0.03179\n",
            "Loss: 3.312577e-05, l1: 0.99991, l2: 0.03180\n",
            "Loss: 3.311759e-05, l1: 0.99990, l2: 0.03180\n",
            "Loss: 3.310231e-05, l1: 0.99986, l2: 0.03180\n",
            "Loss: 3.309178e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.308705e-05, l1: 0.99964, l2: 0.03180\n",
            "Loss: 3.307044e-05, l1: 0.99961, l2: 0.03181\n",
            "Loss: 3.306532e-05, l1: 0.99957, l2: 0.03181\n",
            "Loss: 3.306001e-05, l1: 0.99948, l2: 0.03181\n",
            "Loss: 3.305822e-05, l1: 0.99942, l2: 0.03181\n",
            "Loss: 3.305509e-05, l1: 0.99941, l2: 0.03180\n",
            "Loss: 3.305028e-05, l1: 0.99938, l2: 0.03180\n",
            "Loss: 3.304632e-05, l1: 0.99934, l2: 0.03180\n",
            "Loss: 3.305036e-05, l1: 0.99917, l2: 0.03179\n",
            "Loss: 3.304400e-05, l1: 0.99927, l2: 0.03180\n",
            "Loss: 3.304047e-05, l1: 0.99922, l2: 0.03180\n",
            "Loss: 3.303688e-05, l1: 0.99915, l2: 0.03180\n",
            "Loss: 3.303329e-05, l1: 0.99906, l2: 0.03180\n",
            "Loss: 3.302586e-05, l1: 0.99893, l2: 0.03180\n",
            "Loss: 3.302327e-05, l1: 0.99878, l2: 0.03180\n",
            "Loss: 3.301112e-05, l1: 0.99870, l2: 0.03179\n",
            "Loss: 3.300853e-05, l1: 0.99875, l2: 0.03179\n",
            "Loss: 3.300453e-05, l1: 0.99880, l2: 0.03179\n",
            "Loss: 3.300029e-05, l1: 0.99880, l2: 0.03179\n",
            "Loss: 3.299264e-05, l1: 0.99878, l2: 0.03179\n",
            "Loss: 3.301215e-05, l1: 0.99873, l2: 0.03180\n",
            "Loss: 3.299039e-05, l1: 0.99877, l2: 0.03180\n",
            "Loss: 3.298292e-05, l1: 0.99867, l2: 0.03180\n",
            "Loss: 3.297566e-05, l1: 0.99860, l2: 0.03180\n",
            "Loss: 3.296712e-05, l1: 0.99849, l2: 0.03180\n",
            "Loss: 3.295958e-05, l1: 0.99842, l2: 0.03180\n",
            "Loss: 3.295514e-05, l1: 0.99828, l2: 0.03179\n",
            "Loss: 3.294711e-05, l1: 0.99834, l2: 0.03180\n",
            "Loss: 3.294022e-05, l1: 0.99838, l2: 0.03180\n",
            "Loss: 3.293317e-05, l1: 0.99838, l2: 0.03180\n",
            "Loss: 3.292530e-05, l1: 0.99834, l2: 0.03180\n",
            "Loss: 3.291772e-05, l1: 0.99831, l2: 0.03179\n",
            "Loss: 3.291104e-05, l1: 0.99828, l2: 0.03179\n",
            "Loss: 3.290278e-05, l1: 0.99823, l2: 0.03179\n",
            "Loss: 3.289714e-05, l1: 0.99825, l2: 0.03179\n",
            "Loss: 3.288829e-05, l1: 0.99832, l2: 0.03179\n",
            "Loss: 3.288741e-05, l1: 0.99841, l2: 0.03180\n",
            "Loss: 3.287568e-05, l1: 0.99841, l2: 0.03179\n",
            "Loss: 3.287145e-05, l1: 0.99838, l2: 0.03179\n",
            "Loss: 3.286340e-05, l1: 0.99831, l2: 0.03179\n",
            "Loss: 3.285759e-05, l1: 0.99830, l2: 0.03180\n",
            "Loss: 3.284522e-05, l1: 0.99835, l2: 0.03180\n",
            "Loss: 3.283668e-05, l1: 0.99847, l2: 0.03181\n",
            "Loss: 3.283171e-05, l1: 0.99854, l2: 0.03181\n",
            "Loss: 3.282576e-05, l1: 0.99855, l2: 0.03181\n",
            "Loss: 3.282096e-05, l1: 0.99854, l2: 0.03181\n",
            "Loss: 3.281480e-05, l1: 0.99854, l2: 0.03181\n",
            "Loss: 3.280940e-05, l1: 0.99853, l2: 0.03181\n",
            "Loss: 3.280138e-05, l1: 0.99850, l2: 0.03181\n",
            "Loss: 3.279651e-05, l1: 0.99851, l2: 0.03181\n",
            "Loss: 3.278723e-05, l1: 0.99853, l2: 0.03181\n",
            "Loss: 3.277994e-05, l1: 0.99858, l2: 0.03181\n",
            "Loss: 3.276972e-05, l1: 0.99870, l2: 0.03181\n",
            "Loss: 3.275888e-05, l1: 0.99877, l2: 0.03182\n",
            "Loss: 3.274529e-05, l1: 0.99891, l2: 0.03182\n",
            "Loss: 3.274197e-05, l1: 0.99896, l2: 0.03182\n",
            "Loss: 3.273622e-05, l1: 0.99901, l2: 0.03182\n",
            "Loss: 3.273014e-05, l1: 0.99904, l2: 0.03182\n",
            "Loss: 3.272542e-05, l1: 0.99908, l2: 0.03182\n",
            "Loss: 3.272369e-05, l1: 0.99911, l2: 0.03182\n",
            "Loss: 3.272071e-05, l1: 0.99911, l2: 0.03182\n",
            "Loss: 3.271859e-05, l1: 0.99912, l2: 0.03181\n",
            "Loss: 3.271670e-05, l1: 0.99913, l2: 0.03181\n",
            "Loss: 3.273490e-05, l1: 0.99929, l2: 0.03182\n",
            "Loss: 3.271608e-05, l1: 0.99916, l2: 0.03181\n",
            "Loss: 3.271287e-05, l1: 0.99917, l2: 0.03181\n",
            "Loss: 3.270880e-05, l1: 0.99918, l2: 0.03181\n",
            "Loss: 3.270547e-05, l1: 0.99920, l2: 0.03181\n",
            "Loss: 3.270225e-05, l1: 0.99923, l2: 0.03182\n",
            "Loss: 3.269747e-05, l1: 0.99930, l2: 0.03182\n",
            "Loss: 3.269779e-05, l1: 0.99942, l2: 0.03183\n",
            "Loss: 3.269369e-05, l1: 0.99936, l2: 0.03183\n",
            "Loss: 3.268533e-05, l1: 0.99953, l2: 0.03183\n",
            "Loss: 3.268069e-05, l1: 0.99958, l2: 0.03183\n",
            "Loss: 3.267639e-05, l1: 0.99956, l2: 0.03183\n",
            "Loss: 3.267283e-05, l1: 0.99955, l2: 0.03182\n",
            "Loss: 3.266968e-05, l1: 0.99955, l2: 0.03182\n",
            "Loss: 3.266423e-05, l1: 0.99956, l2: 0.03182\n",
            "Loss: 3.265884e-05, l1: 0.99958, l2: 0.03182\n",
            "Loss: 3.265479e-05, l1: 0.99959, l2: 0.03182\n",
            "Loss: 3.264809e-05, l1: 0.99958, l2: 0.03182\n",
            "Loss: 3.263150e-05, l1: 0.99956, l2: 0.03181\n",
            "Loss: 3.261770e-05, l1: 0.99955, l2: 0.03180\n",
            "Loss: 3.260472e-05, l1: 0.99954, l2: 0.03179\n",
            "Loss: 3.259863e-05, l1: 0.99956, l2: 0.03179\n",
            "Loss: 3.259290e-05, l1: 0.99958, l2: 0.03179\n",
            "Loss: 3.258952e-05, l1: 0.99962, l2: 0.03179\n",
            "Loss: 3.258223e-05, l1: 0.99964, l2: 0.03179\n",
            "Loss: 3.257399e-05, l1: 0.99965, l2: 0.03179\n",
            "Loss: 3.256937e-05, l1: 0.99964, l2: 0.03180\n",
            "Loss: 3.256773e-05, l1: 0.99962, l2: 0.03180\n",
            "Loss: 3.256277e-05, l1: 0.99959, l2: 0.03180\n",
            "Loss: 3.255965e-05, l1: 0.99955, l2: 0.03180\n",
            "Loss: 3.255437e-05, l1: 0.99947, l2: 0.03180\n",
            "Loss: 3.254742e-05, l1: 0.99935, l2: 0.03181\n",
            "Loss: 3.253815e-05, l1: 0.99911, l2: 0.03180\n",
            "Loss: 3.252842e-05, l1: 0.99899, l2: 0.03180\n",
            "Loss: 3.252233e-05, l1: 0.99897, l2: 0.03180\n",
            "Loss: 3.251809e-05, l1: 0.99896, l2: 0.03180\n",
            "Loss: 3.251198e-05, l1: 0.99898, l2: 0.03180\n",
            "Loss: 3.250137e-05, l1: 0.99895, l2: 0.03180\n",
            "Loss: 3.248736e-05, l1: 0.99891, l2: 0.03180\n",
            "Loss: 3.248156e-05, l1: 0.99886, l2: 0.03180\n",
            "Loss: 3.247402e-05, l1: 0.99889, l2: 0.03180\n",
            "Loss: 3.246501e-05, l1: 0.99894, l2: 0.03180\n",
            "Loss: 3.245654e-05, l1: 0.99898, l2: 0.03180\n",
            "Loss: 3.245077e-05, l1: 0.99899, l2: 0.03179\n",
            "Loss: 3.244178e-05, l1: 0.99890, l2: 0.03179\n",
            "Loss: 3.243365e-05, l1: 0.99884, l2: 0.03178\n",
            "Loss: 3.242761e-05, l1: 0.99878, l2: 0.03178\n",
            "Loss: 3.242098e-05, l1: 0.99874, l2: 0.03178\n",
            "Loss: 3.242681e-05, l1: 0.99880, l2: 0.03179\n",
            "Loss: 3.241857e-05, l1: 0.99876, l2: 0.03178\n",
            "Loss: 3.241426e-05, l1: 0.99878, l2: 0.03179\n",
            "Loss: 3.240987e-05, l1: 0.99883, l2: 0.03179\n",
            "Loss: 3.240934e-05, l1: 0.99891, l2: 0.03179\n",
            "Loss: 3.240340e-05, l1: 0.99891, l2: 0.03179\n",
            "Loss: 3.239987e-05, l1: 0.99893, l2: 0.03179\n",
            "Loss: 3.239626e-05, l1: 0.99894, l2: 0.03179\n",
            "Loss: 3.240190e-05, l1: 0.99906, l2: 0.03180\n",
            "Loss: 3.239516e-05, l1: 0.99897, l2: 0.03179\n",
            "Loss: 3.239229e-05, l1: 0.99898, l2: 0.03179\n",
            "Loss: 3.238913e-05, l1: 0.99899, l2: 0.03180\n",
            "Loss: 3.238701e-05, l1: 0.99900, l2: 0.03180\n",
            "Loss: 3.238450e-05, l1: 0.99902, l2: 0.03180\n",
            "Loss: 3.237992e-05, l1: 0.99907, l2: 0.03180\n",
            "Loss: 3.237448e-05, l1: 0.99911, l2: 0.03180\n",
            "Loss: 3.237041e-05, l1: 0.99916, l2: 0.03180\n",
            "Loss: 3.236624e-05, l1: 0.99917, l2: 0.03179\n",
            "Loss: 3.236079e-05, l1: 0.99925, l2: 0.03179\n",
            "Loss: 3.235440e-05, l1: 0.99927, l2: 0.03178\n",
            "Loss: 3.234819e-05, l1: 0.99930, l2: 0.03178\n",
            "Loss: 3.234615e-05, l1: 0.99933, l2: 0.03178\n",
            "Loss: 3.234419e-05, l1: 0.99934, l2: 0.03178\n",
            "Loss: 3.234197e-05, l1: 0.99935, l2: 0.03178\n",
            "Loss: 3.233975e-05, l1: 0.99935, l2: 0.03178\n",
            "Loss: 3.233705e-05, l1: 0.99935, l2: 0.03178\n",
            "Loss: 3.233246e-05, l1: 0.99936, l2: 0.03178\n",
            "Loss: 3.232864e-05, l1: 0.99944, l2: 0.03178\n",
            "Loss: 3.232782e-05, l1: 0.99952, l2: 0.03178\n",
            "Loss: 3.232245e-05, l1: 0.99950, l2: 0.03178\n",
            "Loss: 3.232019e-05, l1: 0.99951, l2: 0.03178\n",
            "Loss: 3.231595e-05, l1: 0.99955, l2: 0.03179\n",
            "Loss: 3.231156e-05, l1: 0.99961, l2: 0.03179\n",
            "Loss: 3.230993e-05, l1: 0.99974, l2: 0.03179\n",
            "Loss: 3.230477e-05, l1: 0.99974, l2: 0.03179\n",
            "Loss: 3.230287e-05, l1: 0.99974, l2: 0.03179\n",
            "Loss: 3.230123e-05, l1: 0.99976, l2: 0.03179\n",
            "Loss: 3.229783e-05, l1: 0.99977, l2: 0.03179\n",
            "Loss: 3.229235e-05, l1: 0.99980, l2: 0.03179\n",
            "Loss: 3.228890e-05, l1: 0.99982, l2: 0.03179\n",
            "Loss: 3.228548e-05, l1: 0.99979, l2: 0.03179\n",
            "Loss: 3.228061e-05, l1: 0.99978, l2: 0.03180\n",
            "Loss: 3.227528e-05, l1: 0.99978, l2: 0.03180\n",
            "Loss: 3.226976e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.226296e-05, l1: 0.99984, l2: 0.03180\n",
            "Loss: 3.225645e-05, l1: 0.99984, l2: 0.03180\n",
            "Loss: 3.226104e-05, l1: 0.99994, l2: 0.03181\n",
            "Loss: 3.225217e-05, l1: 0.99988, l2: 0.03181\n",
            "Loss: 3.224649e-05, l1: 0.99984, l2: 0.03181\n",
            "Loss: 3.224264e-05, l1: 0.99979, l2: 0.03181\n",
            "Loss: 3.223888e-05, l1: 0.99975, l2: 0.03181\n",
            "Loss: 3.223141e-05, l1: 0.99970, l2: 0.03182\n",
            "Loss: 3.222101e-05, l1: 0.99964, l2: 0.03182\n",
            "Loss: 3.221054e-05, l1: 0.99961, l2: 0.03182\n",
            "Loss: 3.220108e-05, l1: 0.99959, l2: 0.03182\n",
            "Loss: 3.219355e-05, l1: 0.99965, l2: 0.03182\n",
            "Loss: 3.218604e-05, l1: 0.99963, l2: 0.03182\n",
            "Loss: 3.217824e-05, l1: 0.99958, l2: 0.03182\n",
            "Loss: 3.217348e-05, l1: 0.99952, l2: 0.03182\n",
            "Loss: 3.216958e-05, l1: 0.99944, l2: 0.03182\n",
            "Loss: 3.216734e-05, l1: 0.99937, l2: 0.03182\n",
            "Loss: 3.216328e-05, l1: 0.99939, l2: 0.03182\n",
            "Loss: 3.216041e-05, l1: 0.99944, l2: 0.03182\n",
            "Loss: 3.215652e-05, l1: 0.99942, l2: 0.03182\n",
            "Loss: 3.215146e-05, l1: 0.99940, l2: 0.03182\n",
            "Loss: 3.214367e-05, l1: 0.99937, l2: 0.03182\n",
            "Loss: 3.213068e-05, l1: 0.99932, l2: 0.03182\n",
            "Loss: 3.211362e-05, l1: 0.99926, l2: 0.03182\n",
            "Loss: 3.210249e-05, l1: 0.99924, l2: 0.03182\n",
            "Loss: 3.209153e-05, l1: 0.99924, l2: 0.03182\n",
            "Loss: 3.208359e-05, l1: 0.99927, l2: 0.03182\n",
            "Loss: 3.207223e-05, l1: 0.99930, l2: 0.03183\n",
            "Loss: 3.208525e-05, l1: 0.99942, l2: 0.03182\n",
            "Loss: 3.206795e-05, l1: 0.99934, l2: 0.03182\n",
            "Loss: 3.205839e-05, l1: 0.99936, l2: 0.03182\n",
            "Loss: 3.205183e-05, l1: 0.99937, l2: 0.03182\n",
            "Loss: 3.204387e-05, l1: 0.99939, l2: 0.03182\n",
            "Loss: 3.203622e-05, l1: 0.99939, l2: 0.03182\n",
            "Loss: 3.202420e-05, l1: 0.99943, l2: 0.03182\n",
            "Loss: 3.201719e-05, l1: 0.99947, l2: 0.03182\n",
            "Loss: 3.200453e-05, l1: 0.99955, l2: 0.03182\n",
            "Loss: 3.198499e-05, l1: 0.99964, l2: 0.03181\n",
            "Loss: 3.198523e-05, l1: 0.99988, l2: 0.03181\n",
            "Loss: 3.197178e-05, l1: 0.99976, l2: 0.03181\n",
            "Loss: 3.195705e-05, l1: 0.99979, l2: 0.03181\n",
            "Loss: 3.194793e-05, l1: 0.99981, l2: 0.03181\n",
            "Loss: 3.193231e-05, l1: 0.99989, l2: 0.03181\n",
            "Loss: 3.191950e-05, l1: 1.00012, l2: 0.03181\n",
            "Loss: 3.190729e-05, l1: 1.00015, l2: 0.03181\n",
            "Loss: 3.189957e-05, l1: 1.00024, l2: 0.03181\n",
            "Loss: 3.189456e-05, l1: 1.00038, l2: 0.03182\n",
            "Loss: 3.188508e-05, l1: 1.00046, l2: 0.03182\n",
            "Loss: 3.187961e-05, l1: 1.00049, l2: 0.03182\n",
            "Loss: 3.187376e-05, l1: 1.00049, l2: 0.03181\n",
            "Loss: 3.186684e-05, l1: 1.00046, l2: 0.03181\n",
            "Loss: 3.186212e-05, l1: 1.00041, l2: 0.03181\n",
            "Loss: 3.185485e-05, l1: 1.00035, l2: 0.03181\n",
            "Loss: 3.185332e-05, l1: 1.00036, l2: 0.03181\n",
            "Loss: 3.185099e-05, l1: 1.00035, l2: 0.03181\n",
            "Loss: 3.184859e-05, l1: 1.00032, l2: 0.03181\n",
            "Loss: 3.184424e-05, l1: 1.00029, l2: 0.03180\n",
            "Loss: 3.183900e-05, l1: 1.00022, l2: 0.03180\n",
            "Loss: 3.183678e-05, l1: 1.00010, l2: 0.03180\n",
            "Loss: 3.183137e-05, l1: 1.00015, l2: 0.03180\n",
            "Loss: 3.182917e-05, l1: 1.00018, l2: 0.03181\n",
            "Loss: 3.182622e-05, l1: 1.00018, l2: 0.03181\n",
            "Loss: 3.182809e-05, l1: 1.00034, l2: 0.03182\n",
            "Loss: 3.182479e-05, l1: 1.00025, l2: 0.03181\n",
            "Loss: 3.182258e-05, l1: 1.00024, l2: 0.03182\n",
            "Loss: 3.181945e-05, l1: 1.00025, l2: 0.03182\n",
            "Loss: 3.181721e-05, l1: 1.00024, l2: 0.03182\n",
            "Loss: 3.181432e-05, l1: 1.00029, l2: 0.03182\n",
            "Loss: 3.181243e-05, l1: 1.00030, l2: 0.03182\n",
            "Loss: 3.180916e-05, l1: 1.00031, l2: 0.03182\n",
            "Loss: 3.180795e-05, l1: 1.00029, l2: 0.03182\n",
            "Loss: 3.180657e-05, l1: 1.00029, l2: 0.03182\n",
            "Loss: 3.180300e-05, l1: 1.00026, l2: 0.03182\n",
            "Loss: 3.182044e-05, l1: 1.00032, l2: 0.03182\n",
            "Loss: 3.180206e-05, l1: 1.00027, l2: 0.03182\n",
            "Loss: 3.180109e-05, l1: 1.00025, l2: 0.03182\n",
            "Loss: 3.179898e-05, l1: 1.00019, l2: 0.03182\n",
            "Loss: 3.179767e-05, l1: 1.00016, l2: 0.03182\n",
            "Loss: 3.179512e-05, l1: 1.00012, l2: 0.03182\n",
            "Loss: 3.178925e-05, l1: 1.00003, l2: 0.03182\n",
            "Loss: 3.178456e-05, l1: 0.99997, l2: 0.03182\n",
            "Loss: 3.181360e-05, l1: 0.99984, l2: 0.03182\n",
            "Loss: 3.178334e-05, l1: 0.99995, l2: 0.03182\n",
            "Loss: 3.178034e-05, l1: 0.99992, l2: 0.03182\n",
            "Loss: 3.177740e-05, l1: 0.99990, l2: 0.03182\n",
            "Loss: 3.177447e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.177142e-05, l1: 0.99988, l2: 0.03181\n",
            "Loss: 3.176857e-05, l1: 0.99989, l2: 0.03181\n",
            "Loss: 3.176527e-05, l1: 0.99986, l2: 0.03181\n",
            "Loss: 3.176280e-05, l1: 0.99984, l2: 0.03181\n",
            "Loss: 3.176010e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.175778e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.175581e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.175309e-05, l1: 0.99981, l2: 0.03180\n",
            "Loss: 3.175121e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.174852e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.174441e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.173880e-05, l1: 0.99981, l2: 0.03180\n",
            "Loss: 3.173414e-05, l1: 0.99977, l2: 0.03180\n",
            "Loss: 3.173342e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.172948e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.172820e-05, l1: 0.99978, l2: 0.03180\n",
            "Loss: 3.172639e-05, l1: 0.99976, l2: 0.03180\n",
            "Loss: 3.172421e-05, l1: 0.99974, l2: 0.03180\n",
            "Loss: 3.176083e-05, l1: 0.99958, l2: 0.03180\n",
            "Loss: 3.172351e-05, l1: 0.99972, l2: 0.03180\n",
            "Loss: 3.172017e-05, l1: 0.99971, l2: 0.03180\n",
            "Loss: 3.171655e-05, l1: 0.99975, l2: 0.03180\n",
            "Loss: 3.171404e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.171018e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.170741e-05, l1: 0.99984, l2: 0.03180\n",
            "Loss: 3.170174e-05, l1: 0.99985, l2: 0.03180\n",
            "Loss: 3.169482e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.171190e-05, l1: 0.99995, l2: 0.03180\n",
            "Loss: 3.169282e-05, l1: 0.99985, l2: 0.03180\n",
            "Loss: 3.168921e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.168670e-05, l1: 0.99980, l2: 0.03179\n",
            "Loss: 3.168407e-05, l1: 0.99978, l2: 0.03179\n",
            "Loss: 3.167954e-05, l1: 0.99978, l2: 0.03179\n",
            "Loss: 3.167278e-05, l1: 0.99979, l2: 0.03179\n",
            "Loss: 3.168093e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.166984e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.166449e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.166155e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.165963e-05, l1: 0.99981, l2: 0.03180\n",
            "Loss: 3.165739e-05, l1: 0.99977, l2: 0.03180\n",
            "Loss: 3.166388e-05, l1: 0.99975, l2: 0.03181\n",
            "Loss: 3.165649e-05, l1: 0.99976, l2: 0.03181\n",
            "Loss: 3.165398e-05, l1: 0.99973, l2: 0.03181\n",
            "Loss: 3.165153e-05, l1: 0.99968, l2: 0.03180\n",
            "Loss: 3.165018e-05, l1: 0.99968, l2: 0.03180\n",
            "Loss: 3.164875e-05, l1: 0.99966, l2: 0.03180\n",
            "Loss: 3.164764e-05, l1: 0.99968, l2: 0.03180\n",
            "Loss: 3.164420e-05, l1: 0.99960, l2: 0.03180\n",
            "Loss: 3.164291e-05, l1: 0.99962, l2: 0.03180\n",
            "Loss: 3.164193e-05, l1: 0.99965, l2: 0.03181\n",
            "Loss: 3.164092e-05, l1: 0.99967, l2: 0.03181\n",
            "Loss: 3.163775e-05, l1: 0.99975, l2: 0.03181\n",
            "Loss: 3.163561e-05, l1: 0.99986, l2: 0.03181\n",
            "Loss: 3.163240e-05, l1: 0.99987, l2: 0.03181\n",
            "Loss: 3.162960e-05, l1: 0.99985, l2: 0.03181\n",
            "Loss: 3.162496e-05, l1: 0.99982, l2: 0.03181\n",
            "Loss: 3.162137e-05, l1: 0.99978, l2: 0.03181\n",
            "Loss: 3.161695e-05, l1: 0.99977, l2: 0.03181\n",
            "Loss: 3.161273e-05, l1: 0.99970, l2: 0.03181\n",
            "Loss: 3.160638e-05, l1: 0.99973, l2: 0.03181\n",
            "Loss: 3.160170e-05, l1: 0.99977, l2: 0.03181\n",
            "Loss: 3.159275e-05, l1: 0.99980, l2: 0.03182\n",
            "Loss: 3.158664e-05, l1: 0.99979, l2: 0.03182\n",
            "Loss: 3.157899e-05, l1: 0.99976, l2: 0.03182\n",
            "Loss: 3.177915e-05, l1: 0.99936, l2: 0.03181\n",
            "Loss: 3.157748e-05, l1: 0.99973, l2: 0.03182\n",
            "Loss: 3.157224e-05, l1: 0.99968, l2: 0.03182\n",
            "Loss: 3.156496e-05, l1: 0.99962, l2: 0.03182\n",
            "Loss: 3.156914e-05, l1: 0.99960, l2: 0.03181\n",
            "Loss: 3.155947e-05, l1: 0.99961, l2: 0.03182\n",
            "Loss: 3.155125e-05, l1: 0.99956, l2: 0.03181\n",
            "Loss: 3.154543e-05, l1: 0.99956, l2: 0.03181\n",
            "Loss: 3.157823e-05, l1: 0.99960, l2: 0.03181\n",
            "Loss: 3.154414e-05, l1: 0.99957, l2: 0.03181\n",
            "Loss: 3.154085e-05, l1: 0.99959, l2: 0.03181\n",
            "Loss: 3.153713e-05, l1: 0.99959, l2: 0.03181\n",
            "Loss: 3.153301e-05, l1: 0.99958, l2: 0.03181\n",
            "Loss: 3.153144e-05, l1: 0.99961, l2: 0.03181\n",
            "Loss: 3.152971e-05, l1: 0.99961, l2: 0.03181\n",
            "Loss: 3.152770e-05, l1: 0.99963, l2: 0.03181\n",
            "Loss: 3.152577e-05, l1: 0.99967, l2: 0.03181\n",
            "Loss: 3.152341e-05, l1: 0.99986, l2: 0.03182\n",
            "Loss: 3.151785e-05, l1: 0.99992, l2: 0.03182\n",
            "Loss: 3.151450e-05, l1: 0.99994, l2: 0.03181\n",
            "Loss: 3.151065e-05, l1: 1.00000, l2: 0.03181\n",
            "Loss: 3.150799e-05, l1: 1.00003, l2: 0.03181\n",
            "Loss: 3.150247e-05, l1: 1.00006, l2: 0.03181\n",
            "Loss: 3.149941e-05, l1: 1.00003, l2: 0.03181\n",
            "Loss: 3.149637e-05, l1: 0.99999, l2: 0.03181\n",
            "Loss: 3.149203e-05, l1: 0.99993, l2: 0.03181\n",
            "Loss: 3.148823e-05, l1: 0.99991, l2: 0.03181\n",
            "Loss: 3.148318e-05, l1: 0.99989, l2: 0.03181\n",
            "Loss: 3.147753e-05, l1: 0.99988, l2: 0.03181\n",
            "Loss: 3.148239e-05, l1: 0.99992, l2: 0.03181\n",
            "Loss: 3.147524e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.146972e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.146461e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.146091e-05, l1: 0.99989, l2: 0.03181\n",
            "Loss: 3.145831e-05, l1: 0.99989, l2: 0.03181\n",
            "Loss: 3.145761e-05, l1: 0.99984, l2: 0.03181\n",
            "Loss: 3.145253e-05, l1: 0.99993, l2: 0.03181\n",
            "Loss: 3.144921e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.144434e-05, l1: 0.99983, l2: 0.03181\n",
            "Loss: 3.144126e-05, l1: 0.99981, l2: 0.03181\n",
            "Loss: 3.143524e-05, l1: 0.99977, l2: 0.03180\n",
            "Loss: 3.143066e-05, l1: 0.99976, l2: 0.03180\n",
            "Loss: 3.142746e-05, l1: 0.99976, l2: 0.03180\n",
            "Loss: 3.142465e-05, l1: 0.99977, l2: 0.03180\n",
            "Loss: 3.142411e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.142126e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.142011e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.141915e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.141781e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.141603e-05, l1: 0.99980, l2: 0.03180\n",
            "Loss: 3.141367e-05, l1: 0.99978, l2: 0.03180\n",
            "Loss: 3.141176e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.140757e-05, l1: 0.99981, l2: 0.03180\n",
            "Loss: 3.140911e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.140592e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.140413e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.139944e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.139583e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.139154e-05, l1: 0.99985, l2: 0.03180\n",
            "Loss: 3.138337e-05, l1: 0.99988, l2: 0.03180\n",
            "Loss: 3.138089e-05, l1: 0.99989, l2: 0.03180\n",
            "Loss: 3.137763e-05, l1: 0.99989, l2: 0.03180\n",
            "Loss: 3.137446e-05, l1: 0.99988, l2: 0.03180\n",
            "Loss: 3.137242e-05, l1: 0.99987, l2: 0.03180\n",
            "Loss: 3.137049e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.136932e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.136790e-05, l1: 0.99984, l2: 0.03180\n",
            "Loss: 3.136674e-05, l1: 0.99985, l2: 0.03180\n",
            "Loss: 3.136879e-05, l1: 0.99995, l2: 0.03181\n",
            "Loss: 3.136566e-05, l1: 0.99989, l2: 0.03180\n",
            "Loss: 3.136299e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.136078e-05, l1: 0.99991, l2: 0.03181\n",
            "Loss: 3.135928e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.135877e-05, l1: 0.99990, l2: 0.03181\n",
            "Loss: 3.135803e-05, l1: 0.99987, l2: 0.03181\n",
            "Loss: 3.135687e-05, l1: 0.99984, l2: 0.03180\n",
            "Loss: 3.135459e-05, l1: 0.99977, l2: 0.03180\n",
            "Loss: 3.135476e-05, l1: 0.99970, l2: 0.03180\n",
            "Loss: 3.135310e-05, l1: 0.99974, l2: 0.03180\n",
            "Loss: 3.135033e-05, l1: 0.99971, l2: 0.03180\n",
            "Loss: 3.134567e-05, l1: 0.99968, l2: 0.03180\n",
            "Loss: 3.134353e-05, l1: 0.99967, l2: 0.03180\n",
            "Loss: 3.134221e-05, l1: 0.99973, l2: 0.03180\n",
            "Loss: 3.133926e-05, l1: 0.99962, l2: 0.03180\n",
            "Loss: 3.133771e-05, l1: 0.99967, l2: 0.03180\n",
            "Loss: 3.133682e-05, l1: 0.99970, l2: 0.03180\n",
            "Loss: 3.133517e-05, l1: 0.99975, l2: 0.03180\n",
            "Loss: 3.133372e-05, l1: 0.99977, l2: 0.03180\n",
            "Loss: 3.133315e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.133092e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.132999e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.132771e-05, l1: 0.99979, l2: 0.03179\n",
            "Loss: 3.132523e-05, l1: 0.99980, l2: 0.03179\n",
            "Loss: 3.135869e-05, l1: 0.99993, l2: 0.03179\n",
            "Loss: 3.132427e-05, l1: 0.99981, l2: 0.03179\n",
            "Loss: 3.132072e-05, l1: 0.99983, l2: 0.03179\n",
            "Loss: 3.131604e-05, l1: 0.99984, l2: 0.03179\n",
            "Loss: 3.131184e-05, l1: 0.99988, l2: 0.03178\n",
            "Loss: 3.130852e-05, l1: 0.99987, l2: 0.03178\n",
            "Loss: 3.130667e-05, l1: 0.99986, l2: 0.03178\n",
            "Loss: 3.130548e-05, l1: 0.99980, l2: 0.03179\n",
            "Loss: 3.130174e-05, l1: 0.99982, l2: 0.03179\n",
            "Loss: 3.129999e-05, l1: 0.99982, l2: 0.03178\n",
            "Loss: 3.129828e-05, l1: 0.99981, l2: 0.03178\n",
            "Loss: 3.129715e-05, l1: 0.99980, l2: 0.03178\n",
            "Loss: 3.129597e-05, l1: 0.99979, l2: 0.03178\n",
            "Loss: 3.129631e-05, l1: 0.99979, l2: 0.03178\n",
            "Loss: 3.129513e-05, l1: 0.99979, l2: 0.03178\n",
            "Loss: 3.129308e-05, l1: 0.99977, l2: 0.03178\n",
            "Loss: 3.129067e-05, l1: 0.99976, l2: 0.03178\n",
            "Loss: 3.129166e-05, l1: 0.99973, l2: 0.03178\n",
            "Loss: 3.128946e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.128798e-05, l1: 0.99974, l2: 0.03178\n",
            "Loss: 3.128548e-05, l1: 0.99975, l2: 0.03178\n",
            "Loss: 3.128334e-05, l1: 0.99974, l2: 0.03177\n",
            "Loss: 3.128030e-05, l1: 0.99975, l2: 0.03178\n",
            "Loss: 3.127764e-05, l1: 0.99975, l2: 0.03178\n",
            "Loss: 3.127614e-05, l1: 0.99975, l2: 0.03178\n",
            "Loss: 3.127428e-05, l1: 0.99976, l2: 0.03178\n",
            "Loss: 3.127119e-05, l1: 0.99980, l2: 0.03178\n",
            "Loss: 3.126970e-05, l1: 0.99985, l2: 0.03178\n",
            "Loss: 3.126594e-05, l1: 0.99984, l2: 0.03178\n",
            "Loss: 3.126506e-05, l1: 0.99987, l2: 0.03178\n",
            "Loss: 3.126402e-05, l1: 0.99986, l2: 0.03178\n",
            "Loss: 3.126329e-05, l1: 0.99987, l2: 0.03178\n",
            "Loss: 3.126229e-05, l1: 0.99988, l2: 0.03178\n",
            "Loss: 3.126096e-05, l1: 0.99990, l2: 0.03178\n",
            "Loss: 3.125927e-05, l1: 0.99992, l2: 0.03179\n",
            "Loss: 3.125881e-05, l1: 0.99998, l2: 0.03179\n",
            "Loss: 3.125676e-05, l1: 0.99995, l2: 0.03179\n",
            "Loss: 3.125513e-05, l1: 0.99993, l2: 0.03179\n",
            "Loss: 3.125317e-05, l1: 0.99991, l2: 0.03179\n",
            "Loss: 3.125206e-05, l1: 0.99990, l2: 0.03179\n",
            "Loss: 3.125053e-05, l1: 0.99989, l2: 0.03179\n",
            "Loss: 3.124894e-05, l1: 0.99989, l2: 0.03179\n",
            "Loss: 3.124744e-05, l1: 0.99987, l2: 0.03179\n",
            "Loss: 3.124557e-05, l1: 0.99988, l2: 0.03179\n",
            "Loss: 3.124291e-05, l1: 0.99990, l2: 0.03179\n",
            "Loss: 3.124055e-05, l1: 0.99989, l2: 0.03179\n",
            "Loss: 3.123567e-05, l1: 0.99989, l2: 0.03180\n",
            "Loss: 3.123389e-05, l1: 0.99986, l2: 0.03180\n",
            "Loss: 3.122820e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.122586e-05, l1: 0.99982, l2: 0.03180\n",
            "Loss: 3.122293e-05, l1: 0.99979, l2: 0.03180\n",
            "Loss: 3.124994e-05, l1: 1.00003, l2: 0.03180\n",
            "Loss: 3.122195e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.122006e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.121733e-05, l1: 0.99983, l2: 0.03180\n",
            "Loss: 3.121538e-05, l1: 0.99986, l2: 0.03180\n",
            "Loss: 3.121430e-05, l1: 0.99991, l2: 0.03180\n",
            "Loss: 3.120856e-05, l1: 0.99993, l2: 0.03180\n",
            "Loss: 3.120540e-05, l1: 0.99993, l2: 0.03180\n",
            "Loss: 3.120166e-05, l1: 0.99993, l2: 0.03180\n",
            "Loss: 3.119940e-05, l1: 0.99992, l2: 0.03180\n",
            "Loss: 3.119716e-05, l1: 0.99990, l2: 0.03180\n",
            "Loss: 3.119563e-05, l1: 0.99988, l2: 0.03180\n",
            "Loss: 3.119477e-05, l1: 0.99989, l2: 0.03180\n",
            "Loss: 3.119431e-05, l1: 0.99990, l2: 0.03180\n",
            "Loss: 3.119379e-05, l1: 0.99990, l2: 0.03180\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
            "  Objective function value: 0.000031\n",
            "  Number of iterations: 1263\n",
            "  Number of functions evaluations: 1344\n",
            "Error lambda_1: 0.009775%\n",
            "Error lambda_2: 0.095078%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################    \n",
        "    \n",
        "fig, ax = newfig(1.0, 1.4)\n",
        "ax.axis('off')\n",
        "    \n",
        "####### Row 0: u(t,x) ##################    \n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                  origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "    \n",
        "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "####### Row 1: u(t,x) slices ##################    \n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')    \n",
        "ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])    \n",
        "ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "####### Row 3: Identified PDE ##################    \n",
        "gs2 = gridspec.GridSpec(1, 3)\n",
        "gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "ax = plt.subplot(gs2[:, :])\n",
        "ax.axis('off')\n",
        "s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "s3 = r'Identified PDE (1\\% noise) & '\n",
        "s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "s5 = r'\\end{tabular}$'\n",
        "s = s1+s2+s3+s4+s5\n",
        "ax.text(0.1,0.1,s)\n",
        "        \n",
        "savefig('./Burgers_identification')   \n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "wQJocGkFn0j4",
        "outputId": "64bbb6e8-df1c-454b-b27c-f079980977b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 388.543x336.186 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}